{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "327a9bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import traceback\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "#Statistics\n",
    "from scipy.special import ndtri\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "# from sklearn.metrics import cohen_kappa_score\n",
    "import scipy.stats as stats #For Mann-Whitney U test\n",
    "\n",
    "import warnings #To stop pandas warnings \n",
    "warnings.simplefilter(action='ignore', category=Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f565aa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths with folders containing subfolders named by each participant id. \n",
    "#These subfolders contain txt files and images of discrepancies reviewed by radiologists (information provided in txt files)\n",
    "path_high=os.getcwd()+'/details_final/high'\n",
    "path_low=os.getcwd()+'/details_final/low'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8cf98639",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read excel files with data\n",
    "high=pd.read_excel(os.getcwd()+'/BMI_exp_files'+\"\\\\high_scans.xlsx\") \n",
    "low=pd.read_excel(os.getcwd()+'/BMI_exp_files'+\"\\\\low_scans.xlsx\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9691b599",
   "metadata": {},
   "source": [
    "Function to check results of radiologists' review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a937bc",
   "metadata": {},
   "source": [
    "Main differences compared to emphysema experiment: Added '[:-1]' to some parameters, 'file[:3]' to some others, \n",
    "'litis' and 'ectasis' added, \"'ai' in file.lower() and 'fp' not in file.lower()\" deleted since not exist in Excel of review. Also some changes in 'other' prints for better viewing.\n",
    "\n",
    "In 'Convert slices..' section: Volumes >=30mm3 kept, replaced '!!!' in manual annotations, slices_range replaced with +-10 to get the full range, \"astype(str).str.contains('L')\"\n",
    "\n",
    "In excel files above, volumes changed along with the number of FPs. We also excluded subsolid nodules in those files too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856b4fa6",
   "metadata": {},
   "source": [
    "Peribronchial tissue is excluded now. We also added a section below to check for peribronchial tissue and to add the rest to peribronchial lymph nodules. \n",
    "\n",
    "Also atypical PFNs now in nodule category. In these changed, instead of file.lower().split('fp')[0][:-1] we removed the '[:-1]'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "7a87a407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_information_of_review(path):\n",
    "    \n",
    "    'Gets the path of a folder with subfolders containing images and txt files of results of nodule review.'\n",
    "    'These results should be of the following format: \"nodule\"/\"no nodule\" and then description and a confidence score'\n",
    "    'It prints the participant_id, the txt file (with the slice number and if it is a FP or FN), the confidence score,'\n",
    "    'and a description of the finding given by the radiologists.'\n",
    "    'Returns dictionaries with participant id and nodule ids of findings belonging to each of the nodule/non-nodule'\n",
    "    'categories (two dictionaries for each category, one with FPs and one with FNs). Moreover, it returns 4 dictionaries,'\n",
    "    '2 containing the participant with the correct nodule ids for each of the FPs and FNs and 2 with the wrong ones.'\n",
    "    'Moreover, we get 4 more dictionaries, 2 containing only lymph nodes and 2 containing only nodule ids (FP and FN again).'\n",
    "    'At last, we get 4 dictionaries with non-nodule categories, 2 with FPs and 2 with FNs. Each of them has lung and non-lung findings.'\n",
    "\n",
    "\n",
    "    uncertain=0 #Unsure of what the finding is\n",
    "    nodule_all=0 #Count all nodules, FPs, and FNs\n",
    "    total_files=0 #Total files\n",
    "    excluded=[] #Files not taken into account\n",
    "    tp_mistakes=0 #For TP accidentaly considered as discrepancies during review - happened probably only once\n",
    "    \n",
    "    \n",
    "    #All possible non-nodule categories - based on new definition   \n",
    "    fibr_scar_pleural=0 \n",
    "    other=0\n",
    "    \n",
    "    #FPs and FNs for non-nodule categories\n",
    "    fibr_FP=0\n",
    "    fibr_FN=0\n",
    "    other_FP=0\n",
    "    other_FN=0\n",
    "    \n",
    "    #Possible TPs (errors) for non-nodule categories\n",
    "    fibr_TP=0\n",
    "    other_TP=0\n",
    "    \n",
    "    \n",
    "    #Nodule categories\n",
    "    cal_nod=0\n",
    "    pleu_nod=0\n",
    "    other_nod=0\n",
    "    subgrou_nod=0\n",
    "    canc_nod=0\n",
    "    \n",
    "    atypical_triang=0 #This and the next are typically benign so less important if AI would miss them\n",
    "    peri_fissur=0\n",
    "    bronchperi=0 \n",
    "    \n",
    "    #TP (errors) for nodules\n",
    "    other_nod_TP=0\n",
    "    cal_TP=0\n",
    "    pleu_TP=0\n",
    "    subgrou_TP=0\n",
    "    canc_TP=0\n",
    "    \n",
    "    atypical_TP=0\n",
    "    peri_TP=0\n",
    "    bronchperi_TP=0\n",
    "    \n",
    "    #FPs and FNs for nodule categories\n",
    "    other_nod_FP=0\n",
    "    cal_FP=0\n",
    "    pleu_FP=0\n",
    "    subgrou_FP=0\n",
    "    canc_FP=0\n",
    "    other_nod_FN=0\n",
    "    cal_FN=0\n",
    "    pleu_FN=0\n",
    "    subgrou_FN=0\n",
    "    canc_FN=0\n",
    "    \n",
    "    atypical_FP=0\n",
    "    atypical_FN=0\n",
    "    peri_FP=0\n",
    "    peri_FN=0\n",
    "    bronchperi_FP=0\n",
    "    bronchperi_FN=0\n",
    "    \n",
    "    #Dictionaries to be filled participant_ids and nodule_ids that belong to a given category\n",
    "    atyp_FN={}\n",
    "    per_FN={}\n",
    "    bronchioperi_FN={}\n",
    "    pleural_FN={}\n",
    "    calcif_FN={}\n",
    "    sub_ground_FN={}\n",
    "    cancer_FN={}\n",
    "    other_nodules_FN={}\n",
    "    \n",
    "    other_nonodules_FN={}\n",
    "    fibrosis_FN={}\n",
    "    other_nonodules_FN_lung={}\n",
    "    other_nonodules_FN_nolung={}\n",
    "    \n",
    "    atyp_FP={}\n",
    "    per_FP={}\n",
    "    bronchioperi_FP={}\n",
    "    pleural_FP={}\n",
    "    calcif_FP={}\n",
    "    sub_ground_FP={}\n",
    "    cancer_FP={}\n",
    "    other_nodules_FP={}\n",
    "    \n",
    "    #Non-nodule categories\n",
    "    other_nonodules_FP={}\n",
    "    fibrosis_FP={}\n",
    "    other_nonodules_FP_lung={}\n",
    "    other_nonodules_FP_nolung={}\n",
    "\n",
    "    peri=0\n",
    "    \n",
    "    #Initialize empty dictionaries to keep track FP and FN slices\n",
    "    \n",
    "    #These are for both nodules (+lymph nodes) and non-nodules\n",
    "    dict_FP_correct={}\n",
    "    dict_FN_correct={}\n",
    "    dict_FP_wrong={}\n",
    "    dict_FN_wrong={}\n",
    "\n",
    "    #Only for lymph nodes\n",
    "    lymph_FN_correct={}\n",
    "    lymph_FP_wrong={}\n",
    "\n",
    "    #Only for nodules\n",
    "    nod_FN_correct={}\n",
    "    nod_FP_wrong={}\n",
    "\n",
    "    #List with confidence scores\n",
    "    conf_scores=[]\n",
    "    \n",
    "    \n",
    "    for dirpath, dirnames, filenames in os.walk(path): #Loop over folders and subfolders\n",
    "        for folder in dirnames: #For each folder (has participant name) in the above directory\n",
    "            for file in os.listdir(dirpath+'/'+folder): #For each file in the above folder\n",
    "\n",
    "                if file.endswith('.txt'): #If it's a txt print it (contains the review) along with the folder name (ID)\n",
    "                    print(dirpath,':',folder,':',file)\n",
    "\n",
    "                    with open(dirpath+'/'+folder+'/'+file) as f: #Read txt file\n",
    "                        lines = f.readlines()\n",
    "\n",
    "                    folder_pat=folder[:6] #keep only first 6 letters that correspond to participant id\n",
    "                        \n",
    "                    #Get confidence score - the only number in the text\n",
    "                    confidence=[num for line in lines for num in line if num.isdigit()] \n",
    "            \n",
    "                    if len(confidence)==1: #If there are more numbers it should be checked for errors\n",
    "                        print('Confidence is',int(confidence[0]))\n",
    "                        conf_scores.append(int(confidence[0]))\n",
    "                    else:\n",
    "                        print(\"ERROR in confidence level of file\",file)\n",
    "\n",
    "                        \n",
    "                    no_nodules=[line for line in lines if 'no ' in line.lower()] #if this string in txt then no nodule\n",
    "\n",
    "                    if len(no_nodules)!=0: #Confirm that above non-empty list\n",
    "                        \n",
    "                        total_files=total_files+1 #Increase total number of files taken into account\n",
    "                        print('Finding is NOT a nodule (or it is a lymph node)')\n",
    "                        \n",
    "                        information=[info.split('nodule',1) for info in no_nodules][0] #split only on first occurence \n",
    "                        details=[elem for elem in information if len(elem)>5] #Since we may also have an element with 'no'\n",
    "                        \n",
    "                        if len(details)>0: #If we have a description of finding\n",
    "\n",
    "                            #Perform some replacements to delete '\\n','-', empty spaces and confidence score\n",
    "                            detailed_info=details[0].replace('-', '').replace('\\n','').replace(confidence[0],'').strip()\n",
    "\n",
    "                            print(detailed_info.replace(':',''), 'was written in the txt file')\n",
    "                            \n",
    "#                             if int(confidence[0])>=4: #Only take into account confident predictions\n",
    "                                \n",
    "                            #Below categories for non-nodules\n",
    "\n",
    "                            #For atypical and perifissural we noted them as non-nodules while actually want to be detected\n",
    "                            #We will consider them as nodules - that's why we changed to 'fn_correct', 'fp_wrong' for them\n",
    "                            if ('atypical' in detailed_info.lower() or 'triangular' in detailed_info.lower() and 'fissural' not in detailed_info.lower() \\\n",
    "                                and 'amidst' not in detailed_info.lower()): \n",
    "\n",
    "                                print('atypical/triangular lymph node')\n",
    "                                atypical_triang=atypical_triang+1 #Count them\n",
    "\n",
    "                                if 'tp' in file.lower():\n",
    "                                    print('This will not be considered')\n",
    "                                    atypical_TP=atypical_TP+1\n",
    "                                    tp_mistakes=tp_mistakes+1\n",
    "                                    nodule_all=nodule_all+1\n",
    "\n",
    "                                elif 'fp' in file.lower() or 'ai' in file.lower() and 'fn' not in file.lower():\n",
    "                                    atypical_FP=atypical_FP+1 #Count them\n",
    "                                    nodule_all=nodule_all+1\n",
    "                                                                        \n",
    "                                    if folder_pat not in dict_FP_wrong: #Add participant id to dictionary - list only with slice numbers\n",
    "                                        dict_FP_wrong[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))] \n",
    "                                    else:\n",
    "                                        dict_FP_wrong[folder_pat]=dict_FP_wrong[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    \n",
    "                                    # if folder_pat not in lymph_FP_wrong: #Add it to dictionary with lymph nodes\n",
    "                                    #     lymph_FP_wrong[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    # else:\n",
    "                                    #     lymph_FP_wrong[folder_pat]=lymph_FP_wrong[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    if folder_pat not in nod_FP_wrong:\n",
    "                                        nod_FP_wrong[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    else:\n",
    "                                        nod_FP_wrong[folder_pat]=nod_FP_wrong[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "\n",
    "                                    if int(folder_pat) in atyp_FP: #Add it to corresponding category dictionary\n",
    "                                        atyp_FP[int(folder_pat)].append(file.lower().split('fp')[0])\n",
    "                                        #All these were file.lower().split('fp')[0][:-1] but [:-1] removed! same for fn\n",
    "\n",
    "                                    else:\n",
    "                                        atyp_FP[int(folder_pat)]=[file.lower().split('fp')[0]]\n",
    "\n",
    "                                elif 'fn' in file.lower(): #Similarly as above for FNs\n",
    "                                    atypical_FN=atypical_FN+1\n",
    "                                    nodule_all=nodule_all+1\n",
    "                                    \n",
    "                                    if folder_pat not in dict_FN_correct:\n",
    "                                        dict_FN_correct[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    else:\n",
    "                                        dict_FN_correct[folder_pat]=dict_FN_correct[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    \n",
    "                                    # if folder_pat not in lymph_FN_correct:\n",
    "                                    #     lymph_FN_correct[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    # else:\n",
    "                                    #     lymph_FN_correct[folder_pat]=lymph_FN_correct[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]    \n",
    "                                    if folder_pat not in nod_FN_correct:\n",
    "                                        nod_FN_correct[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    else:\n",
    "                                        nod_FN_correct[folder_pat]=nod_FN_correct[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]  \n",
    "                                    \n",
    "                                    if int(folder_pat) in atyp_FN:\n",
    "                                        atyp_FN[int(folder_pat)].append(file.lower().split('fn')[0])\n",
    "                                    else:\n",
    "                                        atyp_FN[int(folder_pat)]=[file.lower().split('fn')[0]]\n",
    "\n",
    "                                else:\n",
    "                                    print('ERROR in atypical')\n",
    "\n",
    "\n",
    "                            elif ('periphysural' in detailed_info.lower() or 'perifissural' in detailed_info.lower() # or 'fissural' in detailed_info.lower() \n",
    "                                  or 'fiscu' in detailed_info.lower() or 'pfn' in detailed_info.lower() or 'fissural' in detailed_info.lower() \\\n",
    "                                    and 'thickening' not in detailed_info.lower()): \n",
    "\n",
    "                                print('perifissural/fissural/PFN')\n",
    "                                peri_fissur=peri_fissur+1  \n",
    "\n",
    "                                if 'tp' in file.lower():\n",
    "                                    print('This will not be considered')\n",
    "                                    peri_TP=peri_TP+1\n",
    "                                    tp_mistakes=tp_mistakes+1\n",
    "                                    nodule_all=nodule_all+1\n",
    "\n",
    "                                elif 'fp' in file.lower() or 'ai' in file.lower() and 'fn' not in file.lower():\n",
    "                                    peri_FP=peri_FP+1\n",
    "                                    nodule_all=nodule_all+1\n",
    "                                    \n",
    "                                    if folder_pat not in dict_FP_wrong:\n",
    "                                        dict_FP_wrong[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    else:\n",
    "                                        dict_FP_wrong[folder_pat]=dict_FP_wrong[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                                                                                 \n",
    "                                    if folder_pat not in lymph_FP_wrong:\n",
    "                                        lymph_FP_wrong[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    else:\n",
    "                                        lymph_FP_wrong[folder_pat]=lymph_FP_wrong[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "\n",
    "                                    if int(folder_pat) in per_FP:\n",
    "                                        per_FP[int(folder_pat)].append(file.lower().split('fp')[0])\n",
    "                                    else:\n",
    "                                        per_FP[int(folder_pat)]=[file.lower().split('fp')[0]]\n",
    "\n",
    "                                elif 'fn' in file.lower():\n",
    "                                    peri_FN=peri_FN+1\n",
    "                                    nodule_all=nodule_all+1\n",
    "                                    \n",
    "                                    if folder_pat not in dict_FN_correct:\n",
    "                                        dict_FN_correct[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    else:\n",
    "                                        dict_FN_correct[folder_pat]=dict_FN_correct[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    \n",
    "                                    if folder_pat not in lymph_FN_correct:\n",
    "                                        lymph_FN_correct[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    else:\n",
    "                                        lymph_FN_correct[folder_pat]=lymph_FN_correct[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "\n",
    "\n",
    "                                    if int(folder_pat) in per_FN:\n",
    "                                        per_FN[int(folder_pat)].append(file.lower().split('fn')[0])\n",
    "                                    else:\n",
    "                                        per_FN[int(folder_pat)]=[file.lower().split('fn')[0]]\n",
    "\n",
    "                                else:\n",
    "                                    print('ERROR in periphysural')\n",
    "\n",
    "\n",
    "                            elif ('fibrosis' in detailed_info.lower() or 'scar' in detailed_info.lower() \n",
    "                                  or 'thick' in detailed_info.lower() or 'strand' in detailed_info.lower()):\n",
    "\n",
    "                                print('fibrosis/scar/pleural thickening')\n",
    "                                fibr_scar_pleural=fibr_scar_pleural+1 \n",
    "\n",
    "                                if 'tp' in file.lower():\n",
    "                                    print('This will not be considered')\n",
    "                                    fibr_TP=fibr_TP+1\n",
    "                                    tp_mistakes=tp_mistakes+1\n",
    "                                    nodule_all=nodule_all+1\n",
    "\n",
    "                                elif 'fp' in file.lower() or 'ai' in file.lower() and 'fn' not in file.lower():\n",
    "                                    fibr_FP=fibr_FP+1\n",
    "                                    \n",
    "                                    if folder_pat not in dict_FP_correct:\n",
    "                                        dict_FP_correct[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    else:\n",
    "                                        dict_FP_correct[folder_pat]=dict_FP_correct[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                        \n",
    "                                    if 'ai' in file.lower() and 'fp' not in file.lower():\n",
    "                                        dict_FP_correct[folder_pat]=dict_FP_correct[folder_pat][:-1]+[int(''.join([x for x in file[-7:-4] if x.isdigit()]))]\n",
    "                                    \n",
    "                                    if int(folder_pat) in fibrosis_FP:\n",
    "                                        fibrosis_FP[int(folder_pat)].append(file.lower().split('fp')[0])\n",
    "                                    else:\n",
    "                                        fibrosis_FP[int(folder_pat)]=[file.lower().split('fp')[0]]\n",
    "\n",
    "                                elif 'fn' in file.lower():\n",
    "                                    fibr_FN=fibr_FN+1\n",
    "                                    \n",
    "                                    if folder_pat not in dict_FN_wrong:\n",
    "                                        dict_FN_wrong[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    else:\n",
    "                                        dict_FN_wrong[folder_pat]=dict_FN_wrong[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    \n",
    "                                    if int(folder_pat) in fibrosis_FN:\n",
    "                                        fibrosis_FN[int(folder_pat)].append(file.lower().split('fn')[0])\n",
    "                                    else:\n",
    "                                        fibrosis_FN[int(folder_pat)]=[file.lower().split('fn')[0]]\n",
    "\n",
    "                                else:\n",
    "                                    print('ERROR IN fibrosis')\n",
    "\n",
    "\n",
    "                            elif ('bronch' in detailed_info.lower() or 'peribronchial' in detailed_info.lower() or 'pbv' in detailed_info.lower() \\\n",
    "                                or 'hilar' in detailed_info.lower()) \\\n",
    "                                and ('litis' not in detailed_info.lower() and 'ectasis' not in detailed_info.lower() and 'bundle' not in detailed_info.lower() \\\n",
    "                                and 'bronchovascular tissue' not in detailed_info.lower() and 'supportive tissue' not in detailed_info.lower() \\\n",
    "                                and 'some tissue' not in detailed_info.lower()):\n",
    "\n",
    "                                print('peribronchial/bronchiovascular lymph node 1')\n",
    "                                bronchperi=bronchperi+1\n",
    "\n",
    "                                if 'tp' in file.lower():\n",
    "                                    print('This will not be considered')\n",
    "                                    bronchperi_TP=bronchperi_TP+1\n",
    "                                    tp_mistakes=tp_mistakes+1\n",
    "                                    nodule_all=nodule_all+1\n",
    "\n",
    "                                elif 'peribronchial' in detailed_info.lower() or 'pbv' in detailed_info.lower():\n",
    "                                    print('peribronchial that will not be considered - Need to confirm manually that vol<100mm3.')\n",
    "                                    nodule_all=nodule_all+1\n",
    "                                    peri=peri+1\n",
    "\n",
    "                                elif 'fp' in file.lower() or 'ai' in file.lower() and 'fn' not in file.lower():\n",
    "                                    bronchperi_FP=bronchperi_FP+1\n",
    "                                    nodule_all=nodule_all+1\n",
    "                                    \n",
    "                                    if folder_pat not in dict_FP_wrong:\n",
    "                                        dict_FP_wrong[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    else:\n",
    "                                        dict_FP_wrong[folder_pat]=dict_FP_wrong[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "\n",
    "                                    if folder_pat not in lymph_FP_wrong:\n",
    "                                        lymph_FP_wrong[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    else:\n",
    "                                        lymph_FP_wrong[folder_pat]=lymph_FP_wrong[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    \n",
    "                                    if int(folder_pat) in bronchioperi_FP:\n",
    "                                        bronchioperi_FP[int(folder_pat)].append(file.lower().split('fp')[0])\n",
    "                                    else:\n",
    "                                        bronchioperi_FP[int(folder_pat)]=[file.lower().split('fp')[0]]\n",
    "\n",
    "                                elif 'fn' in file.lower():\n",
    "                                    bronchperi_FN=bronchperi_FN+1\n",
    "                                    nodule_all=nodule_all+1\n",
    "                                    \n",
    "                                    if folder_pat not in dict_FN_correct:\n",
    "                                        dict_FN_correct[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    else:\n",
    "                                        dict_FN_correct[folder_pat]=dict_FN_correct[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                        \n",
    "                                    if folder_pat not in lymph_FN_correct:\n",
    "                                        lymph_FN_correct[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    else:\n",
    "                                        lymph_FN_correct[folder_pat]=lymph_FN_correct[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                        \n",
    "                                    if int(folder_pat) in bronchioperi_FN:\n",
    "                                        bronchioperi_FN[int(folder_pat)].append(file.lower().split('fn')[0])\n",
    "                                    else:\n",
    "                                        bronchioperi_FN[int(folder_pat)]=[file.lower().split('fn')[0]]\n",
    "                                else:\n",
    "                                    print('ERROR IN peribronchial')\n",
    "\n",
    "\n",
    "                            elif 'lymph' in detailed_info.lower():  \n",
    "                                #Seperate from above since sometimes it may start with 'fissural lymph node' - 'intrapulmonary lymph node'\n",
    "                                #and therefore being a different category - this checked first in the 'if' above\n",
    "\n",
    "                                if 'bronch' in detailed_info.lower():\n",
    "\n",
    "                                    print('peribronchial/bronchiovascular lymph node')\n",
    "                                    bronchperi=bronchperi+1\n",
    "\n",
    "                                    if 'tp' in file.lower():\n",
    "                                        print('This will not be considered')\n",
    "                                        bronchperi_TP=bronchperi_TP+1\n",
    "                                        tp_mistakes=tp_mistakes+1\n",
    "                                        nodule_all=nodule_all+1\n",
    "\n",
    "                                    elif 'peribronchial' in detailed_info.lower() or 'pbv' in detailed_info.lower():\n",
    "                                        print('peribronchial that will not be considered - Need to confirm manually that vol<100mm3.')\n",
    "                                        nodule_all=nodule_all+1\n",
    "                                        peri=peri+1\n",
    "\n",
    "                                    elif 'fp' in file.lower() or 'ai' in file.lower() and 'fn' not in file.lower():\n",
    "                                        bronchperi_FP=bronchperi_FP+1\n",
    "                                        nodule_all=nodule_all+1\n",
    "                                        \n",
    "                                        if folder_pat not in dict_FP_wrong:\n",
    "                                            dict_FP_wrong[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                        else:\n",
    "                                            dict_FP_wrong[folder_pat]=dict_FP_wrong[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "\n",
    "                                        if folder_pat not in lymph_FP_wrong:\n",
    "                                            lymph_FP_wrong[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                        else:\n",
    "                                            lymph_FP_wrong[folder_pat]=lymph_FP_wrong[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                        \n",
    "                                        if int(folder_pat) in bronchioperi_FP:\n",
    "                                            bronchioperi_FP[int(folder_pat)].append(file.lower().split('fp')[0])\n",
    "                                        else:\n",
    "                                            bronchioperi_FP[int(folder_pat)]=[file.lower().split('fp')[0]]\n",
    "\n",
    "                                    elif 'fn' in file.lower():\n",
    "                                        bronchperi_FN=bronchperi_FN+1\n",
    "                                        nodule_all=nodule_all+1\n",
    "                                        \n",
    "                                        if folder_pat not in dict_FN_correct:\n",
    "                                            dict_FN_correct[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                        else:\n",
    "                                            dict_FN_correct[folder_pat]=dict_FN_correct[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                            \n",
    "                                        if folder_pat not in lymph_FN_correct:\n",
    "                                            lymph_FN_correct[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                        else:\n",
    "                                            lymph_FN_correct[folder_pat]=lymph_FN_correct[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                            \n",
    "                                        if int(folder_pat) in bronchioperi_FN:\n",
    "                                            bronchioperi_FN[int(folder_pat)].append(file.lower().split('fn')[0])\n",
    "                                        else:\n",
    "                                            bronchioperi_FN[int(folder_pat)]=[file.lower().split('fn')[0]]\n",
    "                                    else:\n",
    "                                        print('ERROR IN peribronchial')\n",
    "\n",
    "\n",
    "                                else:   \n",
    "                                    #We added the above section to check for peribronchial tissue and to add the rest to peribronchial lymph nodules\n",
    "                                    #The ones below same as in emphysema\n",
    "                                    print('atypical/triangular lymph node')\n",
    "                                    atypical_triang=atypical_triang+1\n",
    "\n",
    "                                    if 'tp' in file.lower():\n",
    "                                        print('This will not be considered')\n",
    "                                        atypical_TP=atypical_TP+1\n",
    "                                        tp_mistakes=tp_mistakes+1\n",
    "                                        nodule_all=nodule_all+1\n",
    "\n",
    "                                    elif 'fp' in file.lower() or 'ai' in file.lower() and 'fn' not in file.lower():\n",
    "                                        atypical_FP=atypical_FP+1\n",
    "                                        nodule_all=nodule_all+1\n",
    "                                        \n",
    "                                        if folder_pat not in dict_FP_wrong:\n",
    "                                            dict_FP_wrong[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                        else:\n",
    "                                            dict_FP_wrong[folder_pat]=dict_FP_wrong[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "\n",
    "                                        # if folder_pat not in lymph_FP_wrong:\n",
    "                                        #     lymph_FP_wrong[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                        # else:\n",
    "                                        #     lymph_FP_wrong[folder_pat]=lymph_FP_wrong[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                        if folder_pat not in nod_FP_wrong:\n",
    "                                            nod_FP_wrong[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                        else:\n",
    "                                            nod_FP_wrong[folder_pat]=nod_FP_wrong[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                        \n",
    "                                        if int(folder_pat) in atyp_FP:\n",
    "                                            atyp_FP[int(folder_pat)].append(file.lower().split('fp')[0])\n",
    "                                        else:\n",
    "                                            atyp_FP[int(folder_pat)]=[file.lower().split('fp')[0]]\n",
    "\n",
    "                                    elif 'fn' in file.lower():\n",
    "                                        atypical_FN=atypical_FN+1\n",
    "                                        nodule_all=nodule_all+1\n",
    "                                        \n",
    "                                        if folder_pat not in dict_FN_correct:\n",
    "                                            dict_FN_correct[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                        else:\n",
    "                                            dict_FN_correct[folder_pat]=dict_FN_correct[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    \n",
    "                                        # if folder_pat not in lymph_FN_correct:\n",
    "                                        #     lymph_FN_correct[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                        # else:\n",
    "                                        #     lymph_FN_correct[folder_pat]=lymph_FN_correct[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                        if folder_pat not in nod_FN_correct:\n",
    "                                            nod_FN_correct[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                        else:\n",
    "                                            nod_FN_correct[folder_pat]=nod_FN_correct[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]    \n",
    "                                            \n",
    "                                        if int(folder_pat) in atyp_FN:\n",
    "                                            atyp_FN[int(folder_pat)].append(file.lower().split('fn')[0])\n",
    "                                        else:\n",
    "                                            atyp_FN[int(folder_pat)]=[file.lower().split('fn')[0]]\n",
    "\n",
    "                                    else:\n",
    "                                        print('ERROR IN atypical')\n",
    "\n",
    "\n",
    "                            else: #Here when we have description but it's other non-nods (eg. atelectasis)\n",
    "                                other=other+1\n",
    "\n",
    "                                if 'tp' in file.lower():\n",
    "                                    print('This will not be considered')\n",
    "                                    other_TP=other_TP+1\n",
    "                                    tp_mistakes=tp_mistakes+1\n",
    "                                    nodule_all=nodule_all+1\n",
    "\n",
    "                                elif 'fp' in file.lower() or 'ai' in file.lower() and 'fn' not in file.lower():\n",
    "                                    other_FP=other_FP+1\n",
    "                                    \n",
    "                                    if folder_pat not in dict_FP_correct:\n",
    "                                        dict_FP_correct[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    else:\n",
    "                                        dict_FP_correct[folder_pat]=dict_FP_correct[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    \n",
    "                                    if int(folder_pat) in other_nonodules_FP:\n",
    "                                        other_nonodules_FP[int(folder_pat)].append(file.lower().split('fp')[0])\n",
    "\n",
    "                                        res=detailed_info.lower() #Get information about type of non-nodule\n",
    "\n",
    "                                        if 'atele' in res or 'infe' in res or 'conso' in res or 'mucu' in res or 'vess' in res \\\n",
    "                                            or 'vascular' in res or 'pleural' in res or 'adhesion' in res or 'bronchi' in res \\\n",
    "                                            or 'abnormality' in res or 'infiltrate' in res: #Without parenthesis always get in!\n",
    "                                            print(\"Classified as lung finding\")\n",
    "                                            try: #Lung findings\n",
    "                                                other_nonodules_FP_lung[int(folder_pat)].append(file.lower().split('fp')[0])\n",
    "                                            except:\n",
    "                                                other_nonodules_FP_lung[int(folder_pat)]=[file.lower().split('fp')[0]]\n",
    "\n",
    "                                        elif 'bone' in res or 'osis' in res or 'fat' in res or 'tiss' in res or 'colon' in res \\\n",
    "                                            or 'cartilage' in res or 'diaphragm' in res or 'hernia' in res: #Non-lung\n",
    "                                            print(\"Classified as non-lung finding\")\n",
    "                                            try:\n",
    "                                                other_nonodules_FP_nolung[int(folder_pat)].append(file.lower().split('fp')[0])\n",
    "                                            except:\n",
    "                                                other_nonodules_FP_nolung[int(folder_pat)]=[file.lower().split('fp')[0]]\n",
    "                                        else:\n",
    "                                            print(\"Cannot classify it as lung/non-lung based on description\")\n",
    "\n",
    "\n",
    "                                    else:\n",
    "                                        other_nonodules_FP[int(folder_pat)]=[file.lower().split('fp')[0]]\n",
    "\n",
    "                                        res=detailed_info.lower()\n",
    "\n",
    "                                        if 'atele' in res or 'infe' in res or 'conso' in res or 'mucu' in res or 'vess' in res \\\n",
    "                                            or 'vascular' in res or 'pleural' in res or 'adhesion' in res or 'bronchi' in res \\\n",
    "                                            or 'abnormality' in res or 'infiltrate' in res: #Without parenthesis always get in!\n",
    "                                            print(\"Classified as lung finding\")\n",
    "\n",
    "                                            try:\n",
    "                                                other_nonodules_FP_lung[int(folder_pat)].append(file.lower().split('fp')[0])\n",
    "                                            except:\n",
    "                                                other_nonodules_FP_lung[int(folder_pat)]=[file.lower().split('fp')[0]]\n",
    "\n",
    "                                        elif 'bone' in res or 'osis' in res or 'fat' in res or 'tiss' in res or 'colon' in res \\\n",
    "                                            or 'cartilage' in res or 'diaphragm' in res or 'hernia' in res: #Non-lung\n",
    "                                            print(\"Classified as non-lung finding\")\n",
    "                                            try:\n",
    "                                                other_nonodules_FP_nolung[int(folder_pat)].append(file.lower().split('fp')[0])\n",
    "                                            except:\n",
    "                                                other_nonodules_FP_nolung[int(folder_pat)]=[file.lower().split('fp')[0]]\n",
    "                                        else:\n",
    "                                            print(\"Cannot classify it as lung/non-lung based on description\")\n",
    "                                        \n",
    "                                elif 'fn' in file.lower():\n",
    "                                    other_FN=other_FN+1\n",
    "                                    \n",
    "                                    if folder_pat not in dict_FN_wrong:\n",
    "                                        dict_FN_wrong[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    else:  \n",
    "                                        dict_FN_wrong[folder_pat]=dict_FN_wrong[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    \n",
    "                                    if int(folder_pat) in other_nonodules_FN:\n",
    "                                        other_nonodules_FN[int(folder_pat)].append(file.lower().split('fn')[0][:-1])\n",
    "\n",
    "                                        res=detailed_info.lower()\n",
    "\n",
    "                                        if 'atele' in res or 'infe' in res or 'conso' in res or 'mucu' in res or 'vess' in res \\\n",
    "                                            or 'vascular' in res or 'pleural' in res or 'adhesion' in res or 'bronchi' in res \\\n",
    "                                            or 'abnormality' in res or 'infiltrate' in res: #Without parenthesis always get in!\n",
    "                                            print(\"Classified as lung finding\")\n",
    "                                            try:\n",
    "                                                other_nonodules_FN_lung[int(folder_pat)].append(file.lower().split('fn')[0])\n",
    "                                            except:\n",
    "                                                other_nonodules_FN_lung[int(folder_pat)]=[file.lower().split('fn')[0]]\n",
    "\n",
    "                                        elif 'bone' in res or 'osis' in res or 'fat' in res or 'tiss' in res or 'colon' in res \\\n",
    "                                            or 'cartilage' in res or 'diaphragm' in res or 'hernia' in res: #Non-lung\n",
    "                                            print(\"Classified as non-lung finding\")\n",
    "                                            try:\n",
    "                                                other_nonodules_FN_nolung[int(folder_pat)].append(file.lower().split('fn')[0])\n",
    "                                            except:\n",
    "                                                other_nonodules_FN_nolung[int(folder_pat)]=[file.lower().split('fn')[0]]\n",
    "                                        else:\n",
    "                                            print(\"Cannot classify it as lung/non-lung based on description\")\n",
    "\n",
    "                                    else:\n",
    "                                        other_nonodules_FN[int(folder_pat)]=[file.lower().split('fn')[0][:-1]]\n",
    "\n",
    "                                        res=detailed_info.lower()\n",
    "\n",
    "                                        if 'atele' in res or 'infe' in res or 'conso' in res or 'mucu' in res or 'vess' in res \\\n",
    "                                            or 'vascular' in res or 'pleural' in res or 'adhesion' in res or 'bronchi' in res \\\n",
    "                                            or 'abnormality' in res or 'infiltrate' in res: #Without parenthesis always get in!\n",
    "                                            print(\"Classified as lung finding\")\n",
    "                                            try:\n",
    "                                                other_nonodules_FN_lung[int(folder_pat)].append(file.lower().split('fn')[0])\n",
    "                                            except:\n",
    "                                                other_nonodules_FN_lung[int(folder_pat)]=[file.lower().split('fn')[0]]\n",
    "\n",
    "                                        elif 'bone' in res or 'osis' in res or 'fat' in res or 'tiss' in res or 'colon' in res \\\n",
    "                                            or 'cartilage' in res or 'diaphragm' in res or 'hernia' in res: #Non-lung\n",
    "                                            print(\"Classified as non-lung finding\")\n",
    "                                            try:\n",
    "                                                other_nonodules_FN_nolung[int(folder_pat)].append(file.lower().split('fn')[0])\n",
    "                                            except:\n",
    "                                                other_nonodules_FN_nolung[int(folder_pat)]=[file.lower().split('fn')[0]]\n",
    "                                        else:\n",
    "                                            print(\"Cannot classify it as lung/non-lung based on description\")\n",
    "\n",
    "                                else:\n",
    "                                    print('ERROR IN other')                                    \n",
    "                                        \n",
    "#                             else:\n",
    "#                                 print('Low confidence <=3 - excluded from analysis')\n",
    "#                                 excluded.append(folder+':'+file)\n",
    "                                \n",
    "                                \n",
    "                        else: #If we don't have a description of the finding - we add those 'non-nodule' in 'other'\n",
    "                            \n",
    "#                             if int(confidence[0])>=4:\n",
    "\n",
    "                                print('No information for non-nodule file',dirpath,':',folder,':',file)\n",
    "                                other=other+1\n",
    "\n",
    "                                if 'tp' in file.lower():\n",
    "                                    print('This will not be considered')                    \n",
    "                                    other_TP=other_TP+1\n",
    "                                    tp_mistakes=tp_mistakes+1\n",
    "\n",
    "                                elif 'fp' in file.lower() or 'ai' in file.lower():\n",
    "                                    other_FP=other_FP+1\n",
    "                                    \n",
    "                                    if folder_pat not in dict_FP_correct:\n",
    "                                        dict_FP_correct[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    else:\n",
    "                                        dict_FP_correct[folder_pat]=dict_FP_correct[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    \n",
    "                                    if int(folder_pat) in other_nonodules_FP:\n",
    "                                        other_nonodules_FP[int(folder_pat)].append(file.lower().split('fp')[0])\n",
    "                                    else:\n",
    "                                        other_nonodules_FP[int(folder_pat)]=[file.lower().split('fp')[0]]\n",
    "                                        \n",
    "                                elif 'fn' in file.lower():\n",
    "                                    other_FN=other_FN+1\n",
    "                                    \n",
    "                                    if folder_pat not in dict_FN_wrong:\n",
    "                                        dict_FN_wrong[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    else:\n",
    "                                        dict_FN_wrong[folder_pat]=dict_FN_wrong[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    \n",
    "                                    if int(folder_pat) in other_nonodules_FN:\n",
    "                                        other_nonodules_FN[int(folder_pat)].append(file.lower().split('fn')[0])\n",
    "                                    else:\n",
    "                                        other_nonodules_FN[int(folder_pat)]=[file.lower().split('fn')[0]]\n",
    "                                        \n",
    "                                else:\n",
    "                                    print('ERROR IN other')\n",
    "\n",
    "#                             else:\n",
    "#                                 print('Low confidence <=3 - excluded from analysis')\n",
    "#                                 excluded.append(folder+':'+file)\n",
    "                                    \n",
    "                        print('\\n')\n",
    "\n",
    "\n",
    "                    else: #If it's not a non-nodule, it will be either 'unsure' or 'nodule'\n",
    "                        \n",
    "                        total_files=total_files+1 #Increase total number of files taken into account\n",
    "                        unsure=[line for line in lines if 'unsure' in line.lower()] #If line contains 'unsure'\n",
    "                        \n",
    "                        if len(unsure)!=0: #If it's not empty, then unsure about finding\n",
    "                            print('Unsure about what this finding is')\n",
    "                            uncertain=uncertain+1\n",
    "#                             print('Low confidence <=3 - excluded from analysis')\n",
    "#                             excluded.append(folder+':'+file)\n",
    "                            \n",
    "                        else: #Otherwise it's a nodule\n",
    "                            print('Finding is a nodule')\n",
    "                            \n",
    "                            nodules=[line for line in lines if 'nodule' in line.lower()] #Confirm 'nodule' in line\n",
    "                            information=[info.split('nodule',1) for info in nodules][0] #split only on first occurence\n",
    "                            details=[elem for elem in information if len(elem)>5] #similar as above\n",
    "                            \n",
    "                            if len(details)>0: #If we have a description of finding \n",
    "                                    \n",
    "                                    #Clean as above plus ':'\n",
    "                                    nod_desc=details[0].lower().replace('nodule','').replace('-', '').replace('\\n','').replace(confidence[0],'').replace(':','').strip()\n",
    "                                    print(nod_desc)\n",
    "            \n",
    "#                                     if int(confidence[0])>=4:\n",
    "                    \n",
    "                                    #Below categories for nodules\n",
    "\n",
    "                                    if 'calc' in nod_desc:\n",
    "                                        cal_nod=cal_nod+1\n",
    "                                        print('Calcified nodule added')\n",
    "\n",
    "                                        if 'tp' in file.lower():\n",
    "                                            print('This will not be considered')\n",
    "                                            cal_TP=cal_TP+1\n",
    "                                            tp_mistakes=tp_mistakes+1\n",
    "\n",
    "                                        elif 'fp' in file.lower() or 'ai' in file.lower():\n",
    "                                            cal_FP=cal_FP+1\n",
    "                                            \n",
    "                                            if int(folder_pat) in calcif_FP:\n",
    "                                                calcif_FP[int(folder_pat)].append(file.lower().split('fp')[0])\n",
    "                                            else:\n",
    "                                                calcif_FP[int(folder_pat)]=[file.lower().split('fp')[0]]\n",
    "\n",
    "                                        elif 'fn' in file.lower():\n",
    "                                            cal_FN=cal_FN+1\n",
    "                                            \n",
    "                                            if int(folder_pat) in calcif_FN:\n",
    "                                                calcif_FN[int(folder_pat)].append(file.lower().split('fn')[0])\n",
    "                                            else:\n",
    "                                                calcif_FN[int(folder_pat)]=[file.lower().split('fn')[0]]\n",
    "\n",
    "                                        else:\n",
    "                                            print('ERROR IN calcified')\n",
    "\n",
    "\n",
    "                                    elif 'pleu' in nod_desc:\n",
    "                                        pleu_nod=pleu_nod+1\n",
    "                                        print('pleural nodule added')\n",
    "\n",
    "                                        if 'tp' in file.lower():\n",
    "                                            print('This will not be considered')\n",
    "                                            pleu_TP=pleu_TP+1\n",
    "                                            tp_mistakes=tp_mistakes+1\n",
    "\n",
    "                                        elif 'fp' in file.lower() or 'ai' in file.lower():\n",
    "                                            pleu_FP=pleu_FP+1\n",
    "                                            \n",
    "                                            if int(folder_pat) in pleural_FP:\n",
    "                                                pleural_FP[int(folder_pat)].append(file.lower().split('fp')[0])\n",
    "                                            else:\n",
    "                                                pleural_FP[int(folder_pat)]=[file.lower().split('fp')[0]]\n",
    "\n",
    "                                        elif 'fn' in file.lower():\n",
    "                                            pleu_FN=pleu_FN+1\n",
    "                                            \n",
    "                                            if int(folder_pat) in pleural_FN: \n",
    "                                                pleural_FN[int(folder_pat)].append(file.lower().split('fn')[0])\n",
    "                                            else:\n",
    "                                                pleural_FN[int(folder_pat)]=[file.lower().split('fn')[0]]\n",
    "\n",
    "                                        else:\n",
    "                                            print('ERROR IN pleural nodules')\n",
    "\n",
    "\n",
    "                                    elif 'sub' in nod_desc or 'grou' in nod_desc:\n",
    "                                        subgrou_nod=subgrou_nod+1\n",
    "                                        print('subsolid/ground glass nodule added')\n",
    "\n",
    "                                        if 'tp' in file.lower():\n",
    "                                            print('This will not be considered')\n",
    "                                            subgrou_TP=subgrou_TP+1\n",
    "                                            tp_mistakes=tp_mistakes+1\n",
    "\n",
    "                                        elif 'fp' in file.lower() or 'ai' in file.lower():\n",
    "                                            subgrou_FP=subgrou_FP+1\n",
    "                                            \n",
    "                                            if int(folder_pat) in sub_ground_FP:\n",
    "                                                sub_ground_FP[int(folder_pat)].append(file.lower().split('fp')[0])\n",
    "                                            else:\n",
    "                                                sub_ground_FP[int(folder_pat)]=[file.lower().split('fp')[0]]\n",
    "\n",
    "                                        elif 'fn' in file.lower():\n",
    "                                            subgrou_FN=subgrou_FN+1\n",
    "                                            \n",
    "                                            if int(folder_pat) in sub_ground_FN:\n",
    "                                                sub_ground_FN[int(folder_pat)].append(file.lower().split('fn')[0])\n",
    "                                            else:\n",
    "                                                sub_ground_FN[int(folder_pat)]=[file.lower().split('fn')[0]]\n",
    "\n",
    "                                        else:\n",
    "                                            print('ERROR IN subsolid/ground class nodules')\n",
    "\n",
    "\n",
    "                                    elif 'canc' in nod_desc:\n",
    "                                        canc_nod=canc_nod+1\n",
    "                                        print('cancer added')\n",
    "\n",
    "                                        if 'tp' in file.lower():\n",
    "                                            print('This will not be considered')\n",
    "                                            canc_TP=canc_TP+1\n",
    "                                            tp_mistakes=tp_mistakes+1\n",
    "\n",
    "                                        elif 'fp' in file.lower() or 'ai' in file.lower():\n",
    "                                            canc_FP=canc_FP+1\n",
    "                                            \n",
    "                                            if int(folder_pat) in cancer_FP:\n",
    "                                                cancer_FP[int(folder_pat)].append(file.lower().split('fp')[0])\n",
    "                                            else:\n",
    "                                                cancer_FP[int(folder_pat)]=[file.lower().split('fp')[0]]                                           \n",
    "\n",
    "                                        elif 'fn' in file.lower():\n",
    "                                            canc_FN=canc_FN+1\n",
    "                                            \n",
    "                                            if int(folder_pat) in cancer_FN:\n",
    "                                                cancer_FN[int(folder_pat)].append(file.lower().split('fn')[0])\n",
    "                                            else:\n",
    "                                                cancer_FN[int(folder_pat)]=[file.lower().split('fn')[0]]\n",
    "\n",
    "                                        else:\n",
    "                                            print('ERROR IN cancer')\n",
    "    \n",
    "#                                     else:\n",
    "#                                         print('Low confidence <=3 - excluded from analysis')\n",
    "#                                         excluded.append(folder+':'+file)\n",
    "                        \n",
    "                                    else:\n",
    "                                        \n",
    "        #                                 if int(confidence[0])>=4:\n",
    "                                            \n",
    "                                            other_nod=other_nod+1\n",
    "                                            print('No information for file with nodule:',dirpath,':',folder,':',file)\n",
    "\n",
    "\n",
    "                                            if 'tp' in file.lower():\n",
    "                                                print('This will not be considered')\n",
    "                                                other_nod_TP=other_nod_TP+1\n",
    "                                                tp_mistakes=tp_mistakes+1\n",
    "\n",
    "                                            elif 'fp' in file.lower() or 'ai' in file.lower():\n",
    "                                                other_nod_FP=other_nod_FP+1\n",
    "                                                \n",
    "                                                if int(folder_pat) in other_nodules_FP:\n",
    "                                                    other_nodules_FP[int(folder_pat)].append(file.lower().split('fp')[0])\n",
    "                                                else:     \n",
    "                                                    other_nodules_FP[int(folder_pat)]=[file.lower().split('fp')[0]]\n",
    "\n",
    "                                            elif 'fn' in file.lower():\n",
    "                                                other_nod_FN=other_nod_FN+1\n",
    "                                                \n",
    "                                                if int(folder_pat) in other_nodules_FN:\n",
    "                                                    other_nodules_FN[int(folder_pat)].append(file.lower().split('fn')[0])\n",
    "                                                else:\n",
    "                                                    other_nodules_FN[int(folder_pat)]=[file.lower().split('fn')[0]]\n",
    "\n",
    "                                            else:\n",
    "                                                print('ERROR IN other')               \n",
    "                                                \n",
    "        #                                 else:\n",
    "        #                                     print('Low confidence <=3 - excluded from analysis')\n",
    "        #                                     excluded.append(folder+':'+file)\n",
    "                                      \n",
    "                            \n",
    "#                             if int(confidence[0])>=4:\n",
    "                            \n",
    "                            nodule_all=nodule_all+1                                      \n",
    "\n",
    "                            if 'fn' in file.lower() and 'fp' not in file.lower(): \n",
    "                            #Ensure that it was FN - second condition to confirm it\n",
    "                        \n",
    "                                if folder_pat not in dict_FN_correct:\n",
    "                                    dict_FN_correct[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                else:\n",
    "                                    #First letters pick up the slice number\n",
    "                                    dict_FN_correct[folder_pat]=dict_FN_correct[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                    \n",
    "                                    \n",
    "                                if folder_pat not in nod_FN_correct:\n",
    "                                    nod_FN_correct[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                else:\n",
    "                                    nod_FN_correct[folder_pat]=nod_FN_correct[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]    \n",
    "                                    \n",
    "\n",
    "                            elif ('fp' in file.lower() or 'ai' in file.lower()) and 'fn' not in file.lower():\n",
    "                               \n",
    "                                if folder_pat not in dict_FP_wrong:\n",
    "                                    dict_FP_wrong[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                else:\n",
    "                                    dict_FP_wrong[folder_pat]=dict_FP_wrong[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                    \n",
    "                                if folder_pat not in nod_FP_wrong:\n",
    "                                    nod_FP_wrong[folder_pat]=[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                                else:\n",
    "                                    nod_FP_wrong[folder_pat]=nod_FP_wrong[folder_pat]+[int(''.join([x for x in file[:3] if x.isdigit()]))]\n",
    "                        \n",
    "\n",
    "                                if 'ai' in file.lower() and 'fp' not in file.lower():\n",
    "                                    dict_FP_wrong[folder_pat]=dict_FP_wrong[folder_pat][:-1]+[int(''.join([x for x in file[-7:-4] if x.isdigit()]))]\n",
    "                                    nod_FP_wrong[folder_pat]=nod_FP_wrong[folder_pat][:-1]+[int(''.join([x for x in file[-7:-4] if x.isdigit()]))]\n",
    "\n",
    "                        print('\\n')\n",
    "\n",
    "                                                                                                 \n",
    "    print('Num of uncertainties',uncertain)\n",
    "    print('\\n')\n",
    "        \n",
    "    print(\"From nodules, there were {} FP, {} FN, and {} TP calcified nodules\".format(cal_FP,cal_FN,cal_TP))    \n",
    "    print(\"From nodules, there were {} FP, {} FN, and {} TP pleural nodules\".format(pleu_FP,pleu_FN,pleu_TP))\n",
    "    print(\"From nodules, there were {} FP, {} FN, and {} TP 'other' nodules\".format(other_nod_FP,other_nod_FN,other_nod_TP))\n",
    "    print(\"From nodules, there were {} FP, {} FN, and {} TP subsolid/ground class nodules\".format(subgrou_FP,subgrou_FN,subgrou_TP))\n",
    "    print(\"From nodules, there were {} FP, {} FN, and {} TP cancer cases\".format(canc_FP,canc_FN,canc_TP))\n",
    "    print(\"There are {} FP, {} FN, and {} TP atypical PFN and/or triangular lymph nodes\".format(atypical_FP,atypical_FN,atypical_TP))\n",
    "    print(\"There are {} FP, {} FN, and {} TP perifissural/fissural/PFN\".format(peri_FP,peri_FN,peri_TP))\n",
    "    print(\"There are {} FP, {} FN, and {} TP bronchiovascular lymph nodes\".format(bronchperi_FP,bronchperi_FN,bronchperi_TP))\n",
    "    print(\"There are {} peribronchial (excluded) lymph nodes - both FP and FN\".format(peri))\n",
    "    print('\\n')\n",
    "\n",
    "    print(\"There are {} FP, {} FN, and {} TP fibrosis/scar/pleural thickening\".format(fibr_FP,fibr_FN,fibr_TP))\n",
    "    print(\"There are {} FP, {} FN, and {} TP other findings (bone, tissue, mucus, arthosis, vessel, consolidation, infection, fat, atelectasis, etc. )\".format(other_FP,other_FN,other_TP))\n",
    "    print('\\n')\n",
    "\n",
    "    print(\"Total number of files is \",total_files)\n",
    "    print(\"From those, there were {} files excluded due to low confidence <=3 and their names are: {}\".format(len(excluded),excluded))\n",
    "\n",
    "    #Confirm that non-nodules found properly\n",
    "    assert fibr_scar_pleural==fibr_FP+fibr_FN+fibr_TP\n",
    "\n",
    "    #Confirm that nodules found properly\n",
    "    assert cal_nod==cal_FP+cal_FN+cal_TP\n",
    "    assert pleu_nod==pleu_FP+pleu_FN+pleu_TP\n",
    "    assert subgrou_nod==subgrou_FP+subgrou_FN+subgrou_TP\n",
    "    assert canc_nod==canc_FP+canc_FN+canc_TP\n",
    "    \n",
    "    assert atypical_triang==atypical_FP+atypical_FN+atypical_TP\n",
    "    assert peri_fissur==peri_FP+peri_FN+peri_TP\n",
    "    assert bronchperi==bronchperi_TP+bronchperi_FP+bronchperi_FN+peri\n",
    "\n",
    "    assert nodule_all==cal_nod+pleu_nod+other_nod+subgrou_nod+canc_nod+atypical_triang+peri_fissur+bronchperi\n",
    "\n",
    "    print(\"Mean confidence score is \",np.mean(conf_scores),'and median is', np.median(conf_scores), \\\n",
    "          'for a total of ',len(conf_scores),'files')\n",
    "    \n",
    "    return (atyp_FN,per_FN,pleural_FN,calcif_FN,sub_ground_FN,cancer_FN,other_nodules_FN,other_nonodules_FN,fibrosis_FN,\n",
    "            bronchioperi_FN,atyp_FP,per_FP,pleural_FP,calcif_FP,sub_ground_FP,cancer_FP,other_nodules_FP,\n",
    "            other_nonodules_FP,fibrosis_FP,bronchioperi_FP,dict_FP_correct,dict_FP_wrong,dict_FN_correct,dict_FN_wrong,\n",
    "            lymph_FP_wrong,lymph_FN_correct, nod_FP_wrong, nod_FN_correct,\n",
    "            other_nonodules_FN_lung,other_nonodules_FN_nolung, other_nonodules_FP_lung,other_nonodules_FP_nolung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "733d41ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%capture cap --no-stderr\n",
    "\n",
    "(atyp_FN_high,per_FN_high,pleural_FN_high,calcif_FN_high,sub_ground_FN_high,cancer_FN_high,\n",
    " other_nodules_FN_high,other_nonodules_FN_high,fibrosis_FN_high,bronchioperi_FN_high,atyp_FP_high,per_FP_high,\n",
    " pleural_FP_high,calcif_FP_high,sub_ground_FP_high,cancer_FP_high,other_nodules_FP_high,other_nonodules_FP_high,\n",
    " fibrosis_FP_high,bronchioperi_FP_high,dict_FP_correct_high,dict_FP_wrong_high,dict_FN_correct_high,\n",
    " dict_FN_wrong_high,lymph_FP_wrong_high,lymph_FN_correct_high, nod_FP_wrong_high, nod_FN_correct_high,\n",
    " other_nonodules_FN_lung_high,other_nonodules_FN_nolung_high,\n",
    " other_nonodules_FP_lung_high,other_nonodules_FP_nolung_high)=show_information_of_review(path_high)\n",
    "\n",
    "#If we want to save the cell output to txt use below and activate above\n",
    "# with open('no_emph_review.txt', 'w') as f:\n",
    "#     f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67f4c35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture cap --no-stderr\n",
    "\n",
    "(atyp_FN_low,per_FN_low,pleural_FN_low,calcif_FN_low,sub_ground_FN_low,cancer_FN_low,other_nodules_FN_low,\n",
    " other_nonodules_FN_low,fibrosis_FN_low,bronchioperi_FN_low,atyp_FP_low,per_FP_low,pleural_FP_low,calcif_FP_low,\n",
    " sub_ground_FP_low,cancer_FP_low,other_nodules_FP_low,other_nonodules_FP_low,fibrosis_FP_low,bronchioperi_FP_low,\n",
    " dict_FP_correct_low,dict_FP_wrong_low,dict_FN_correct_low,dict_FN_wrong_low,\n",
    " lymph_FP_wrong_low,lymph_FN_correct_low, nod_FP_wrong_low, nod_FN_correct_low,\n",
    " other_nonodules_FN_lung_low,other_nonodules_FN_nolung_low,\n",
    " other_nonodules_FP_lung_low,other_nonodules_FP_nolung_low)=show_information_of_review(path_low)\n",
    "\n",
    "# with open('emph_review.txt', 'w') as f:\n",
    "#     f.write(cap.stdout)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0011253",
   "metadata": {},
   "source": [
    "### Convert slices to IDs - Use manually checked annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "620fa15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "6224a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Added to replace issues with automation algorithm output - Cases with errors\n",
    "high['AI_nod1']=high['AI_nod1'].replace(\"!!!\",'',regex=True) #Replace double exclamation marks \n",
    "low['AI_nod1']=low['AI_nod1'].replace(\"!!!\",'',regex=True) #Replace double exclamation marks \n",
    "\n",
    "high['AI_nod1']=high['AI_nod1'].replace(\"xxx\",'',regex=True) #Replace double exclamation marks \n",
    "low['AI_nod1']=low['AI_nod1'].replace(\"xxx\",'',regex=True) #Replace double exclamation marks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa37e93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e9215b44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vol_cols=[col for col in high.columns if 'V' in col] #Get name of columns containing volumes of AI nodules\n",
    "\n",
    "BMI_deg=['high_fp','low_fp']\n",
    "\n",
    "for deg in BMI_deg: #Loop over BMI degrees\n",
    "    for col in vol_cols: #Loop over columns with volumes\n",
    "        #If the volume is less than 30mm3 we should ignore them - set it along with the corresponding AI nod to '-' \n",
    "        #This can be done since we get TP and FN from other file - This only considers FPs\n",
    "        for ind,val in eval(deg[:-3]+\"[(\"+deg[:-3]+\"['\"+col+\"']<30)]['\"+col+\"'].items()\"): #Changed to keep all volumes >=30mm3\n",
    "            exec(deg[:-3]+\"['\"+col+\"'].iloc[ind]=np.nan\") #was '-' instead of nan\n",
    "            exec(deg[:-3]+\"['AI_nod\"+str(col[1:])+\"'].iloc[ind]=np.nan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8481e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Added since we will keep all volumes, replace 0 with nans to be used below\n",
    "high['0-100fp'].replace(0, np.nan, inplace=True)\n",
    "high['100-300fp'].replace(0, np.nan, inplace=True)\n",
    "high['300+ fp'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "low['0-100fp'].replace(0, np.nan, inplace=True)\n",
    "low['100-300fp'].replace(0, np.nan, inplace=True)\n",
    "low['300+ fp'].replace(0, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "112f55d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select rows where we have at least one FP in any of the 0-100 or 100-300 or 300+ volume subgroup\n",
    "high_fp=high[(high['100-300fp'].notnull() | high['0-100fp'].notnull() | high['300+ fp'].notnull()) & high['participant_id'].notnull()]\n",
    "low_fp=low[(low['100-300fp'].notnull() | low['0-100fp'].notnull() | low['300+ fp'].notnull()) & low['participant_id'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "aa1f91b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize empty dicts in the form {'pat_id1':[],'pat_id2':[],...}\n",
    "high_dict=dict.fromkeys([str(numeric_string) for numeric_string in high_fp['participant_id'].values], [])\n",
    "high_dict=[[key[:6],[]] for (key, value) in high_dict.items()]\n",
    "high_dict = {item[0]: item[1] for item in high_dict}\n",
    "high_fp['participant_id']=list(high_dict.keys())\n",
    "\n",
    "low_dict=dict.fromkeys([str(numeric_string) for numeric_string in low_fp['participant_id'].values], [])\n",
    "low_dict=[[key[:6],[]] for (key, value) in low_dict.items()]\n",
    "low_dict = {item[0]: item[1] for item in low_dict}\n",
    "low_fp['participant_id']=list(low_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "29623898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Volume dictionaries - Normal copies don't work properly. This is why deepcopy is used\n",
    "high_dict_vol=copy.deepcopy(high_dict)\n",
    "low_dict_vol=copy.deepcopy(low_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "20e567f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_cols=[col for col in high_fp.columns if 'AI_nod' in col] #Get name of columns containing AI nodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a1b8671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a4f15fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high_fp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low_fp\n"
     ]
    }
   ],
   "source": [
    "BMI_deg=['high_fp','low_fp'] #list with strings of dfs to loop\n",
    "\n",
    "for deg in BMI_deg: #Loop over emphysema degrees\n",
    "    print(deg)\n",
    "    \n",
    "    for ind_col,col in enumerate(AI_cols): #Loop over AI nodule columns\n",
    "        \n",
    "        #Following line to change nan with '-' since otherwise cannot check for string with 'L' below\n",
    "        exec(deg[:-3]+\"_fp['\"+col+\"']=\"+deg[:-3]+\"_fp['\"+col+\"'].fillna('-')\")\n",
    "        exec(deg[:-3]+\"_fp['\"+str(col)+\"'] = \"+deg[:-3]+\"_fp['\"+str(col)+\"'].astype(str)\") #Convert to string type to use below\n",
    "\n",
    "        #Create variables storing only those rows of df that a specific AI_nod col contains 'L' (denotes a TP)-or not those\n",
    "        exec('temp='+deg[:-3]+'_fp[~'+deg[:-3]+\"_fp['\"+str(col)+\"'].astype(str).str.contains('L')]\") #FPs\n",
    "        exec('temp_tp='+deg[:-3]+'_fp['+deg[:-3]+\"_fp['\"+str(col)+\"'].astype(str).str.contains('L')]\") #TPs\n",
    "\n",
    "        if not temp.empty: #If we have FP for that participant\n",
    "\n",
    "            for ind,pat in enumerate(temp['participant_id']): #Loop over all participants with FP in a specific AI col\n",
    "\n",
    "                try: #To ensure that there are no errors\n",
    "                    nod_id=temp.iloc[ind,ind_col+1][temp.iloc[ind,ind_col+1].find('L')+1:] #Get id\n",
    "                    nod_id=nod_id.split(' ')[0] #To get actual id\n",
    "                    vol=temp.iloc[ind,ind_col+11] #To get the value of the volume\n",
    "                    \n",
    "                    exec(deg[:-3]+'_dict'+\"['\"+str(pat)+\"'].append('\"+nod_id+\"')\") #Add that to the dictionary\n",
    "                    \n",
    "                    if pd.isnull(vol): #When there is no volume - is nan\n",
    "                        exec(deg[:-3]+'_dict_vol'+\"['\"+str(pat)+\"'].append('-')\") #Same for volume dictionary\n",
    "                    else:\n",
    "                        exec(deg[:-3]+'_dict_vol'+\"['\"+str(pat)+\"'].append('\"+str(vol)+\"')\") #Same for volume dictionary\n",
    "                except:\n",
    "                    print(traceback.print_exc()) #print error\n",
    "                    \n",
    "                    \n",
    "        if not temp_tp.empty: #If we have TP for that participant\n",
    "\n",
    "            for ind,pat in enumerate(temp_tp['participant_id']): #Loop over all participants with TP in a specific AI col\n",
    "\n",
    "                try: #To ensure that there are no errors\n",
    "                    exec(deg[:-3]+'_dict'+\"['\"+str(pat)+\"'].append('\"+\"-\"+\"')\") #Add that to the dictionary\n",
    "                    exec(deg[:-3]+'_dict_vol'+\"['\"+str(pat)+\"'].append('\"+\"-\"+\"')\") #Same for volume dictionary\n",
    "                except:\n",
    "                    print(traceback.print_exc())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8f074c7",
   "metadata": {},
   "source": [
    "#### Check below IDs again - They have nodules not reviewed by radiologists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "04e110ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All participants below have nodules not reviewed yet. We should not have any participants printed below at the end. \n",
      "For now we do, since we manually deleted those in which AI detected a finding >30mm3 but manually measured <30mm3.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Confirm all participants' nodules counted - These are nodules not reviewed - Should be checked again\n",
    "\n",
    "print(\"All participants below have nodules not reviewed yet. We should not have any participants printed below at the end. \\n\\\n",
    "For now we do, since we manually deleted those in which AI detected a finding >30mm3 but manually measured <30mm3.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "for deg in BMI_deg: #Loop over emphysema degrees\n",
    "    \n",
    "    for pat in eval(deg[:-3]+\"_dict\"): #For each participant in a given degree\n",
    "        \n",
    "        temp=[x for x in eval(deg[:-3]+\"_dict['\"+pat+\"']\") if x!='-'] #Get how many FP we have - ignore '-' values\n",
    "        counted=0 #Initialize an index to 0\n",
    "\n",
    "        temp_deg=deg\n",
    "            \n",
    "        #If the participant is in any of the FP dictionaries increase the count by the values of that dictionary list\n",
    "        if pat in eval(\"dict_FP_wrong_\"+temp_deg[:-3]): \n",
    "            counted=counted+len(eval(\"dict_FP_wrong_\"+temp_deg[:-3]+\"['\"+pat+\"']\"))\n",
    "\n",
    "        if pat in eval(\"dict_FP_correct_\"+temp_deg[:-3]):\n",
    "            counted=counted+len(eval(\"dict_FP_correct_\"+temp_deg[:-3]+\"['\"+pat+\"']\"))\n",
    "            \n",
    "        #Check if the counts match - If not then we missed some slices\n",
    "        try:\n",
    "            assert len(temp)==counted\n",
    "        except:\n",
    "            print('Missing FP slice(s) for participant',pat,'with',temp_deg[:-3]+' BMI')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd7b1c7",
   "metadata": {},
   "source": [
    "If we ignore peribronchial lymph nodes (for now that's not the case), we would expect errors here like in 103302 slice 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f954e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create copies of dictionaries with FP to be filled with the corresponding ids\n",
    "dict_FP_wrong_high_ids=copy.deepcopy(dict_FP_wrong_high)\n",
    "dict_FP_correct_high_ids=copy.deepcopy(dict_FP_correct_high)\n",
    "dict_FP_wrong_low_ids=copy.deepcopy(dict_FP_wrong_low)\n",
    "dict_FP_correct_low_ids=copy.deepcopy(dict_FP_correct_low)\n",
    "\n",
    "#Same for dictionaries with lymph nodes only and nodule only\n",
    "lymph_FP_wrong_high_ids=copy.deepcopy(lymph_FP_wrong_high)\n",
    "lymph_FP_wrong_low_ids=copy.deepcopy(lymph_FP_wrong_low)\n",
    "nod_FP_wrong_high_ids=copy.deepcopy(nod_FP_wrong_high)\n",
    "nod_FP_wrong_low_ids=copy.deepcopy(nod_FP_wrong_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1efe41c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new dictionaries 'correct' and 'wrong' with FP indices\n",
    "\n",
    "pat_manual_check=[] #Initialize a list to be filled with participants in whom nodules should be filled in manually\n",
    "\n",
    "for deg in BMI_deg: #Loop over BMI degrees\n",
    "    \n",
    "    deg=deg[:-3] #Keep name of BMI degree only, without '_fp'\n",
    "    \n",
    "    for pat in eval(deg+\"['participant_id']\"): #Loop over participants in each BMI degree\n",
    "        \n",
    "        if isinstance(pat,str): #required conversions to only keep first 6 digits of participant_id\n",
    "            try:\n",
    "                pat=int(pat[:6])\n",
    "            except:\n",
    "                pass\n",
    "        else: #If participant_id consists only of numbers, then we assume that this is the 6 digit participant_id\n",
    "            try:\n",
    "                pat=int(pat)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        \n",
    "    #Initialize a list to keep track of slices and ensure that there is no overlap between them - Unique mapping to ids\n",
    "        all_slices=[] \n",
    "\n",
    "        #There are a lot of key errors since a participant might not exist in the FP dictionaries - only TP and/or FNs\n",
    "        try: \n",
    "            for elem in eval(deg+\"_dict['\"+str(pat)+\"']\"): #Loop over all participants findings\n",
    "                if elem!='-': #If there is a value in that finding\n",
    "\n",
    "                    #We replaced with +-10 compared to emphysema experiment since before we had the full range of values\n",
    "                    slices_range=[elemnew for elemnew in eval(deg+\"_dict['\"+str(pat)+\"']\") if elemnew!=elem]\n",
    "                    limit_range=[y for x in slices_range if x!='-' for y in range(int(x)-10,int(x)+10)]\n",
    "\n",
    "                    if int(elem) in limit_range: #If the slice is in the range of +-10 of any other slice\n",
    "\n",
    "                        if pat in pat_manual_check: #If that participant has already been added in list to check \n",
    "                            pass\n",
    "                        else: #If not in the list to check, add it\n",
    "                            pat_manual_check.append(pat)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        temp_deg=deg\n",
    "        \n",
    "        #For those that there is a unique mapping make the conversion:\n",
    "        try:\n",
    "            for AI_ind,elem in enumerate(eval(deg+\"_dict['\"+str(pat)+\"']\")): #Loop over participants of given degree\n",
    "                if elem!='-' and (pat not in pat_manual_check): #For participants with a 1-to-1 mapping\n",
    "\n",
    "                    for i in range(int(elem)-10,int(elem)+10): #Loop over the range of slices\n",
    "                        \n",
    "                        #Then loop over the unique slices in the FP dictionaries\n",
    "                        try: #Again avoid key errors\n",
    "                            for ind,slice_FP_wrong in enumerate(eval(\"dict_FP_wrong_\"+str(temp_deg)+\"_ids['\"+str(pat)+\"']\")): \n",
    "                                    if slice_FP_wrong==i: #If we found that slice replace it with id\n",
    "                                        exec(\"dict_FP_wrong_\"+str(temp_deg)+\"_ids['\"+str(pat)+\"']\"+\"[\"+str(ind)+\"]=\"+str(AI_ind+1))\n",
    "                        except:\n",
    "                            pass\n",
    "                        \n",
    "                        try:\n",
    "                            for ind,slice_FP_wrong in enumerate(eval(\"dict_FP_correct_\"+str(temp_deg)+\"_ids['\"+str(pat)+\"']\")): \n",
    "                                    if slice_FP_wrong==i: #If we found that slice replace it with id\n",
    "                                        exec(\"dict_FP_correct_\"+str(temp_deg)+\"_ids['\"+str(pat)+\"']\"+\"[\"+str(ind)+\"]=\"+str(AI_ind+1))   \n",
    "                        except:\n",
    "                            pass\n",
    "                        \n",
    "                                            \n",
    "                        #Again avoid key errors for lymph nodes and nodules\n",
    "                        try: \n",
    "                            for ind,slice_FP_wrong in enumerate(eval(\"lymph_FP_wrong_\"+str(temp_deg)+\"_ids['\"+str(pat)+\"']\")): \n",
    "                                    if slice_FP_wrong==i: #If we found that slice replace it with id\n",
    "                                        exec(\"lymph_FP_wrong_\"+str(temp_deg)+\"_ids['\"+str(pat)+\"']\"+\"[\"+str(ind)+\"]=\"+str(AI_ind+1))\n",
    "                        except:\n",
    "                            pass\n",
    "                        \n",
    "                        try:\n",
    "                            for ind,slice_FP_wrong in enumerate(eval(\"nod_FP_wrong_\"+str(temp_deg)+\"_ids['\"+str(pat)+\"']\")): \n",
    "                                    if slice_FP_wrong==i: #If we found that slice replace it with id\n",
    "                                        exec(\"nod_FP_wrong_\"+str(temp_deg)+\"_ids['\"+str(pat)+\"']\"+\"[\"+str(ind)+\"]=\"+str(AI_ind+1))   \n",
    "                        except:\n",
    "                            pass\n",
    "                        \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print(\"The following participants have to be checked manually to map slices to ids:\",pat_manual_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1598b2a4",
   "metadata": {},
   "source": [
    "Correct the above errors - The rest are ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High BMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "2ae69b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_FP_wrong_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a24fe1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_FP_wrong_high_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2ab8dec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_FP_correct_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "8d408de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_FP_correct_high_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "40c2f584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nod_FP_wrong_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "51d380f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nod_FP_wrong_high_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4d91701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lymph_FP_wrong_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "25685d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lymph_FP_wrong_high_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039926f8",
   "metadata": {},
   "source": [
    "Low BMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "b80a182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_FP_wrong_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "df49c84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_FP_wrong_low_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "76a01b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_FP_correct_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "abdb52f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_FP_correct_low_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "6141ca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nod_FP_wrong_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "78f6411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nod_FP_wrong_low_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "80772d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lymph_FP_wrong_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "7c25ab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lymph_FP_wrong_low_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115c8c6a",
   "metadata": {},
   "source": [
    "Manually add participants for whom a unique mapping wasn't possible - Atypical lymph nodes considered as nodules below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de36229e",
   "metadata": {},
   "source": [
    "We consider atypical PFNs as lymph nodes here! - In the main analysis below (tables) there is a version in which we exclude them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "195ced8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Low\n",
    "# dict_FP_wrong_low_ids['....']=[4]\n",
    "# dict_FP_correct_low_ids['....']=[5,1,3,2]\n",
    "# nod_FP_wrong_low_ids['...']=[4]\n",
    "\n",
    "# ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6773efc",
   "metadata": {},
   "source": [
    "Check if the FP in the dictionaries are as expected until now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "91619ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pat in dict_FP_correct_low_ids: #Loop over participants in low BMI FP_correct\n",
    "    for id in dict_FP_correct_low_ids[pat]:\n",
    "        try:\n",
    "            if id in dict_FP_wrong_low_ids[pat]: #If this ID is also in FP_wrong\n",
    "                print(\"Low BMI and FP_correct\",pat,id,\" should be checked manually\") #Should fix that manually\n",
    "        except: #might not exist in above dict\n",
    "            pass\n",
    "\n",
    "for pat in dict_FP_wrong_low_ids: #Loop over participants in low BMI FP_wrong\n",
    "    for id in dict_FP_wrong_low_ids[pat]:\n",
    "        try:\n",
    "            if id in dict_FP_correct_low_ids[pat]: #If this ID is also in FP_correct\n",
    "                print(\"Low BMI and FP_wrong\",pat,id,\"should be checked manually\") #Should fix that manually\n",
    "        except: #might not exist in above dict\n",
    "            pass\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for pat in dict_FP_correct_high_ids: #Loop over participants in high BMI FP_correct\n",
    "    for id in dict_FP_correct_high_ids[pat]:\n",
    "        try:\n",
    "            if id in dict_FP_wrong_high_ids[pat]: #If this ID is also in FP_wrong\n",
    "                print(\"High BMI FP_correct\",pat,id,\"should be checked manually\") #Should fix that manually\n",
    "        except: #might not exist in above dict\n",
    "            pass\n",
    "\n",
    "for pat in dict_FP_wrong_high_ids: #Loop over participants in high BMI FP_wrong\n",
    "    for id in dict_FP_wrong_high_ids[pat]: \n",
    "        try:\n",
    "            if id in dict_FP_correct_high_ids[pat]: #If this ID is also in FP_correct\n",
    "                print(\"High BMI FP_wrong\",pat,id,\"should be checked manually\") #Should fix that manually\n",
    "        except: #might not exist in above dict\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad01c8f",
   "metadata": {},
   "source": [
    "Manually correct above errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "9f1a60d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Correct above error\n",
    "# dict_FP_wrong_low_ids['137966']=[1]\n",
    "# lymph_FP_wrong_low_ids['137966']=[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b82420ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are for  high BMI\n",
      "For dict_FP_correct we have 1 \"-\" values for 686142\n",
      "dict_FP_correct_high_ids['686142']deleted\n",
      "\n",
      "\n",
      "Below are for  low BMI\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Delete participants with on cases that mapping is not possible\n",
    "for deg in BMI_deg: #Loop over BMI degree\n",
    "    print(\"Below are for \",deg[:-3],\"BMI\")\n",
    "    \n",
    "    deg=deg[:-3] #Keep name of BMI degree only, without '_fp'\n",
    "\n",
    "    temp_deg=deg\n",
    "    \n",
    "    for key,values in eval(\"dict_FP_wrong_\"+str(temp_deg)+\"_ids.items()\"): #Loop over participants and their id\n",
    "        for ind,val in enumerate(values):\n",
    "\n",
    "            try: #Since we will loop many times in the 'emph' and so, we won't be able to evaluate '-' value as integer\n",
    "                if int(val)>10: #For cases with errors\n",
    "                    exec(\"dict_FP_wrong_\"+str(temp_deg)+\"_ids['\"+key+\"'][\"+str(ind)+\"]=\"+\"'-'\")\n",
    "            except:\n",
    "                print('dict_FP_wrong_'+str(temp_deg)+\"_ids['\"+str(key)+\"']\"+' not deleted')\n",
    "                pass\n",
    "                \n",
    "    for key,values in eval(\"dict_FP_correct_\"+str(temp_deg)+\"_ids.items()\"): \n",
    "        for ind,val in enumerate(values):\n",
    "            try:\n",
    "                if int(val)>10:\n",
    "                    exec(\"dict_FP_correct_\"+str(temp_deg)+\"_ids['\"+key+\"'][\"+str(ind)+\"]=\"+\"'-'\")\n",
    "            except:\n",
    "                print('dict_FP_correct_'+str(temp_deg)+\"_ids['\"+str(key)+\"']\"+' not deleted')\n",
    "                pass       \n",
    "\n",
    "\n",
    "    #Delete participants with only '-' values\n",
    "    del_keys=[]\n",
    "    for key,values in eval(\"dict_FP_wrong_\"+str(temp_deg)+\"_ids.items()\"): \n",
    "        if np.unique(values)[0]=='-': #If a participant has '-' which denotes an error\n",
    "            del_keys.append(key)\n",
    "            print('For dict_FP_wrong we have',len(values),'\"-\" values for',key)\n",
    "            \n",
    "    for key in del_keys:\n",
    "        try:\n",
    "            exec(\"del dict_FP_wrong_\"+str(temp_deg)+\"_ids['\"+key+\"']\") #Delete those participants and its values from dict\n",
    "            print('dict_FP_wrong_'+str(temp_deg)+\"_ids['\"+str(key)+\"']\"+'deleted')\n",
    "        except:\n",
    "            print('dict_FP_wrong_'+str(temp_deg)+\"_ids['\"+str(key)+\"']\"+' not deleted')\n",
    "            pass\n",
    "        \n",
    "    #Similar for dict_FP_correct    \n",
    "    del_keys=[]\n",
    "    for key,values in eval(\"dict_FP_correct_\"+str(temp_deg)+\"_ids.items()\"): \n",
    "        if np.unique(values)[0]=='-':\n",
    "            del_keys.append(key)\n",
    "            print('For dict_FP_correct we have',len(values),'\"-\" values for',key)\n",
    "       \n",
    "    for key in del_keys:\n",
    "        try:\n",
    "            exec(\"del dict_FP_correct_\"+str(temp_deg)+\"_ids['\"+key+\"']\") \n",
    "            print('dict_FP_correct_'+str(temp_deg)+\"_ids['\"+str(key)+\"']\"+'deleted')\n",
    "        except:\n",
    "            print('dict_FP_correct_'+str(temp_deg)+\"_ids['\"+str(key)+\"']\"+' not deleted')\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        \n",
    "    #Similar for lymph nodes only and nod only dictionaries\n",
    "    for key,values in eval(\"lymph_FP_wrong_\"+str(temp_deg)+\"_ids.items()\"): \n",
    "        for ind,val in enumerate(values):\n",
    "\n",
    "            try: #Since we will loop many times in the 'emph' and so, we won't be able to evaluate '-' value as integer\n",
    "                if int(val)>10:\n",
    "                    exec(\"lymph_FP_wrong_\"+str(temp_deg)+\"_ids['\"+key+\"'][\"+str(ind)+\"]=\"+\"'-'\")\n",
    "            except:\n",
    "                print(\"lymph_FP_wrong_\"+str(temp_deg)+\"_ids[\"+key+\"] not deleted\")\n",
    "                pass\n",
    "                \n",
    "    for key,values in eval(\"nod_FP_wrong_\"+str(temp_deg)+\"_ids.items()\"): \n",
    "        for ind,val in enumerate(values):\n",
    "            try:\n",
    "                if int(val)>10:\n",
    "                    exec(\"nod_FP_wrong_\"+str(temp_deg)+\"_ids['\"+key+\"'][\"+str(ind)+\"]=\"+\"'-'\")\n",
    "            except:\n",
    "                print(\"nod_FP_wrong_\"+str(temp_deg)+\"_ids[\"+key+\"] not deleted\")\n",
    "                pass\n",
    "            \n",
    "            \n",
    "    #Delete participants with only '-' values\n",
    "    del_keys=[]\n",
    "    for key,values in eval(\"lymph_FP_wrong_\"+str(temp_deg)+\"_ids.items()\"): \n",
    "        if np.unique(values)[0]=='-':\n",
    "            del_keys.append(key)            \n",
    "       \n",
    "    for key in del_keys:\n",
    "        try:\n",
    "            exec(\"del lymph_FP_wrong_\"+str(temp_deg)+\"_ids['\"+key+\"']\") \n",
    "            print('lymph_FP_wrong_'+str(temp_deg)+\"_ids['\"+str(key)+\"']\"+'deleted' )\n",
    "        except:\n",
    "            print('lymph_FP_wrong_'+str(temp_deg)+\"_ids['\"+str(key)+\"']\"+'not deleted')\n",
    "            pass\n",
    "        \n",
    "    del_keys=[]\n",
    "    for key,values in eval(\"nod_FP_wrong_\"+str(temp_deg)+\"_ids.items()\"): \n",
    "        if np.unique(values)[0]=='-':\n",
    "            del_keys.append(key)\n",
    "       \n",
    "    for key in del_keys:\n",
    "        try:\n",
    "            exec(\"del nod_FP_wrong_\"+str(temp_deg)+\"_ids['\"+key+\"']\") \n",
    "            print('nod_FP_wrong_'+str(temp_deg)+\"_ids['\"+str(key)+\"']\"+'deleted')\n",
    "        except:\n",
    "            print('nod_FP_wrong_'+str(temp_deg)+\"_ids['\"+str(key)+\"']\"+' not deleted')\n",
    "            pass\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There might be errors above that should be checked and deleted from nodule list (initially there was 119215)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_FP_correct_high['686142']=[290]\n",
    "dict_FP_correct_high_ids['686142']=[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c15925b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pat_manual_check #These are the participants that should be checked manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "5b09af76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checks again - Copy pasted from above\n",
    "for pat in dict_FP_correct_low_ids: #Loop over participants in low BMI FP_correct\n",
    "    for id in dict_FP_correct_low_ids[pat]:\n",
    "        try:\n",
    "            if id in dict_FP_wrong_low_ids[pat]: #If this ID is also in FP_wrong\n",
    "                print(\"Low BMI and FP_correct\",pat,'with ID',id,\"should be checked manually\") #Should fix that manually\n",
    "        except: #might not exist in above dict\n",
    "            pass\n",
    "\n",
    "for pat in dict_FP_wrong_low_ids: #Loop over participants in low BMI FP_wrong\n",
    "    for id in dict_FP_wrong_low_ids[pat]:\n",
    "        try:\n",
    "            if id in dict_FP_correct_low_ids[pat]: #If this ID is also in FP_correct\n",
    "                print(\"Low BMI and FP_wrong\",pat,'with ID',id,\"should be checked manually\") #Should fix that manually\n",
    "        except: #might not exist in above dict\n",
    "            pass\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for pat in dict_FP_correct_high_ids: #Loop over participants in high BMI FP_correct\n",
    "    for id in dict_FP_correct_high_ids[pat]:\n",
    "        try:\n",
    "            if id in dict_FP_wrong_high_ids[pat]: #If this ID is also in FP_wrong\n",
    "                print(\"High BMI FP_correct\",pat,'with ID',id,\"should be checked manually\") #Should fix that manually\n",
    "        except: #might not exist in above dict\n",
    "            pass\n",
    "\n",
    "for pat in dict_FP_wrong_high_ids: #Loop over participants in high BMI FP_wrong\n",
    "    for id in dict_FP_wrong_high_ids[pat]: \n",
    "        try:\n",
    "            if id in dict_FP_correct_high_ids[pat]: #If this ID is also in FP_correct\n",
    "                print(\"High BMI FP_wrong\",pat,'with ID',id,\"should be checked manually\") #Should fix that manually\n",
    "        except: #might not exist in above dict\n",
    "            pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9e9cd18",
   "metadata": {},
   "source": [
    "Further checks based on number of nodules, lymph nodes, and non-nodules (originally FPs based on AI initial reading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "76851915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low BMI\n",
      "Non-nodules: 96\n",
      "Lymph nodes: 5\n",
      "Nodules 17\n",
      "All findings: 118\n"
     ]
    }
   ],
   "source": [
    "print(\"Low BMI\")\n",
    "all=0\n",
    "for pat in dict_FP_correct_low:\n",
    "    all=all+len(dict_FP_correct_low[pat])\n",
    "print('Non-nodules:',all)\n",
    "\n",
    "FP_lymph_wrong=0\n",
    "for pat in lymph_FP_wrong_low:\n",
    "    FP_lymph_wrong=FP_lymph_wrong+len(lymph_FP_wrong_low[pat])\n",
    "print(\"Lymph nodes:\",FP_lymph_wrong) \n",
    "\n",
    "FP_nod_wrong=0\n",
    "for pat in nod_FP_wrong_low:\n",
    "    FP_nod_wrong=FP_nod_wrong+len(nod_FP_wrong_low[pat])\n",
    "print(\"Nodules\",FP_nod_wrong)\n",
    "\n",
    "for pat in dict_FP_wrong_low:\n",
    "    all=all+len(dict_FP_wrong_low[pat])\n",
    "print(\"All findings:\",all) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "af358add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High BMI\n",
      "Non-nodules: 53\n",
      "Lymph nodes: 9\n",
      "Nodules 22\n",
      "All findings: 84\n"
     ]
    }
   ],
   "source": [
    "print(\"High BMI\")\n",
    "all=0\n",
    "for pat in dict_FP_correct_high:\n",
    "    all=all+len(dict_FP_correct_high[pat])\n",
    "print('Non-nodules:',all)\n",
    "\n",
    "FP_lymph_wrong=0\n",
    "for pat in lymph_FP_wrong_high:\n",
    "    FP_lymph_wrong=FP_lymph_wrong+len(lymph_FP_wrong_high[pat])\n",
    "print(\"Lymph nodes:\",FP_lymph_wrong) \n",
    "\n",
    "FP_nod_wrong=0\n",
    "for pat in nod_FP_wrong_high:\n",
    "    FP_nod_wrong=FP_nod_wrong+len(nod_FP_wrong_high[pat])\n",
    "print(\"Nodules\",FP_nod_wrong)\n",
    "\n",
    "for pat in dict_FP_wrong_high:\n",
    "    all=all+len(dict_FP_wrong_high[pat])\n",
    "print(\"All findings:\",all) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "9cc3c3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total FP wrong for low BMI are: 22\n",
      "Total FP wrong for high BMI are: 31\n",
      "Total FP correct for low BMI are: 96\n",
      "Total FP correct for high BMI are: 53\n"
     ]
    }
   ],
   "source": [
    "FP_wrong_low=np.sum([len(x) for x in dict_FP_wrong_low.values()])\n",
    "FP_wrong_high=np.sum([len(x) for x in dict_FP_wrong_high.values()])\n",
    "print(\"Total FP wrong for low BMI are:\",FP_wrong_low)\n",
    "print(\"Total FP wrong for high BMI are:\",FP_wrong_high)\n",
    "\n",
    "FP_correct_low=np.sum([len(x) for x in dict_FP_correct_low.values()])\n",
    "FP_correct_high=np.sum([len(x) for x in dict_FP_correct_high.values()])\n",
    "print(\"Total FP correct for low BMI are:\",FP_correct_low)\n",
    "print(\"Total FP correct for high BMI are:\",FP_correct_high)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63b99818",
   "metadata": {},
   "source": [
    "Manually add participants for whom a unique mapping wasn't possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "3af05d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_FP_correct_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ec33a806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_FP_wrong_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "6929ca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_FP_correct_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "8cbae17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_FP_wrong_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "b950c510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checks again - Copy pasted from above and added lymph nodes and nodules - Should not print anything now\n",
    "for pat in dict_FP_correct_low_ids: #Loop over participants in low BMI FP_correct\n",
    "    for id in dict_FP_correct_low_ids[pat]:\n",
    "        try:\n",
    "            if id in dict_FP_wrong_low_ids[pat]: #If this ID is also in FP_wrong\n",
    "                print(\"Low BMI and FP_correct\",pat,'with ID',id,\"should be checked manually\") #Should fix that manually\n",
    "        except: #might not exist in above dict\n",
    "            pass\n",
    "\n",
    "for pat in dict_FP_wrong_low_ids: #Loop over participants in low BMI FP_wrong\n",
    "    for id in dict_FP_wrong_low_ids[pat]:\n",
    "        try:\n",
    "            if id in dict_FP_correct_low_ids[pat]: #If this ID is also in FP_correct\n",
    "                print(\"Low BMI and FP_wrong\",pat,'with ID',id,\"should be checked manually\") #Should fix that manually\n",
    "        except: #might not exist in above dict\n",
    "            pass\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for pat in dict_FP_correct_high_ids: #Loop over participants in high BMI FP_correct\n",
    "    for id in dict_FP_correct_high_ids[pat]:\n",
    "        try:\n",
    "            if id in dict_FP_wrong_high_ids[pat]: #If this ID is also in FP_wrong\n",
    "                print(\"High BMI FP_correct\",pat,'with ID',id,\"should be checked manually\") #Should fix that manually\n",
    "        except: #might not exist in above dict\n",
    "            pass\n",
    "\n",
    "for pat in dict_FP_wrong_high_ids: #Loop over participants in high BMI FP_wrong\n",
    "    for id in dict_FP_wrong_high_ids[pat]: \n",
    "        try:\n",
    "            if id in dict_FP_correct_high_ids[pat]: #If this ID is also in FP_correct\n",
    "                print(\"High BMI FP_wrong\",pat,'with ID',id,\"should be checked manually\") #Should fix that manually\n",
    "        except: #might not exist in above dict\n",
    "            pass\n",
    "\n",
    "\n",
    "#For lymph nodes and nodules\n",
    "for pat in lymph_FP_wrong_low_ids: #Loop over participants in low BMI FP_wrong\n",
    "    for id in lymph_FP_wrong_low_ids[pat]:\n",
    "        try:\n",
    "            if id in nod_FP_wrong_low_ids[pat]: #If this ID is also in nod_FP_wrong\n",
    "                print(\"Low BMI lymph_FP_wrong\",pat,'with ID',id,\"should be checked manually\")\n",
    "        except: #might not exist in above dict\n",
    "            pass\n",
    "\n",
    "for pat in nod_FP_wrong_low_ids: #Loop over participants in low BMI nod_FP_wrong\n",
    "    for id in nod_FP_wrong_low_ids[pat]:\n",
    "        try:\n",
    "            if id in lymph_FP_wrong_low_ids[pat]: #If this ID is also in lymph_FP_wrong\n",
    "                print(\"Low BMI nod_FP_wrong\",pat,'with ID',id,\"should be checked manually\")\n",
    "        except: #might not exist in above dict\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "097f9cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-nodules: 96\n",
      "Lymph nodes: 5\n",
      "Nodules 17\n",
      "All findings: 118\n"
     ]
    }
   ],
   "source": [
    "all=0\n",
    "for pat in dict_FP_correct_low:\n",
    "    all=all+len(dict_FP_correct_low[pat])\n",
    "print('Non-nodules:',all)\n",
    "\n",
    "FP_lymph_wrong=0\n",
    "for pat in lymph_FP_wrong_low:\n",
    "    FP_lymph_wrong=FP_lymph_wrong+len(lymph_FP_wrong_low[pat])\n",
    "print(\"Lymph nodes:\",FP_lymph_wrong) \n",
    "\n",
    "FP_nod_wrong=0\n",
    "for pat in nod_FP_wrong_low:\n",
    "    FP_nod_wrong=FP_nod_wrong+len(nod_FP_wrong_low[pat])\n",
    "print(\"Nodules\",FP_nod_wrong)\n",
    "\n",
    "for pat in dict_FP_wrong_low:\n",
    "    all=all+len(dict_FP_wrong_low[pat])\n",
    "print(\"All findings:\",all) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "639218e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total FP wrong for low BMI are: 22\n",
      "Total FP wrong for high BMI are: 31\n",
      "Total FP correct for low BMI are: 96\n",
      "Total FP correct for high BMI are: 53\n"
     ]
    }
   ],
   "source": [
    "FP_wrong_low=np.sum([len(x) for x in dict_FP_wrong_low.values()])\n",
    "FP_wrong_high=np.sum([len(x) for x in dict_FP_wrong_high.values()])\n",
    "print(\"Total FP wrong for low BMI are:\",FP_wrong_low)\n",
    "print(\"Total FP wrong for high BMI are:\",FP_wrong_high)\n",
    "\n",
    "FP_correct_low=np.sum([len(x) for x in dict_FP_correct_low.values()])\n",
    "FP_correct_high=np.sum([len(x) for x in dict_FP_correct_high.values()])\n",
    "print(\"Total FP correct for low BMI are:\",FP_correct_low)\n",
    "print(\"Total FP correct for high BMI are:\",FP_correct_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3403f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Low BMI - FPs based on initial AI reading\n",
    "print(\"Atypical PFNs from AI\",np.sum([len(atyp_FP_low[x]) for x in atyp_FP_low]),atyp_FP_low) #9\n",
    "print(\"PFNs from AI\", np.sum([len(per_FP_low[x]) for x in per_FP_low]), per_FP_low) #2\n",
    "print(\"Calcified nodules from AI\",np.sum([len(calcif_FP_low[x]) for x in calcif_FP_low]),calcif_FP_low) #1\n",
    "print(\"Subsolid and ground-glass nodules from AI\",np.sum([len(sub_ground_FP_low[x]) for x in sub_ground_FP_low]),sub_ground_FP_low) #4\n",
    "print(\"Cancer from AI\",np.sum([len(cancer_FP_low[x]) for x in cancer_FP_low]),cancer_FP_low) #1\n",
    "print(\"Pleural nodules from AI\",np.sum([len(pleural_FP_low[x]) for x in pleural_FP_low]),pleural_FP_low) #0\n",
    "print(\"Other nodules from AI\",np.sum([len(other_nodules_FP_low[x]) for x in other_nodules_FP_low]),other_nodules_FP_low) #2\n",
    "print(\"Other non-nodules (FPs) from AI\",np.sum([len(other_nonodules_FP_low[x]) for x in other_nonodules_FP_low]),other_nonodules_FP_low) #56\n",
    "print(\"Fibrosis/scars (FPs) from AI\",np.sum([len(fibrosis_FP_low[x]) for x in fibrosis_FP_low]),fibrosis_FP_low) #40\n",
    "print(\"Bronchovascular and Peribronchial non-nodules (FPs) from AI\",\n",
    "      np.sum([len(bronchioperi_FP_low[x]) for x in bronchioperi_FP_low]),bronchioperi_FP_low) #3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#High BMI - FPs based on initial AI reading\n",
    "print(\"Atypical PFNs from AI\",np.sum([len(atyp_FP_high[x]) for x in atyp_FP_high]),atyp_FP_high) #17\n",
    "print(\"PFNs from AI\", np.sum([len(per_FP_high[x]) for x in per_FP_high]), per_FP_high) #8\n",
    "print(\"Calcified nodules from AI\",np.sum([len(calcif_FP_high[x]) for x in calcif_FP_high]),calcif_FP_high) #0\n",
    "print(\"Subsolid and ground-glass nodules from AI\",np.sum([len(sub_ground_FP_high[x]) for x in sub_ground_FP_high]),sub_ground_FP_high) #2\n",
    "print(\"Cancer from AI\",np.sum([len(cancer_FP_high[x]) for x in cancer_FP_high]),cancer_FP_high) #0\n",
    "print(\"Pleural nodules from AI\",np.sum([len(pleural_FP_high[x]) for x in pleural_FP_high]),pleural_FP_high) #0\n",
    "print(\"Other nodules from AI\",np.sum([len(other_nodules_FP_high[x]) for x in other_nodules_FP_high]),other_nodules_FP_high) #3\n",
    "print(\"Other non-nodules (FPs) from AI\",np.sum([len(other_nonodules_FP_high[x]) for x in other_nonodules_FP_high]),other_nonodules_FP_high) #41\n",
    "print(\"Fibrosis/scars (FPs) from AI\",np.sum([len(fibrosis_FP_high[x]) for x in fibrosis_FP_high]),fibrosis_FP_high) #12\n",
    "print(\"Bronchovascular and Peribronchial non-nodules (FPs) from AI\", \n",
    "        np.sum([len(bronchioperi_FP_high[x]) for x in bronchioperi_FP_high]),bronchioperi_FP_high) #1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cba74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lists of dictionaries with participant and nodule ids that belong to a given category\n",
    "all_categories=[atyp_FN_high,per_FN_high,pleural_FN_high,calcif_FN_high,sub_ground_FN_high,\n",
    "                cancer_FN_high,other_nodules_FN_high,other_nonodules_FN_high,fibrosis_FN_high,\n",
    "                bronchioperi_FN_high,atyp_FP_high,per_FP_high,pleural_FP_high,calcif_FP_high,\n",
    "                sub_ground_FP_high,cancer_FP_high,other_nodules_FP_high,other_nonodules_FP_high,\n",
    "                fibrosis_FP_high,bronchioperi_FP_high,  #Until here high BMI\n",
    "                atyp_FN_low,per_FN_low,pleural_FN_low,calcif_FN_low,sub_ground_FN_low,cancer_FN_low,\n",
    "                other_nodules_FN_low,other_nonodules_FN_low,fibrosis_FN_low,bronchioperi_FN_low,atyp_FP_low,\n",
    "                per_FP_low,pleural_FP_low,calcif_FP_low,sub_ground_FP_low,cancer_FP_low,\n",
    "                other_nodules_FP_low,other_nonodules_FP_low,fibrosis_FP_low,bronchioperi_FP_low] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd957a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as above with the names as strings\n",
    "name_cats=['atyp_FN_high','per_FN_high','pleural_FN_high','calcif_FN_high','sub_ground_FN_high',\n",
    "            'cancer_FN_high','other_nodules_FN_high','other_nonodules_FN_high','fibrosis_FN_high',\n",
    "            'bronchioperi_FN_high','atyp_FP_high','per_FP_high','pleural_FP_high','calcif_FP_high',\n",
    "            'sub_ground_FP_high','cancer_FP_high','other_nodules_FP_high','other_nonodules_FP_high',\n",
    "            'fibrosis_FP_high','bronchioperi_FP_high',  #Until here high BMI\n",
    "            'atyp_FN_low','per_FN_low','pleural_FN_low','calcif_FN_low','sub_ground_FN_low','cancer_FN_low',\n",
    "            'other_nodules_FN_low','other_nonodules_FN_low','fibrosis_FN_low','bronchioperi_FN_low','atyp_FP_low',\n",
    "            'per_FP_low','pleural_FP_low','calcif_FP_low','sub_ground_FP_low','cancer_FP_low',\n",
    "            'other_nodules_FP_low','other_nonodules_FP_low','fibrosis_FP_low','bronchioperi_FP_low']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32074c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save dictionaries to be used from 'patient_selection_emphysema_experiment.ipynb' file to match slices with ids\n",
    "\n",
    "with open('dict_FN_wrong_low.pickle','wb') as f:\n",
    "    pickle.dump(dict_FN_wrong_low,f)\n",
    "\n",
    "with open('dict_FN_correct_low.pickle','wb') as f:\n",
    "    pickle.dump(dict_FN_correct_low,f)    \n",
    "\n",
    "with open('dict_FN_wrong_high.pickle','wb') as f:\n",
    "    pickle.dump(dict_FN_wrong_high,f)\n",
    "\n",
    "with open('dict_FN_correct_high.pickle','wb') as f:\n",
    "    pickle.dump(dict_FN_correct_high,f) \n",
    "    \n",
    "#Same for lymph nodes only and nodules only dictionaries\n",
    "with open('lymph_FN_correct_low.pickle','wb') as f:\n",
    "    pickle.dump(lymph_FN_correct_low,f)    \n",
    "\n",
    "with open('lymph_FN_correct_high.pickle','wb') as f:\n",
    "    pickle.dump(lymph_FN_correct_high,f) \n",
    "    \n",
    "with open('nod_FN_correct_low.pickle','wb') as f:\n",
    "    pickle.dump(nod_FN_correct_low,f)    \n",
    "\n",
    "with open('nod_FN_correct_high.pickle','wb') as f:\n",
    "    pickle.dump(nod_FN_correct_high,f) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7524dba0",
   "metadata": {},
   "source": [
    "### Get volume subgroups for nodules/non-nodules for each of high/low BMI\n",
    "\n",
    "##### AI found, reader missed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b1527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get numbers for nodules (+lymph nodes) vs no nodules\n",
    "#Nodules can also be found be just adding nodules only + lymph nodes only from below\n",
    "\n",
    "ai_nonods_high_30_100=0\n",
    "ai_nonods_high_100_300=0\n",
    "ai_nonods_high_300=0\n",
    "\n",
    "ai_nonods_low_30_100=0\n",
    "ai_nonods_low_100_300=0\n",
    "ai_nonods_low_300=0\n",
    "\n",
    "#Similarly get numbers for nodules only and for lymph nodes only\n",
    "ai_only_nods_high_30_100=0\n",
    "ai_only_nods_high_100_300=0\n",
    "ai_only_nods_high_300=0\n",
    "\n",
    "ai_lymph_high_30_100=0\n",
    "ai_lymph_high_100_300=0\n",
    "ai_lymph_high_300=0\n",
    "\n",
    "ai_only_nods_low_30_100=0\n",
    "ai_only_nods_low_100_300=0\n",
    "ai_only_nods_low_300=0\n",
    "\n",
    "ai_lymph_low_30_100=0\n",
    "ai_lymph_low_100_300=0\n",
    "ai_lymph_low_300=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda77b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similarly for volume of nodules (if comparison between groups with Mann-Whitney U test is used below)\n",
    "\n",
    "#Get detailed list of volume of nodules (+lymph nodes) vs no nodules\n",
    "#Nodules can also be found be just adding nodules only + lymph nodes only from below\n",
    "\n",
    "ai_nonods_high_30_100_vols=[]\n",
    "ai_nonods_high_100_300_vols=[]\n",
    "ai_nonods_high_300_vols=[]\n",
    "\n",
    "ai_nonods_low_30_100_vols=[]\n",
    "ai_nonods_low_100_300_vols=[]\n",
    "ai_nonods_low_300_vols=[]\n",
    "\n",
    "#Similarly get numbers for nodules only and for lymph nodes only\n",
    "ai_only_nods_high_30_100_vols=[]\n",
    "ai_only_nods_high_100_300_vols=[]\n",
    "ai_only_nods_high_300_vols=[]\n",
    "\n",
    "ai_lymph_high_30_100_vols=[]\n",
    "ai_lymph_high_100_300_vols=[]\n",
    "ai_lymph_high_300_vols=[]\n",
    "\n",
    "ai_only_nods_low_30_100_vols=[]\n",
    "ai_only_nods_low_100_300_vols=[]\n",
    "ai_only_nods_low_300_vols=[]\n",
    "\n",
    "ai_lymph_low_30_100_vols=[]\n",
    "ai_lymph_low_100_300_vols=[]\n",
    "ai_lymph_low_300_vols=[]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7538cc0",
   "metadata": {},
   "source": [
    "Get numbers of AI detected nodules (FPs based on initial reading) for lymph nodes only, nodules only, and non-nodule categories in low/high BMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81c134d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lymph nodes in high BMI group is 9\n",
      "Total lymph nodes in low BMI group is 5\n"
     ]
    }
   ],
   "source": [
    "#For lymph node subgroup in high/low BMI group from discrepancies\n",
    "\n",
    "for deg in ['high','low']:\n",
    "    total=0 #count total number\n",
    "\n",
    "    for pat in eval('lymph_FP_wrong_'+deg+'_ids'): #loop over participants\n",
    "\n",
    "        for nod_id in eval('lymph_FP_wrong_'+deg+'_ids[pat]'): #Loop over nodule ids\n",
    "\n",
    "            if deg=='high':\n",
    "                vol=float(eval(high_dict_vol[pat][nod_id-1])) #Get volume of that nodule id\n",
    "            else: #For BMI groups volume will be taken from the corresponding degree of that participant\n",
    "                vol=float(eval(low_dict_vol[pat][nod_id-1])) #Get volume of that nodule id\n",
    "\n",
    "            #Increase the number of findings of a specific volume subgroup depending on volume of finding - Add volume to the corresponding variable\n",
    "            if vol>=30 and vol<=100:\n",
    "                exec('ai_lymph_'+deg+'_30_100=ai_lymph_'+deg+'_30_100+1')\n",
    "                exec('ai_lymph_'+deg+'_30_100_vols.append(vol)')\n",
    "                total=total+1\n",
    "            elif vol>100 and vol<=300:\n",
    "                exec('ai_lymph_'+deg+'_100_300=ai_lymph_'+deg+'_100_300+1')\n",
    "                exec('ai_lymph_'+deg+'_100_300_vols.append(vol)')\n",
    "                total=total+1\n",
    "            elif vol>300:\n",
    "                exec('ai_lymph_'+deg+'_300=ai_lymph_'+deg+'_300+1') \n",
    "                exec('ai_lymph_'+deg+'_300_vols.append(vol)')\n",
    "                total=total+1\n",
    "            else:\n",
    "                print('For participant {} volume is smaller than 30mm3',pat)\n",
    "\n",
    "    print('Total lymph nodes in {} BMI group is {}'.format(deg,total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a35d39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodules in high BMI group is 22\n",
      "Total nodules in low BMI group is 17\n"
     ]
    }
   ],
   "source": [
    "#Similarly for nodule only group high/low BMI from discrepancies\n",
    "\n",
    "for deg in ['high','low']:\n",
    "\n",
    "    total=0\n",
    "\n",
    "    for pat in eval('nod_FP_wrong_'+deg+'_ids'):\n",
    "\n",
    "        for nod_id in eval('nod_FP_wrong_'+deg+'_ids[pat]'):\n",
    "\n",
    "            if deg=='high':\n",
    "                vol=float(high_dict_vol[pat][nod_id-1])\n",
    "            else:\n",
    "                vol=float(low_dict_vol[pat][nod_id-1])\n",
    "\n",
    "            if vol>=30 and vol<=100:\n",
    "                exec('ai_only_nods_'+deg+'_30_100=ai_only_nods_'+deg+'_30_100+1')\n",
    "                exec('ai_only_nods_'+deg+'_30_100_vols.append(vol)')\n",
    "                total=total+1\n",
    "            elif vol>100 and vol<=300:\n",
    "                exec('ai_only_nods_'+deg+'_100_300=ai_only_nods_'+deg+'_100_300+1')\n",
    "                exec('ai_only_nods_'+deg+'_100_300_vols.append(vol)')\n",
    "                total=total+1\n",
    "            elif vol>300:\n",
    "                exec('ai_only_nods_'+deg+'_300=ai_only_nods_'+deg+'_300+1') \n",
    "                exec('ai_only_nods_'+deg+'_300_vols.append(vol)')\n",
    "                total=total+1\n",
    "            else:\n",
    "                print('For participant {} volume is smaller than 30mm3',pat)\n",
    "                \n",
    "    print('Total nodules in {} BMI group is {}'.format(deg,total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d2c002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total non-nodules in low BMI group is 96\n",
      "Total non-nodules in high BMI group is 53\n"
     ]
    }
   ],
   "source": [
    "#Similarly for non-nodule low/high BMI groups from discrepancies\n",
    "\n",
    "for deg in ['low','high']:\n",
    "    total=0\n",
    "\n",
    "    for pat in eval('dict_FP_correct_'+deg+'_ids'):\n",
    "\n",
    "        for nod_id in eval('dict_FP_correct_'+deg+'_ids[pat]'):\n",
    "\n",
    "            if deg=='high':\n",
    "                vol=float(high_dict_vol[pat][nod_id-1])\n",
    "            else:\n",
    "                vol=float(low_dict_vol[pat][nod_id-1])\n",
    "\n",
    "            if vol>=30 and vol<=100:\n",
    "                exec('ai_nonods_'+deg+'_30_100=ai_nonods_'+deg+'_30_100+1')\n",
    "                exec('ai_nonods_'+deg+'_30_100_vols.append(vol)')\n",
    "                total=total+1\n",
    "            elif vol>100 and vol<=300:\n",
    "                exec('ai_nonods_'+deg+'_100_300=ai_nonods_'+deg+'_100_300+1')\n",
    "                exec('ai_nonods_'+deg+'_100_300_vols.append(vol)')\n",
    "                total=total+1\n",
    "            elif vol>300:\n",
    "                exec('ai_nonods_'+deg+'_300=ai_nonods_'+deg+'_300+1') \n",
    "                exec('ai_nonods_'+deg+'_300_vols.append(vol)')\n",
    "                total=total+1   \n",
    "            else:\n",
    "                print('For participant {} volume is smaller than 30mm3',pat)\n",
    "\n",
    "    print('Total non-nodules in {} BMI group is {}'.format(deg,total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a968e52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total FP wrong for low BMI are: 22\n",
      "Total FP wrong for high BMI are: 31\n",
      "Total FP correct for low BMI are: 96\n",
      "Total FP correct for high BMI are: 53\n"
     ]
    }
   ],
   "source": [
    "FP_wrong_low=np.sum([len(x) for x in dict_FP_wrong_low.values()])\n",
    "FP_wrong_high=np.sum([len(x) for x in dict_FP_wrong_high.values()])\n",
    "print(\"Total FP wrong for low BMI are:\",FP_wrong_low)\n",
    "print(\"Total FP wrong for high BMI are:\",FP_wrong_high)\n",
    "\n",
    "FP_correct_low=np.sum([len(x) for x in dict_FP_correct_low.values()])\n",
    "FP_correct_high=np.sum([len(x) for x in dict_FP_correct_high.values()])\n",
    "print(\"Total FP correct for low BMI are:\",FP_correct_low)\n",
    "print(\"Total FP correct for high BMI are:\",FP_correct_high)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0dd99749",
   "metadata": {},
   "source": [
    "Statistics for FPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b600b9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "low=low.dropna(axis=1, how='all') \n",
    "high=high.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c079c8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select rows with participant ids and create new cols with the total number of FPs and FNs for each participant\n",
    "low_all=low[~low['participant_id'].isnull()]\n",
    "low_all['fp_all']=0\n",
    "low_all['fn_all']=0\n",
    "low_all['fp_30_100']=0\n",
    "low_all['fp_100_300']=0\n",
    "low_all['fp_300']=0\n",
    "low_all['fn_30_100']=0\n",
    "low_all['fn_100_300']=0\n",
    "low_all['fn_300']=0\n",
    "\n",
    "high_all=high[~high['participant_id'].isnull()]\n",
    "high_all['fp_all']=0\n",
    "high_all['fn_all']=0\n",
    "high_all['fp_30_100']=0\n",
    "high_all['fp_100_300']=0\n",
    "high_all['fp_300']=0\n",
    "high_all['fn_30_100']=0\n",
    "high_all['fn_100_300']=0\n",
    "high_all['fn_300']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5d15e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep only valid IDs\n",
    "low_all['participant_id']=[int(str(pat)[:6]) for pat in low_all['participant_id']]\n",
    "high_all['participant_id']=[int(str(pat)[:6]) for pat in high_all['participant_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reader's FP (based on consensus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715b7c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through all participants and add the number of FPs for AI for each participant (based on consensus review)\n",
    "for ind,pat in enumerate(high['participant_id']):\n",
    "    try: #avoid nan\n",
    "        if int(str(pat)[:6]) in (list(other_nonodules_FP_high.keys())):            \n",
    "            high_all.loc[ind,'fp_all']=high_all.loc[ind,'fp_all']+len(other_nonodules_FP_high[int(str(pat)[:6])])\n",
    "        if int(str(pat)[:6]) in list(fibrosis_FP_high.keys()):\n",
    "            high_all.loc[ind,'fp_all']=high_all.loc[ind,'fp_all']+len(fibrosis_FP_high[int(str(pat)[:6])])\n",
    "    except:\n",
    "        try:\n",
    "            if int(str(pat)[:6]) in list(fibrosis_FP_high.keys()):\n",
    "                high_all.loc[ind,'fp_all']=high_all.loc[ind,'fp_all']+len(fibrosis_FP_high[int(str(pat)[:6])])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "for ind,pat in enumerate(low['participant_id']):\n",
    "    try: #avoid nan\n",
    "        if int(str(pat)[:6]) in (list(other_nonodules_FP_low.keys())):\n",
    "            low_all.loc[ind,'fp_all']=low_all.loc[ind,'fp_all']+len(other_nonodules_FP_low[int(str(pat)[:6])])\n",
    "        if int(str(pat)[:6]) in list(fibrosis_FP_low.keys()):\n",
    "            low_all.loc[ind,'fp_all']=low_all.loc[ind,'fp_all']+len(fibrosis_FP_low[int(str(pat)[:6])])\n",
    "    except:\n",
    "        try:\n",
    "            if int(str(pat)[:6]) in list(fibrosis_FP_low.keys()):\n",
    "                low_all.loc[ind,'fp_all']=low_all.loc[ind,'fp_all']+len(fibrosis_FP_low[int(str(pat)[:6])])\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4c6cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through all participants and add the number of FPs for the reader for each participant (based on consensus review)\n",
    "for ind,pat in enumerate(high['participant_id']):\n",
    "    try: #avoid nan\n",
    "        if int(str(pat)[:6]) in (list(other_nonodules_FN_high.keys())):            \n",
    "            high_all.loc[ind,'fn_all']=high_all.loc[ind,'fn_all']+len(other_nonodules_FN_high[int(str(pat)[:6])])\n",
    "        if int(str(pat)[:6]) in list(fibrosis_FN_high.keys()):\n",
    "            high_all.loc[ind,'fn_all']=high_all.loc[ind,'fn_all']+len(fibrosis_FN_high[int(str(pat)[:6])])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for ind,pat in enumerate(low['participant_id']):\n",
    "    try: #avoid nan\n",
    "        if int(str(pat)[:6]) in (list(other_nonodules_FN_low.keys())):            \n",
    "            low_all.loc[ind,'fn_all']=low_all.loc[ind,'fn_all']+len(other_nonodules_FN_low[int(str(pat)[:6])])\n",
    "        if int(str(pat)[:6]) in list(fibrosis_FN_low.keys()):\n",
    "            low_all.loc[ind,'fn_all']=low_all.loc[ind,'fn_all']+len(fibrosis_FN_low[int(str(pat)[:6])])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e66731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of FPs for AI in high BMI cases: 53\n",
      "Num of FPs for AI in low BMI cases: 96\n",
      "Num of FPs for reader in high BMI cases: 9\n",
      "Num of FPs for reader in low BMI cases: 28\n"
     ]
    }
   ],
   "source": [
    "print(\"Num of FPs for AI in high BMI cases:\",np.sum(high_all['fp_all']))\n",
    "print(\"Num of FPs for AI in low BMI cases:\",np.sum(low_all['fp_all']))\n",
    "\n",
    "print(\"Num of FPs for reader in high BMI cases:\",np.sum(high_all['fn_all']))\n",
    "print(\"Num of FPs for reader in low BMI cases:\",np.sum(low_all['fn_all']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b326d374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Not used for now\n",
    "# print('Paired T-test')\n",
    "# print(\"High BMI Reader vs AI:\",stats.ttest_rel(high_all['fn_all'], high_all['fp_all']).pvalue)\n",
    "# print(\"Low BMI Reader vs AI:\",stats.ttest_rel(low_all['fn_all'], low_all['fp_all']).pvalue) \n",
    "# print('\\n')\n",
    "\n",
    "# print('T-test of independent samples')\n",
    "# print(\"High BMI Reader vs AI:\",stats.ttest_ind(high_all['fn_all'], high_all['fp_all']).pvalue)\n",
    "# print(\"Low BMI Reader vs AI:\",stats.ttest_ind(low_all['fn_all'], low_all['fp_all']).pvalue) \n",
    "# print('\\n')\n",
    "\n",
    "# print(\"Below only possible is independent samples t-test. Paired t-test does not make sense here.\")\n",
    "# print(\"Low BMI vs High BMI for reader\",stats.ttest_ind(high_all['fn_all'], low_all['fn_all']).pvalue)\n",
    "# print(\"Low BMI vs High BMI for AI\",stats.ttest_ind(high_all['fp_all'], low_all['fp_all']).pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cfbef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High BMI vs Low BMI for reader 0.08938425827413506\n",
      "High BMI vs Low BMI for AI 0.004844752004185089\n",
      "Reader vs AI for low BMI 1.2148044683373157e-09\n",
      "Reader vs AI for high BMI 1.4409608525112892e-07\n"
     ]
    }
   ],
   "source": [
    "# # conduct the Wilcoxon-Signed Rank Test\n",
    "# print(\"High BMI Reader vs AI:\",stats.wilcoxon(high_all['fn_all'], high_all['fp_all']).pvalue)\n",
    "# print(\"Low BMI Reader vs AI:\",stats.wilcoxon(low_all['fn_all'], low_all['fp_all']).pvalue) \n",
    "# print('\\n')\n",
    "\n",
    "#For unequal sample size Mann-Whitney U test is used\n",
    "print(\"High BMI vs Low BMI for reader\",stats.mannwhitneyu(high_all['fn_all'], low_all['fn_all']).pvalue)\n",
    "print(\"High BMI vs Low BMI for AI\",stats.mannwhitneyu(high_all['fp_all'], low_all['fp_all']).pvalue)\n",
    "\n",
    "print(\"Reader vs AI for low BMI\",stats.mannwhitneyu(low_all['fn_all'], low_all['fp_all']).pvalue)\n",
    "print(\"Reader vs AI for high BMI\",stats.mannwhitneyu(high_all['fn_all'], high_all['fp_all']).pvalue)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e16c34f7",
   "metadata": {},
   "source": [
    "Statistics for FP for AI volume subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1663b13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low\n",
      "high\n"
     ]
    }
   ],
   "source": [
    "for bmi in ['low','high']:\n",
    "    print(bmi)\n",
    "\n",
    "    for pat in eval('dict_FP_correct_'+bmi+'_ids'):\n",
    "\n",
    "            for nod_id in eval('dict_FP_correct_'+bmi+'_ids[pat]'):\n",
    "\n",
    "                if bmi=='low':\n",
    "                    vol=float(low_dict_vol[pat][nod_id-1])\n",
    "                else:\n",
    "                    vol=float(high_dict_vol[pat][nod_id-1])\n",
    "\n",
    "                if vol>=30 and vol<=100:\n",
    "                    exec(\"index=\"+bmi+\"_all[\"+bmi+\"_all['participant_id']==int(pat)].index[0]\")\n",
    "                    exec(bmi+\"_all.iloc[index,\"+bmi+\"_all.columns.get_loc('fp_30_100')]=\"+bmi+\"_all.iloc[index,\"+bmi+\"_all.columns.get_loc('fp_30_100')]+1\")\n",
    "\n",
    "                elif vol>100 and vol<=300:\n",
    "                    exec(\"index=\"+bmi+\"_all[\"+bmi+\"_all['participant_id']==int(pat)].index[0]\")\n",
    "                    exec(bmi+\"_all.iloc[index,\"+bmi+\"_all.columns.get_loc('fp_100_300')]=\"+bmi+\"_all.iloc[index,\"+bmi+\"_all.columns.get_loc('fp_100_300')]+1\")\n",
    "                elif vol>300:\n",
    "                    exec(\"index=\"+bmi+\"_all[\"+bmi+\"_all['participant_id']==int(pat)].index[0]\")\n",
    "                    exec(bmi+\"_all.iloc[index,\"+bmi+\"_all.columns.get_loc('fp_300')]=\"+bmi+\"_all.iloc[index,\"+bmi+\"_all.columns.get_loc('fp_300')]+1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9007b12f",
   "metadata": {},
   "source": [
    "##### AI missed, reader found\n",
    "\n",
    "Before running part below we should execute the other file ('patient_selection_emphysema_experiment.ipynb') to get dictionaries containing information about the ids of FNs. We need REDCap information to extract those \n",
    "\n",
    "Up until here there are 8 files generated that will be used by the other notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf91d625",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run other notebook and continue execution on next cell if it gives error\n",
    "\n",
    "try: #To ignore error and continue in next cell we need try-except and 'no raise error' flag\n",
    "    %run ./patient_selection_BMI_experiment.ipynb --no-raise-error\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef73266a",
   "metadata": {},
   "source": [
    "FP for reader's volume subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c83312ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_FN_wrong_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "968ece1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_FN_wrong_low_vols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09403cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low\n",
      "high\n"
     ]
    }
   ],
   "source": [
    "for bmi in ['low','high']: \n",
    "    print(bmi)\n",
    "\n",
    "    for pat in eval('dict_FN_wrong_'+bmi+'_ids'):\n",
    "\n",
    "            for nod_id,_ in enumerate(eval('dict_FN_wrong_'+bmi+'_ids[pat]')):\n",
    "\n",
    "                vol=float(eval('dict_FN_wrong_'+bmi+'_vols[pat][nod_id]'))\n",
    "\n",
    "                if vol>=30 and vol<=100:\n",
    "\n",
    "                    exec(\"index=\"+bmi+\"_all[\"+bmi+\"_all['participant_id']==int(pat)].index[0]\")\n",
    "                    exec(bmi+\"_all.iloc[index,\"+bmi+\"_all.columns.get_loc('fn_30_100')]=\"+bmi+\"_all.iloc[index,\"+bmi+\"_all.columns.get_loc('fn_30_100')]+1\")\n",
    "\n",
    "                elif vol>100 and vol<=300:\n",
    "\n",
    "                    exec(\"index=\"+bmi+\"_all[\"+bmi+\"_all['participant_id']==int(pat)].index[0]\")\n",
    "                    exec(bmi+\"_all.iloc[index,\"+bmi+\"_all.columns.get_loc('fn_100_300')]=\"+bmi+\"_all.iloc[index,\"+bmi+\"_all.columns.get_loc('fn_100_300')]+1\")\n",
    "\n",
    "                elif vol>300:\n",
    "\n",
    "                    exec(\"index=\"+bmi+\"_all[\"+bmi+\"_all['participant_id']==int(pat)].index[0]\")\n",
    "                    exec(bmi+\"_all.iloc[index,\"+bmi+\"_all.columns.get_loc('fn_300')]=\"+bmi+\"_all.iloc[index,\"+bmi+\"_all.columns.get_loc('fn_300')]+1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d600c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP AI low 96\n",
      "FP AI high 53\n",
      "FN read low 28\n",
      "FN read high 9\n"
     ]
    }
   ],
   "source": [
    "assert list(low_all['fp_all'])==list(low_all['fp_30_100']+low_all['fp_100_300']+low_all['fp_300'])\n",
    "assert list(low_all['fn_all'])==list(low_all['fn_30_100']+low_all['fn_100_300']+low_all['fn_300'])\n",
    "assert list(high_all['fp_all'])==list(high_all['fp_30_100']+high_all['fp_100_300']+high_all['fp_300'])\n",
    "assert list(high_all['fn_all'])==list(high_all['fn_30_100']+high_all['fn_100_300']+high_all['fn_300'])\n",
    "\n",
    "assert np.sum(low_all['fp_all'])==np.sum(low_all['fp_30_100'])+np.sum(low_all['fp_100_300'])+np.sum(low_all['fp_300'])\n",
    "assert np.sum(low_all['fn_all'])==np.sum(low_all['fn_30_100'])+np.sum(low_all['fn_100_300'])+np.sum(low_all['fn_300'])\n",
    "assert np.sum(high_all['fp_all'])==np.sum(high_all['fp_30_100'])+np.sum(high_all['fp_100_300'])+np.sum(high_all['fp_300'])\n",
    "assert np.sum(high_all['fn_all'])==np.sum(high_all['fn_30_100'])+np.sum(high_all['fn_100_300'])+np.sum(high_all['fn_300'])\n",
    "\n",
    "print(\"FP AI low\",np.sum(low_all['fp_all']))\n",
    "print(\"FP AI high\",np.sum(high_all['fp_all']))\n",
    "print(\"FN read low\",np.sum(low_all['fn_all']))\n",
    "print(\"FN read high\",np.sum(high_all['fn_all']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577bf4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume subgroup 30-100mm3\n",
      "For unequal sample size Mann-Whitney U test is used\n",
      "Low vs High BMI for reader 0.23676620801434844\n",
      "Low vs High BMI for AI 0.017800661925651263\n",
      "Low BMI Reader vs AI: 0.10057216771517977\n",
      "High BMI Reader vs AI: 0.6221348606202346\n"
     ]
    }
   ],
   "source": [
    "print(\"Volume subgroup 30-100mm3\")\n",
    "# print('Paired T-test')\n",
    "# print(\"High BMI Reader vs AI:\",stats.ttest_rel(high_all['fn_30_100'], high_all['fp_30_100']).pvalue)\n",
    "# print(\"Low BMI Reader vs AI:\",stats.ttest_rel(low_all['fn_30_100'], low_all['fp_30_100']).pvalue) \n",
    "# print('\\n')\n",
    "\n",
    "# print('T-test of independent samples')\n",
    "# print(\"High BMI Reader vs AI:\",stats.ttest_ind(high_all['fn_30_100'], high_all['fp_30_100']).pvalue)\n",
    "# print(\"Low BMI Reader vs AI:\",stats.ttest_ind(low_all['fn_30_100'], low_all['fp_30_100']).pvalue) \n",
    "# print('\\n')\n",
    "\n",
    "# print(\"Below only possible is independent samples t-test. Paired t-test does not make sense here.\")\n",
    "# print(\"Low vs High BMI for reader\",stats.ttest_ind(high_all['fn_30_100'], low_all['fn_30_100']).pvalue)\n",
    "# print(\"Low vs High BMI for AI\",stats.ttest_ind(high_all['fp_30_100'], low_all['fp_30_100']).pvalue)\n",
    "# print('\\n')\n",
    "\n",
    "# print(\"Below Wilcoxon-Signed Rank Test is used\")\n",
    "# print(\"High BMI Reader vs AI:\",stats.wilcoxon(high_all['fn_30_100'], high_all['fp_30_100']).pvalue)\n",
    "# print(\"Low BMI Reader vs AI:\",stats.wilcoxon(low_all['fn_30_100'], low_all['fp_30_100']).pvalue)\n",
    "# print('\\n')\n",
    "\n",
    "print(\"For unequal sample size Mann-Whitney U test is used\")\n",
    "print(\"Low vs High BMI for reader\",stats.mannwhitneyu(high_all['fn_30_100'], low_all['fn_30_100']).pvalue)\n",
    "print(\"Low vs High BMI for AI\",stats.mannwhitneyu(high_all['fp_30_100'], low_all['fp_30_100']).pvalue)\n",
    "\n",
    "print(\"Low BMI Reader vs AI:\",stats.mannwhitneyu(low_all['fn_30_100'], low_all['fp_30_100']).pvalue)\n",
    "print(\"High BMI Reader vs AI:\",stats.mannwhitneyu(high_all['fn_30_100'], high_all['fp_30_100']).pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddbe26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume subgroup 100-300mm3\n",
      "For unequal sample size Mann-Whitney U test is used\n",
      "Low vs High BMI for reader 0.1772312071394676\n",
      "Low vs High BMI for AI 0.06440207310333836\n",
      "Low BMI Reader vs AI: 4.8693462975873924e-08\n",
      "High BMI Reader vs AI: 1.8716855511876229e-06\n"
     ]
    }
   ],
   "source": [
    "print(\"Volume subgroup 100-300mm3\")\n",
    "# print('Paired T-test')\n",
    "# print(\"High BMI Reader vs AI:\",stats.ttest_rel(high_all['fn_100_300'], high_all['fp_100_300']).pvalue)\n",
    "# print(\"Low BMI Reader vs AI:\",stats.ttest_rel(low_all['fn_100_300'], low_all['fp_100_300']).pvalue) \n",
    "# print('\\n')\n",
    "\n",
    "# print('T-test of independent samples')\n",
    "# print(\"High BMI Reader vs AI:\",stats.ttest_ind(high_all['fn_100_300'], high_all['fp_100_300']).pvalue)\n",
    "# print(\"Low BMI Reader vs AI:\",stats.ttest_ind(low_all['fn_100_300'], low_all['fp_100_300']).pvalue) \n",
    "# print('\\n')\n",
    "\n",
    "# print(\"Below only possible is independent samples t-test. Paired t-test does not make sense here.\")\n",
    "# print(\"Low BMI vs High BMI for reader\",stats.ttest_ind(high_all['fn_100_300'], low_all['fn_100_300']).pvalue)\n",
    "# print(\"Low BMI vs High BMI for AI\",stats.ttest_ind(high_all['fp_100_300'], low_all['fp_100_300']).pvalue)\n",
    "# print('\\n')\n",
    "\n",
    "# print(\"Below Wilcoxon-Signed Rank Test is used\")\n",
    "# print(\"High BMI Reader vs AI:\",stats.wilcoxon(high_all['fn_100_300'], high_all['fp_100_300']).pvalue)\n",
    "# print(\"Low BMI Reader vs AI:\",stats.wilcoxon(low_all['fn_100_300'], low_all['fp_100_300']).pvalue)\n",
    "# print('\\n')\n",
    "\n",
    "print(\"For unequal sample size Mann-Whitney U test is used\")\n",
    "print(\"Low vs High BMI for reader\",stats.mannwhitneyu(high_all['fn_100_300'], low_all['fn_100_300']).pvalue)\n",
    "print(\"Low vs High BMI for AI\",stats.mannwhitneyu(high_all['fp_100_300'], low_all['fp_100_300']).pvalue)\n",
    "\n",
    "print(\"Low BMI Reader vs AI:\",stats.mannwhitneyu(low_all['fn_100_300'], low_all['fp_100_300']).pvalue)\n",
    "print(\"High BMI Reader vs AI:\",stats.mannwhitneyu(high_all['fn_100_300'], high_all['fp_100_300']).pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fcbc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume subgroup 300+mm3\n",
      "For unequal sample size Mann-Whitney U test is used\n",
      "Low vs High BMI for reader 0.3200679866998154\n",
      "Low vs High BMI for AI 0.08575824039743388\n",
      "Low BMI Reader vs AI: 1.8788003562302944e-06\n",
      "High BMI Reader vs AI: 0.00013830624677904377\n"
     ]
    }
   ],
   "source": [
    "print(\"Volume subgroup 300+mm3\")\n",
    "# print('Paired T-test')\n",
    "# print(\"High BMI Reader vs AI:\",stats.ttest_rel(high_all['fn_300'], high_all['fp_300']).pvalue)\n",
    "# print(\"Low BMI Reader vs AI:\",stats.ttest_rel(low_all['fn_300'], low_all['fp_300']).pvalue) \n",
    "# print('\\n')\n",
    "\n",
    "# print('T-test of independent samples')\n",
    "# print(\"High BMI Reader vs AI:\",stats.ttest_ind(high_all['fn_300'], high_all['fp_300']).pvalue)\n",
    "# print(\"Low BMI Reader vs AI:\",stats.ttest_ind(low_all['fn_300'], low_all['fp_300']).pvalue) \n",
    "# print('\\n')\n",
    "\n",
    "# print(\"Below only possible is independent samples t-test. Paired t-test does not make sense here.\")\n",
    "# print(\"Low BMI vs High BMI for reader\",stats.ttest_ind(high_all['fn_300'], low_all['fn_300']).pvalue)\n",
    "# print(\"Low BMI vs High BMI for AI\",stats.ttest_ind(high_all['fp_300'], low_all['fp_300']).pvalue)\n",
    "# print('\\n')\n",
    "\n",
    "# print(\"Below Wilcoxon-Signed Rank Test is used\")\n",
    "# print(\"High BMI Reader vs AI:\",stats.wilcoxon(high_all['fn_300'], high_all['fp_300']).pvalue)\n",
    "# print(\"Low BMI Reader vs AI:\",stats.wilcoxon(low_all['fn_300'], low_all['fp_300']).pvalue)\n",
    "# print('\\n')\n",
    "\n",
    "print(\"For unequal sample size Mann-Whitney U test is used\")\n",
    "print(\"Low vs High BMI for reader\",stats.mannwhitneyu(high_all['fn_300'], low_all['fn_300']).pvalue)\n",
    "print(\"Low vs High BMI for AI\",stats.mannwhitneyu(high_all['fp_300'], low_all['fp_300']).pvalue)\n",
    "\n",
    "print(\"Low BMI Reader vs AI:\",stats.mannwhitneyu(low_all['fn_300'], low_all['fp_300']).pvalue)\n",
    "print(\"High BMI Reader vs AI:\",stats.mannwhitneyu(high_all['fn_300'], high_all['fp_300']).pvalue)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98c9822f",
   "metadata": {},
   "source": [
    "Load dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8219c016",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load ids of FNs\n",
    "\n",
    "with open('dict_FN_wrong_low_ids.pickle', 'rb') as f:\n",
    "    dict_FN_wrong_low_ids = pickle.load(f)\n",
    "\n",
    "with open('dict_FN_correct_low_ids.pickle', 'rb') as f:\n",
    "    dict_FN_correct_low_ids = pickle.load(f)\n",
    "\n",
    "with open('dict_FN_wrong_high_ids.pickle', 'rb') as f:\n",
    "    dict_FN_wrong_high_ids = pickle.load(f)\n",
    "    \n",
    "with open('dict_FN_correct_high_ids.pickle', 'rb') as f:\n",
    "    dict_FN_correct_high_ids = pickle.load(f)\n",
    "    \n",
    "    \n",
    "#Same for their vols\n",
    "\n",
    "with open('dict_FN_wrong_low_vols.pickle', 'rb') as f:\n",
    "    dict_FN_wrong_low_vols = pickle.load(f)\n",
    "\n",
    "with open('dict_FN_correct_low_vols.pickle', 'rb') as f:\n",
    "    dict_FN_correct_low_vols = pickle.load(f)\n",
    "\n",
    "with open('dict_FN_wrong_high_vols.pickle', 'rb') as f:\n",
    "    dict_FN_wrong_high_vols = pickle.load(f)\n",
    "    \n",
    "with open('dict_FN_correct_high_vols.pickle', 'rb') as f:\n",
    "    dict_FN_correct_high_vols = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80fa5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similarly for lymph nodes and nodules only and of their ids and volumes\n",
    "\n",
    "with open('lymph_FN_correct_low.pickle','rb') as f:\n",
    "    lymph_FN_correct_low=pickle.load(f)    \n",
    "\n",
    "with open('lymph_FN_correct_high.pickle','rb') as f:\n",
    "    lymph_FN_correct_high=pickle.load(f) \n",
    "    \n",
    "with open('nod_FN_correct_low.pickle','rb') as f:\n",
    "    nod_FN_correct_low=pickle.load(f)    \n",
    "\n",
    "with open('nod_FN_correct_high.pickle','rb') as f:\n",
    "    nod_FN_correct_high=pickle.load(f) \n",
    "    \n",
    "    \n",
    "with open('lymph_FN_correct_low_ids.pickle','rb') as f:\n",
    "    lymph_FN_correct_low_ids=pickle.load(f)    \n",
    "\n",
    "with open('lymph_FN_correct_high_ids.pickle','rb') as f:\n",
    "    lymph_FN_correct_high_ids=pickle.load(f) \n",
    "    \n",
    "with open('nod_FN_correct_low_ids.pickle','rb') as f:\n",
    "    nod_FN_correct_low_ids=pickle.load(f)    \n",
    "\n",
    "with open('nod_FN_correct_high_ids.pickle','rb') as f:\n",
    "    nod_FN_correct_high_ids=pickle.load(f) \n",
    "    \n",
    "    \n",
    "with open('lymph_FN_correct_low_vols.pickle','rb') as f:\n",
    "    lymph_FN_correct_low_vols=pickle.load(f)    \n",
    "\n",
    "with open('lymph_FN_correct_high_vols.pickle','rb') as f:\n",
    "    lymph_FN_correct_high_vols=pickle.load(f) \n",
    "    \n",
    "with open('nod_FN_correct_low_vols.pickle','rb') as f:\n",
    "    nod_FN_correct_low_vols=pickle.load(f)    \n",
    "\n",
    "with open('nod_FN_correct_high_vols.pickle','rb') as f:\n",
    "    nod_FN_correct_high_vols=pickle.load(f)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12dace4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize zero values for non-nodules, nodules only, and lymph nodes for each volume subgroup and for each low/high BMI groups\n",
    "\n",
    "reader_nonods_high_30_100=0\n",
    "reader_nonods_high_100_300=0\n",
    "reader_nonods_high_300=0\n",
    "\n",
    "reader_nonods_low_30_100=0\n",
    "reader_nonods_low_100_300=0\n",
    "reader_nonods_low_300=0\n",
    "\n",
    "\n",
    "reader_only_nods_high_30_100=0\n",
    "reader_only_nods_high_100_300=0\n",
    "reader_only_nods_high_300=0\n",
    "\n",
    "reader_only_nods_low_30_100=0\n",
    "reader_only_nods_low_100_300=0\n",
    "reader_only_nods_low_300=0\n",
    "\n",
    "\n",
    "reader_lymph_high_30_100=0\n",
    "reader_lymph_high_100_300=0\n",
    "reader_lymph_high_300=0\n",
    "\n",
    "reader_lymph_low_30_100=0\n",
    "reader_lymph_low_100_300=0\n",
    "reader_lymph_low_300=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43c205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similarly keep track of volumes for each of those groups (if Mann-Whitney U test is used below)\n",
    "reader_nonods_high_30_100_vols=[]\n",
    "reader_nonods_high_100_300_vols=[]\n",
    "reader_nonods_high_300_vols=[]\n",
    "reader_nonods_low_30_100_vols=[]\n",
    "reader_nonods_low_100_300_vols=[]\n",
    "reader_nonods_low_300_vols=[]\n",
    "\n",
    "reader_only_nods_high_30_100_vols=[]\n",
    "reader_only_nods_high_100_300_vols=[]\n",
    "reader_only_nods_high_300_vols=[]\n",
    "reader_only_nods_low_30_100_vols=[]\n",
    "reader_only_nods_low_100_300_vols=[]\n",
    "reader_only_nods_low_300_vols=[]\n",
    "\n",
    "reader_lymph_high_30_100_vols=[]\n",
    "reader_lymph_high_100_300_vols=[]\n",
    "reader_lymph_high_300_vols=[]\n",
    "reader_lymph_low_30_100_vols=[]\n",
    "reader_lymph_low_100_300_vols=[]\n",
    "reader_lymph_low_300_vols=[]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7768571",
   "metadata": {},
   "source": [
    "Get numbers of reader nodules for lymph nodes only, nodules only, and non-nodule categories in low/high BMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8efdd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total non-nodules in high BMI group is 9\n",
      "Total non-nodules in low BMI group is 28\n"
     ]
    }
   ],
   "source": [
    "#Similarly for non-nodule low/high BMI groups for FNs\n",
    "\n",
    "for deg in ['high','low']:\n",
    "    total=0\n",
    "    for pat in eval('dict_FN_wrong_'+deg+'_ids'):\n",
    "        for ind,nod_id in enumerate(eval('dict_FN_wrong_'+deg+'_ids[pat]')):\n",
    "\n",
    "            if deg=='high':\n",
    "                vol=float(dict_FN_wrong_high_vols[pat][ind])\n",
    "            else:\n",
    "                vol=float(dict_FN_wrong_low_vols[pat][ind])\n",
    "\n",
    "\n",
    "            if vol>=30 and vol<=100:\n",
    "                exec('reader_nonods_'+deg+'_30_100=reader_nonods_'+deg+'_30_100+1')\n",
    "                exec('reader_nonods_'+deg+'_30_100_vols.append(vol)')\n",
    "                total+=1\n",
    "\n",
    "            elif vol>100 and vol<=300:\n",
    "                exec('reader_nonods_'+deg+'_100_300=reader_nonods_'+deg+'_100_300+1')\n",
    "                exec('reader_nonods_'+deg+'_100_300_vols.append(vol)')\n",
    "                total+=1\n",
    "            elif vol>300:\n",
    "                exec('reader_nonods_'+deg+'_300=reader_nonods_'+deg+'_300+1') \n",
    "                exec('reader_nonods_'+deg+'_300_vols.append(vol)') \n",
    "                total+=1\n",
    "            else:\n",
    "                print('For participant {} volume is smaller than 30mm3',pat)\n",
    "\n",
    "    print('Total non-nodules in {} BMI group is {}'.format(deg,total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99be2e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lymph nodes in high BMI group is 20\n",
      "Total lymph nodes in low BMI group is 23\n"
     ]
    }
   ],
   "source": [
    "#Similarly for lymph nodes low/high BMI groups for FNs\n",
    "\n",
    "for deg in ['high','low']:\n",
    "    total=0\n",
    "    for pat in eval('lymph_FN_correct_'+deg+'_ids'):\n",
    "        for ind,nod_id in enumerate(eval('lymph_FN_correct_'+deg+'_ids[pat]')):\n",
    "            \n",
    "            if deg=='high':\n",
    "                vol=float(lymph_FN_correct_high_vols[pat][ind])\n",
    "            else:\n",
    "                vol=float(lymph_FN_correct_low_vols[pat][ind])\n",
    "\n",
    "            if vol>=30 and vol<=100:\n",
    "                exec('reader_lymph_'+deg+'_30_100=reader_lymph_'+deg+'_30_100+1')\n",
    "                exec('reader_lymph_'+deg+'_30_100_vols.append(vol)')\n",
    "                total+=1\n",
    "            elif vol>100 and vol<=300:\n",
    "                exec('reader_lymph_'+deg+'_100_300=reader_lymph_'+deg+'_100_300+1')\n",
    "                exec('reader_lymph_'+deg+'_100_300_vols.append(vol)')\n",
    "                total+=1\n",
    "            elif vol>300:\n",
    "                exec('reader_lymph_'+deg+'_300=reader_lymph_'+deg+'_300+1') \n",
    "                exec('reader_lymph_'+deg+'_300_vols.append(vol)')\n",
    "                total+=1\n",
    "            else:\n",
    "                print('For participant {} volume is smaller than 30mm3',pat)\n",
    "\n",
    "    print('Total lymph nodes in {} BMI group is {}'.format(deg,total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d778a2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodules in high BMI group is 13\n",
      "Total nodules in low BMI group is 4\n"
     ]
    }
   ],
   "source": [
    "#Similarly for nodules only low/high BMI groups for FNs\n",
    "for deg in ['high','low']:\n",
    "    total=0\n",
    "    for pat in eval('nod_FN_correct_'+deg+'_ids'):\n",
    "        for ind,nod_id in enumerate(eval('nod_FN_correct_'+deg+'_ids[pat]')):\n",
    "\n",
    "            if deg=='high':\n",
    "                vol=float(nod_FN_correct_high_vols[pat][ind])\n",
    "            else:\n",
    "                vol=float(nod_FN_correct_low_vols[pat][ind])\n",
    "\n",
    "            if vol>=30 and vol<=100:\n",
    "                exec('reader_only_nods_'+deg+'_30_100=reader_only_nods_'+deg+'_30_100+1')\n",
    "                exec('reader_only_nods_'+deg+'_30_100_vols.append(vol)')\n",
    "                total+=1\n",
    "            elif vol>100 and vol<=300:\n",
    "                exec('reader_only_nods_'+deg+'_100_300=reader_only_nods_'+deg+'_100_300+1')\n",
    "                exec('reader_only_nods_'+deg+'_100_300_vols.append(vol)')\n",
    "                total+=1\n",
    "            elif vol>300:\n",
    "                exec('reader_only_nods_'+deg+'_300=reader_only_nods_'+deg+'_300+1') \n",
    "                exec('reader_only_nods_'+deg+'_300_vols.append(vol)')\n",
    "                total+=1\n",
    "            else:\n",
    "                print('For participant {} volume is smaller than 30mm3',pat)\n",
    "\n",
    "    print('Total nodules in {} BMI group is {}'.format(deg,total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caution here! 'reader_nods' volume subgroups have both 'only_nods' (nodules+atypical lymph nodes) and 'lymph nodes' (PFNs and bronchovascular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe43e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total number of nodules in each of the high/low groups is the sum of the nodules and lymph nodes in those\n",
    "\n",
    "reader_nods_high_30_100=reader_lymph_high_30_100+reader_only_nods_high_30_100\n",
    "reader_nods_high_100_300=reader_lymph_high_100_300+reader_only_nods_high_100_300\n",
    "reader_nods_high_300=reader_lymph_high_300+reader_only_nods_high_300\n",
    "\n",
    "reader_nods_low_30_100=reader_lymph_low_30_100+reader_only_nods_low_30_100\n",
    "reader_nods_low_100_300=reader_lymph_low_100_300+reader_only_nods_low_100_300\n",
    "reader_nods_low_300=reader_lymph_low_300+reader_only_nods_low_300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44d51b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[85.0, 54.0, 38.0, 49.0, 49.0, 46.0, 91.0, 30.0, 37.0, 42.0, 35.0]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader_only_nods_high_30_100_vols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef7698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader_only_nods_high_30_100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af5d3ae1",
   "metadata": {},
   "source": [
    "## Create Tables & Statistics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef8fd251",
   "metadata": {},
   "source": [
    "##### Based on the current definition the following equations hold true:\n",
    "1. AI found nodules, reader missed = FN reader\n",
    "2. AI found non-nodules, reader missed = FP AI\n",
    "3. AI missed nodules, reader found = FN AI\n",
    "4. AI missed non-nodules, reader found = FP reader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c07312b8",
   "metadata": {},
   "source": [
    "#### Low BMI non-nodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8ae87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fibrosis/scar low FP: 40\n",
      "Other non-nodules low FP: 56\n",
      "Other non-nodules low FP (lung): 44\n",
      "Other non-nodules low FP (non-lung): 12\n",
      "Fibrosis/scar low FN: 18\n",
      "Other non-nodules low FN: 10\n",
      "Other non-nodules low FN (lung): 10\n",
      "Other non-nodules low FN (non-lung): 0\n"
     ]
    }
   ],
   "source": [
    "#Below are the non-nodule categories. With FP is denoted a finding that was missed by AI, whereas with FN a finding missed by the reader\n",
    "#Transform above dictionaries to numbers to be used below\n",
    "fibrosis_FP_low=sum([len(x) for x in fibrosis_FP_low.values()])\n",
    "other_nonodules_FP_low=sum([len(x) for x in other_nonodules_FP_low.values()])\n",
    "fibrosis_FN_low=sum([len(x) for x in fibrosis_FN_low.values()])\n",
    "other_nonodules_FN_low=sum([len(x) for x in other_nonodules_FN_low.values()])\n",
    "other_nonodules_FN_lung_low=sum([len(x) for x in other_nonodules_FN_lung_low.values()])\n",
    "other_nonodules_FN_nolung_low=sum([len(x) for x in other_nonodules_FN_nolung_low.values()])\n",
    "other_nonodules_FP_lung_low=sum([len(x) for x in other_nonodules_FP_lung_low.values()])\n",
    "other_nonodules_FP_nolung_low=sum([len(x) for x in other_nonodules_FP_nolung_low.values()])\n",
    "\n",
    "#Print the above\n",
    "print('Fibrosis/scar low FP: '+str(fibrosis_FP_low))\n",
    "print('Other non-nodules low FP: '+str(other_nonodules_FP_low))\n",
    "print('Other non-nodules low FP (lung): '+str(other_nonodules_FP_lung_low))\n",
    "print('Other non-nodules low FP (non-lung): '+str(other_nonodules_FP_nolung_low))\n",
    "print('Fibrosis/scar low FN: '+str(fibrosis_FN_low))\n",
    "print('Other non-nodules low FN: '+str(other_nonodules_FN_low))\n",
    "print('Other non-nodules low FN (lung): '+str(other_nonodules_FN_lung_low))\n",
    "print('Other non-nodules low FN (non-lung): '+str(other_nonodules_FN_nolung_low))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ee5f38",
   "metadata": {},
   "source": [
    "Some of the above findings cannot be classified as lung/non-lung findings and might be better to manually checked or add extra conditions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70925dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Incorrectly detected by AI</th>\n",
       "      <th>Incorrectly detected by reader</th>\n",
       "      <th>All findings</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GT by radiologists for discrepancies</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fibrosis/scar low</th>\n",
       "      <td>40 (32.3%)</td>\n",
       "      <td>18 (14.5%)</td>\n",
       "      <td>58 (46.8%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other non-nodules low</th>\n",
       "      <td>56 (45.2%)</td>\n",
       "      <td>10 (8.1%)</td>\n",
       "      <td>66 (53.2%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>96 (77.4%)</td>\n",
       "      <td>28 (22.6%)</td>\n",
       "      <td>124 (100.0%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Incorrectly detected by AI  \\\n",
       "GT by radiologists for discrepancies                              \n",
       "fibrosis/scar low                                    40 (32.3%)   \n",
       "other non-nodules low                                56 (45.2%)   \n",
       "Total                                                96 (77.4%)   \n",
       "\n",
       "                                     Incorrectly detected by reader  \\\n",
       "GT by radiologists for discrepancies                                  \n",
       "fibrosis/scar low                                        18 (14.5%)   \n",
       "other non-nodules low                                     10 (8.1%)   \n",
       "Total                                                    28 (22.6%)   \n",
       "\n",
       "                                      All findings  \n",
       "GT by radiologists for discrepancies                \n",
       "fibrosis/scar low                       58 (46.8%)  \n",
       "other non-nodules low                   66 (53.2%)  \n",
       "Total                                 124 (100.0%)  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Detailed comparison of FP categories for low and high BMI groups (no volume subgroups)\n",
    "\n",
    "df_categories=pd.DataFrame(columns=['Incorrectly detected by AI','Incorrectly detected by reader'], #below index with the correct order as above\n",
    "                          index=['fibrosis/scar low','other non-nodules low'])\n",
    "\n",
    "df_categories.index.name = 'GT by radiologists for discrepancies'\n",
    "\n",
    "df_categories['Incorrectly detected by AI']=[fibrosis_FP_low,other_nonodules_FP_low]\n",
    "\n",
    "df_categories['Incorrectly detected by reader']=[fibrosis_FN_low,other_nonodules_FN_low]\n",
    "\n",
    "df_categories['All findings']=df_categories['Incorrectly detected by AI']+df_categories['Incorrectly detected by reader'] #Sum of findings for each of emph/non-emph categories\n",
    "\n",
    "df_categories.loc['Total']= df_categories.sum() #Total FP findings for AI/reader\n",
    "\n",
    "all_findings=df_categories.iloc[:-1,:-1].sum().sum() #All findings\n",
    "\n",
    "#Add percentages next to the number of each category\n",
    "percentage_fp=np.round((df_categories['Incorrectly detected by AI']/all_findings)*100,1)  \n",
    "df_categories['Incorrectly detected by AI']=[str(value[1])+' ('+str(percentage_fp[index])+'%)' \n",
    "                                             for index,value in enumerate(df_categories['Incorrectly detected by AI'].items())]\n",
    "\n",
    "percentage_fn=np.round((df_categories['Incorrectly detected by reader']/all_findings)*100,1) \n",
    "df_categories['Incorrectly detected by reader']=[str(value[1])+' ('+str(percentage_fn[index])+'%)' \n",
    "                                                 for index,value in enumerate(df_categories['Incorrectly detected by reader'].items())]\n",
    "\n",
    "df_categories['All findings']=[str(val)+' ('+str(np.round(100*val/all_findings,1))+'%)' for val in df_categories['All findings'].values]\n",
    "\n",
    "# #Rename columns\n",
    "# df_categories.rename(columns={'FP': 'Incorrectly detected by AI', 'FN': 'Incorrectly detected by reader'}, inplace=True)\n",
    "\n",
    "df_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56597ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_categories.style.to_latex() #Just as a starting point - Need to be modified manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcede04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categories.to_excel('non_nodules_low.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6cd264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Incorrectly detected by AI</th>\n",
       "      <th>Incorrectly detected by reader</th>\n",
       "      <th>All findings</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GT by radiologists for discrepancies</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fibrosis/scar low</th>\n",
       "      <td>40 (32.3%)</td>\n",
       "      <td>18 (14.5%)</td>\n",
       "      <td>58 (46.8%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other non-nodules lung low</th>\n",
       "      <td>44 (35.5%)</td>\n",
       "      <td>10 (8.1%)</td>\n",
       "      <td>54 (43.5%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other non-nodules nolung low</th>\n",
       "      <td>12 (9.7%)</td>\n",
       "      <td>0 (0.0%)</td>\n",
       "      <td>12 (9.7%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other non-nodules (no description)</th>\n",
       "      <td>0 (0.0%)</td>\n",
       "      <td>0 (0.0%)</td>\n",
       "      <td>0 (0.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>96 (77.4%)</td>\n",
       "      <td>28 (22.6%)</td>\n",
       "      <td>124 (100.0%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Incorrectly detected by AI  \\\n",
       "GT by radiologists for discrepancies                              \n",
       "fibrosis/scar low                                    40 (32.3%)   \n",
       "other non-nodules lung low                           44 (35.5%)   \n",
       "other non-nodules nolung low                          12 (9.7%)   \n",
       "other non-nodules (no description)                     0 (0.0%)   \n",
       "Total                                                96 (77.4%)   \n",
       "\n",
       "                                     Incorrectly detected by reader  \\\n",
       "GT by radiologists for discrepancies                                  \n",
       "fibrosis/scar low                                        18 (14.5%)   \n",
       "other non-nodules lung low                                10 (8.1%)   \n",
       "other non-nodules nolung low                               0 (0.0%)   \n",
       "other non-nodules (no description)                         0 (0.0%)   \n",
       "Total                                                    28 (22.6%)   \n",
       "\n",
       "                                      All findings  \n",
       "GT by radiologists for discrepancies                \n",
       "fibrosis/scar low                       58 (46.8%)  \n",
       "other non-nodules lung low              54 (43.5%)  \n",
       "other non-nodules nolung low             12 (9.7%)  \n",
       "other non-nodules (no description)        0 (0.0%)  \n",
       "Total                                 124 (100.0%)  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Detailed comparison of FP categories for low and high BMI groups (no volume subgroups)\n",
    "\n",
    "df_categories=pd.DataFrame(columns=['Incorrectly detected by AI','Incorrectly detected by reader'], #below index with the correct order as above\n",
    "                          index=['fibrosis/scar low','other non-nodules lung low','other non-nodules nolung low','other non-nodules (no description)'])\n",
    "\n",
    "df_categories.index.name = 'GT by radiologists for discrepancies'\n",
    "\n",
    "rest_no_desc_fp=other_nonodules_FP_low-other_nonodules_FP_lung_low-other_nonodules_FP_nolung_low\n",
    "df_categories['Incorrectly detected by AI']=[fibrosis_FP_low,other_nonodules_FP_lung_low, other_nonodules_FP_nolung_low, rest_no_desc_fp]\n",
    "\n",
    "rest_no_desc_fn=other_nonodules_FN_low-other_nonodules_FN_lung_low-other_nonodules_FN_nolung_low\n",
    "df_categories['Incorrectly detected by reader']=[fibrosis_FN_low,other_nonodules_FN_lung_low, other_nonodules_FN_nolung_low, rest_no_desc_fn]\n",
    "\n",
    "df_categories['All findings']=df_categories['Incorrectly detected by AI']+df_categories['Incorrectly detected by reader'] #Sum of findings for each of emph/non-emph categories\n",
    "\n",
    "df_categories.loc['Total']= df_categories.sum() #Total FP findings for AI/reader\n",
    "\n",
    "all_findings=df_categories.iloc[:-1,:-1].sum().sum() #All findings\n",
    "\n",
    "#Add percentages next to the number of each category\n",
    "percentage_fp=np.round((df_categories['Incorrectly detected by AI']/all_findings)*100,1)  \n",
    "df_categories['Incorrectly detected by AI']=[str(value[1])+' ('+str(percentage_fp[index])+'%)' \n",
    "                                             for index,value in enumerate(df_categories['Incorrectly detected by AI'].items())]\n",
    "\n",
    "percentage_fn=np.round((df_categories['Incorrectly detected by reader']/all_findings)*100,1) \n",
    "df_categories['Incorrectly detected by reader']=[str(value[1])+' ('+str(percentage_fn[index])+'%)' \n",
    "                                                 for index,value in enumerate(df_categories['Incorrectly detected by reader'].items())]\n",
    "\n",
    "df_categories['All findings']=[str(val)+' ('+str(np.round(100*val/all_findings,1))+'%)' for val in df_categories['All findings'].values]\n",
    "\n",
    "# #Rename columns\n",
    "# df_categories.rename(columns={'FP': 'Incorrectly detected by AI', 'FN': 'Incorrectly detected by reader'}, inplace=True)\n",
    "\n",
    "df_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d35b68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_categories.to_excel('non_nodules_types_low.xlsx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d4fc869",
   "metadata": {},
   "source": [
    "#### High BMI non-nodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa8a7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fibrosis/scar high FP: 12\n",
      "Other non-nodules high FP: 41\n",
      "Other non-nodules high FP (lung): 15\n",
      "Other non-nodules high FP (non-lung): 26\n",
      "Fibrosis/scar high FN: 1\n",
      "Other non-nodules high FN: 8\n",
      "Other non-nodules high FN (lung): 8\n",
      "Other non-nodules high FN (non-lung): 0\n"
     ]
    }
   ],
   "source": [
    "#Same as above for high BMI group\n",
    "fibrosis_FP_high=sum([len(x) for x in fibrosis_FP_high.values()])\n",
    "other_nonodules_FP_high=sum([len(x) for x in other_nonodules_FP_high.values()])\n",
    "other_nonodules_FP_lung_high=sum([len(x) for x in other_nonodules_FP_lung_high.values()])\n",
    "other_nonodules_FP_nolung_high=sum([len(x) for x in other_nonodules_FP_nolung_high.values()])\n",
    "fibrosis_FN_high=sum([len(x) for x in fibrosis_FN_high.values()])\n",
    "other_nonodules_FN_high=sum([len(x) for x in other_nonodules_FN_high.values()])\n",
    "other_nonodules_FN_lung_high=sum([len(x) for x in other_nonodules_FN_lung_high.values()])\n",
    "other_nonodules_FN_nolung_high=sum([len(x) for x in other_nonodules_FN_nolung_high.values()])\n",
    "\n",
    "#Print the above\n",
    "print('Fibrosis/scar high FP: '+str(fibrosis_FP_high))\n",
    "print('Other non-nodules high FP: '+str(other_nonodules_FP_high))\n",
    "print('Other non-nodules high FP (lung): '+str(other_nonodules_FP_lung_high))\n",
    "print('Other non-nodules high FP (non-lung): '+str(other_nonodules_FP_nolung_high))\n",
    "print('Fibrosis/scar high FN: '+str(fibrosis_FN_high))\n",
    "print('Other non-nodules high FN: '+str(other_nonodules_FN_high))\n",
    "print('Other non-nodules high FN (lung): '+str(other_nonodules_FN_lung_high))\n",
    "print('Other non-nodules high FN (non-lung): '+str(other_nonodules_FN_nolung_high))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388bda82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Incorrectly detected by AI</th>\n",
       "      <th>Incorrectly detected by reader</th>\n",
       "      <th>All findings</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GT by radiologists for discrepancies</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fibrosis/scar high</th>\n",
       "      <td>12 (19.4%)</td>\n",
       "      <td>1 (1.6%)</td>\n",
       "      <td>13 (21.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other non-nodules high</th>\n",
       "      <td>41 (66.1%)</td>\n",
       "      <td>8 (12.9%)</td>\n",
       "      <td>49 (79.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>53 (85.5%)</td>\n",
       "      <td>9 (14.5%)</td>\n",
       "      <td>62 (100.0%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Incorrectly detected by AI  \\\n",
       "GT by radiologists for discrepancies                              \n",
       "fibrosis/scar high                                   12 (19.4%)   \n",
       "other non-nodules high                               41 (66.1%)   \n",
       "Total                                                53 (85.5%)   \n",
       "\n",
       "                                     Incorrectly detected by reader  \\\n",
       "GT by radiologists for discrepancies                                  \n",
       "fibrosis/scar high                                         1 (1.6%)   \n",
       "other non-nodules high                                    8 (12.9%)   \n",
       "Total                                                     9 (14.5%)   \n",
       "\n",
       "                                     All findings  \n",
       "GT by radiologists for discrepancies               \n",
       "fibrosis/scar high                     13 (21.0%)  \n",
       "other non-nodules high                 49 (79.0%)  \n",
       "Total                                 62 (100.0%)  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Same as above for high\n",
    "\n",
    "#Detailed comparison of FP categories for low and high BMI groups (no volume subgroups)\n",
    "\n",
    "df_categories=pd.DataFrame(columns=['Incorrectly detected by AI','Incorrectly detected by reader'], #below index with the correct order as above\n",
    "                          index=['fibrosis/scar high',\n",
    "                                 'other non-nodules high'\n",
    "                                ])\n",
    "\n",
    "df_categories.index.name = 'GT by radiologists for discrepancies'\n",
    "\n",
    "df_categories['Incorrectly detected by AI']=[fibrosis_FP_high,other_nonodules_FP_high]\n",
    "\n",
    "df_categories['Incorrectly detected by reader']=[fibrosis_FN_high,other_nonodules_FN_high]\n",
    "\n",
    "df_categories['All findings']=df_categories['Incorrectly detected by AI']+df_categories['Incorrectly detected by reader']\n",
    "\n",
    "df_categories.loc['Total']= df_categories.sum()\n",
    "\n",
    "all_findings=df_categories.iloc[:-1,:-1].sum().sum()\n",
    "\n",
    "percentage_fp=np.round((df_categories['Incorrectly detected by AI']/all_findings)*100,1)  \n",
    "df_categories['Incorrectly detected by AI']=[str(value[1])+' ('+str(percentage_fp[index])+'%)' \n",
    "                                             for index,value in enumerate(df_categories['Incorrectly detected by AI'].items())]\n",
    "\n",
    "percentage_fn=np.round((df_categories['Incorrectly detected by reader']/all_findings)*100,1) \n",
    "df_categories['Incorrectly detected by reader']=[str(value[1])+' ('+str(percentage_fn[index])+'%)' \n",
    "                                                 for index,value in enumerate(df_categories['Incorrectly detected by reader'].items())]\n",
    "\n",
    "df_categories['All findings']=[str(val)+' ('+str(np.round(100*val/all_findings,1))+'%)' for val in df_categories['All findings'].values]\n",
    "\n",
    "df_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e78d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categories.to_excel('non_nodules_high.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09580e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Incorrectly detected by AI</th>\n",
       "      <th>Incorrectly detected by reader</th>\n",
       "      <th>All findings</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GT by radiologists for discrepancies</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fibrosis/scar high</th>\n",
       "      <td>12 (19.4%)</td>\n",
       "      <td>1 (1.6%)</td>\n",
       "      <td>13 (21.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other non-nodules lung high</th>\n",
       "      <td>15 (24.2%)</td>\n",
       "      <td>8 (12.9%)</td>\n",
       "      <td>23 (37.1%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other non-nodules nolung high</th>\n",
       "      <td>26 (41.9%)</td>\n",
       "      <td>0 (0.0%)</td>\n",
       "      <td>26 (41.9%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other non-nodules (no description)</th>\n",
       "      <td>0 (0.0%)</td>\n",
       "      <td>0 (0.0%)</td>\n",
       "      <td>0 (0.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>53 (85.5%)</td>\n",
       "      <td>9 (14.5%)</td>\n",
       "      <td>62 (100.0%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Incorrectly detected by AI  \\\n",
       "GT by radiologists for discrepancies                              \n",
       "fibrosis/scar high                                   12 (19.4%)   \n",
       "other non-nodules lung high                          15 (24.2%)   \n",
       "other non-nodules nolung high                        26 (41.9%)   \n",
       "other non-nodules (no description)                     0 (0.0%)   \n",
       "Total                                                53 (85.5%)   \n",
       "\n",
       "                                     Incorrectly detected by reader  \\\n",
       "GT by radiologists for discrepancies                                  \n",
       "fibrosis/scar high                                         1 (1.6%)   \n",
       "other non-nodules lung high                               8 (12.9%)   \n",
       "other non-nodules nolung high                              0 (0.0%)   \n",
       "other non-nodules (no description)                         0 (0.0%)   \n",
       "Total                                                     9 (14.5%)   \n",
       "\n",
       "                                     All findings  \n",
       "GT by radiologists for discrepancies               \n",
       "fibrosis/scar high                     13 (21.0%)  \n",
       "other non-nodules lung high            23 (37.1%)  \n",
       "other non-nodules nolung high          26 (41.9%)  \n",
       "other non-nodules (no description)       0 (0.0%)  \n",
       "Total                                 62 (100.0%)  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Same as above for high\n",
    "\n",
    "#Detailed comparison of FP categories for low and high BMI groups (no volume subgroups)\n",
    "\n",
    "df_categories=pd.DataFrame(columns=['Incorrectly detected by AI','Incorrectly detected by reader'], #below index with the correct order as above\n",
    "                          index=['fibrosis/scar high',\n",
    "                                 'other non-nodules lung high','other non-nodules nolung high','other non-nodules (no description)'\n",
    "                                ])\n",
    "\n",
    "df_categories.index.name = 'GT by radiologists for discrepancies'\n",
    "\n",
    "rest_no_desc_fp=other_nonodules_FP_high-other_nonodules_FP_lung_high-other_nonodules_FP_nolung_high\n",
    "df_categories['Incorrectly detected by AI']=[fibrosis_FP_high,other_nonodules_FP_lung_high,other_nonodules_FP_nolung_high,rest_no_desc_fp]\n",
    "\n",
    "rest_no_desc_fn=other_nonodules_FN_high-other_nonodules_FN_lung_high-other_nonodules_FN_nolung_high\n",
    "df_categories['Incorrectly detected by reader']=[fibrosis_FN_high,other_nonodules_FN_lung_high,other_nonodules_FN_nolung_high,rest_no_desc_fn]\n",
    "\n",
    "df_categories['All findings']=df_categories['Incorrectly detected by AI']+df_categories['Incorrectly detected by reader']\n",
    "\n",
    "df_categories.loc['Total']= df_categories.sum()\n",
    "\n",
    "all_findings=df_categories.iloc[:-1,:-1].sum().sum()\n",
    "\n",
    "percentage_fp=np.round((df_categories['Incorrectly detected by AI']/all_findings)*100,1)  \n",
    "df_categories['Incorrectly detected by AI']=[str(value[1])+' ('+str(percentage_fp[index])+'%)' \n",
    "                                             for index,value in enumerate(df_categories['Incorrectly detected by AI'].items())]\n",
    "\n",
    "percentage_fn=np.round((df_categories['Incorrectly detected by reader']/all_findings)*100,1) \n",
    "df_categories['Incorrectly detected by reader']=[str(value[1])+' ('+str(percentage_fn[index])+'%)' \n",
    "                                                 for index,value in enumerate(df_categories['Incorrectly detected by reader'].items())]\n",
    "\n",
    "df_categories['All findings']=[str(val)+' ('+str(np.round(100*val/all_findings,1))+'%)' for val in df_categories['All findings'].values]\n",
    "\n",
    "df_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0db7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_categories.to_excel('non_nodules_types_high.xlsx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06ad0ec0",
   "metadata": {},
   "source": [
    "### Metrics Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1300ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load number of nodules and lymph nodes for each of the high/low BMI groups - These are the TPs\n",
    "\n",
    "#Define nodule group names\n",
    "nod_groups_only=['sub_ground','pleural', 'calcified','other_all','atypical_triangular'] \n",
    "lymph_groups=['per_fisu','peri_bronch'] \n",
    "\n",
    "#Initialize number of TP for each of the reader and AI to 0\n",
    "\n",
    "#These can also be the sum of the volume subgroups below - Kept as is for now\n",
    "TP_nod_low=0\n",
    "TP_nod_high=0\n",
    "TP_lymph_low=0\n",
    "TP_lymph_high=0\n",
    "\n",
    "TP_nod_low_30_100=0\n",
    "TP_nod_high_30_100=0\n",
    "TP_nod_low_100_300=0\n",
    "TP_nod_high_100_300=0\n",
    "TP_nod_low_300=0\n",
    "TP_nod_high_300=0\n",
    "\n",
    "TP_lymph_low_30_100=0\n",
    "TP_lymph_high_30_100=0\n",
    "TP_lymph_low_100_300=0\n",
    "TP_lymph_high_100_300=0\n",
    "TP_lymph_low_300=0\n",
    "TP_lymph_high_300=0\n",
    "\n",
    "\n",
    "for deg in ['_high','_low']: #Loop over high/low BMI groups\n",
    "    \n",
    "    for nod_group in nod_groups_only: #Loop over nodule groups\n",
    "        \n",
    "        #Load variables with TP created in 'patient_selection_BMI_experiment.ipynb' notebook\n",
    "        with open(nod_group+deg+'_nod_only'+'.pickle','rb') as f:\n",
    "            exec(nod_group+deg+'_nod_only= pickle.load(f)')\n",
    "            \n",
    "        #Same for each volume subgroup    \n",
    "        with open(nod_group+deg+'_nod_only_30_100'+'.pickle','rb') as f:\n",
    "            exec(nod_group+deg+'_nod_only_30_100= pickle.load(f)')   \n",
    "        with open(nod_group+deg+'_nod_only_100_300'+'.pickle','rb') as f:\n",
    "            exec(nod_group+deg+'_nod_only_100_300= pickle.load(f)')             \n",
    "        with open(nod_group+deg+'_nod_only_300'+'.pickle','rb') as f:\n",
    "            exec(nod_group+deg+'_nod_only_300= pickle.load(f)')    \n",
    "\n",
    "        if deg=='_low': #Set variables depending on if we have low of high BMI + for each volume subgroup\n",
    "            TP_nod_low=TP_nod_low+eval(nod_group+deg+'_nod_only')\n",
    "            \n",
    "            TP_nod_low_30_100=TP_nod_low_30_100+eval(nod_group+deg+'_nod_only_30_100')\n",
    "            TP_nod_low_100_300=TP_nod_low_100_300+eval(nod_group+deg+'_nod_only_100_300')\n",
    "            TP_nod_low_300=TP_nod_low_300+eval(nod_group+deg+'_nod_only_300')\n",
    "            \n",
    "        else:\n",
    "            TP_nod_high=TP_nod_high+eval(nod_group+deg+'_nod_only')\n",
    "            \n",
    "            TP_nod_high_30_100=TP_nod_high_30_100+eval(nod_group+deg+'_nod_only_30_100')   \n",
    "            TP_nod_high_100_300=TP_nod_high_100_300+eval(nod_group+deg+'_nod_only_100_300')\n",
    "            TP_nod_high_300=TP_nod_high_300+eval(nod_group+deg+'_nod_only_300')\n",
    "\n",
    "        \n",
    "    for lymph_group in lymph_groups: #Similar as above for lymph node groups\n",
    "        \n",
    "        with open(lymph_group+deg+'_lymph'+'.pickle','rb') as f:\n",
    "            exec(lymph_group+deg+'_lymph= pickle.load(f)')\n",
    "            \n",
    "        with open(lymph_group+deg+'_lymph_30_100'+'.pickle','rb') as f:\n",
    "            exec(lymph_group+deg+'_lymph_30_100= pickle.load(f)')   \n",
    "        with open(lymph_group+deg+'_lymph_100_300'+'.pickle','rb') as f:\n",
    "            exec(lymph_group+deg+'_lymph_100_300= pickle.load(f)')             \n",
    "        with open(lymph_group+deg+'_lymph_300'+'.pickle','rb') as f:\n",
    "            exec(lymph_group+deg+'_lymph_300= pickle.load(f)')    \n",
    "            \n",
    "        if deg=='_low':\n",
    "            TP_lymph_low=TP_lymph_low+eval(lymph_group+deg+'_lymph')\n",
    "            \n",
    "            TP_lymph_low_30_100=TP_lymph_low_30_100+eval(lymph_group+deg+'_lymph_30_100')\n",
    "            TP_lymph_low_100_300=TP_lymph_low_100_300+eval(lymph_group+deg+'_lymph_100_300')\n",
    "            TP_lymph_low_300=TP_lymph_low_300+eval(lymph_group+deg+'_lymph_300')\n",
    "            \n",
    "        else:\n",
    "            TP_lymph_high=TP_lymph_high+eval(lymph_group+deg+'_lymph')\n",
    "            TP_lymph_high_30_100=TP_lymph_high_30_100+eval(lymph_group+deg+'_lymph_30_100')   \n",
    "            TP_lymph_high_100_300=TP_lymph_high_100_300+eval(lymph_group+deg+'_lymph_100_300')\n",
    "            TP_lymph_high_300=TP_lymph_high_300+eval(lymph_group+deg+'_lymph_300')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38f520cc",
   "metadata": {},
   "source": [
    "#### Below definition of TP depends on reader/AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d646edbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get total number of nodules (nodules+lymph nodes) for the whole low/high BMI groups and for volume subgroups\n",
    "#300+ volumes kept here since all these values should be 0 - If not, then delete them\n",
    "TP_low=TP_nod_low+TP_lymph_low\n",
    "TP_high=TP_nod_high+TP_lymph_high\n",
    "\n",
    "TP_low_30_100=TP_nod_low_30_100+TP_lymph_low_30_100\n",
    "TP_low_100_300=TP_nod_low_100_300+TP_lymph_low_100_300\n",
    "TP_low_300=TP_nod_low_300+TP_lymph_low_300\n",
    "TP_high_30_100=TP_nod_high_30_100+TP_lymph_high_30_100\n",
    "TP_high_100_300=TP_nod_high_100_300+TP_lymph_high_100_300\n",
    "TP_high_300=TP_nod_high_300+TP_lymph_high_300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109fa9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert TP_low==TP_low_30_100+TP_low_100_300+TP_low_300\n",
    "assert TP_high==TP_high_30_100+TP_high_100_300+TP_high_300"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccbf9927",
   "metadata": {},
   "source": [
    "#### Confidence Interval Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d081c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code below taken from https://gist.github.com/maidens/29939b3383a5e57935491303cf0d8e0b\n",
    "#For F1 score there was a suggestion on https://github.com/sousanunes/confidence_intervals/blob/master/propagation_confidence_interval.py\n",
    "#This will not used since it assumes normal distribution\n",
    "\n",
    "def _proportion_confidence_interval(r, n, z): \n",
    "    \"\"\"Compute confidence interval for a proportion.\n",
    "    https://real-statistics.com/binomial-and-related-distributions/proportion-distribution/proportion-parameter-confidence-interval/\n",
    "    Follows notation described on pages 46--47 of [1]. \n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    [1] R. G. Newcombe and D. G. Altman, Proportions and their differences, in Statisics\n",
    "    with Confidence: Confidence intervals and statisctical guidelines, 2nd Ed., D. G. Altman, \n",
    "    D. Machin, T. N. Bryant and M. J. Gardner (Eds.), pp. 45-57, BMJ Books, 2000. \n",
    "\n",
    "    Based on the book, r is the observed number of subjects with some feature in a sample of size n. z is a percentile from the norm distribution.\n",
    "    The formula in the link of the code is the same as in https://real-statistics.com/binomial-and-related-distributions/proportion-distribution/proportion-parameter-confidence-interval/\n",
    "    There is no continuity correction here. This is used in http://stats.org.uk/statistical-inference/Newcombe1998.pdf\n",
    "    The actual implementation used continuity correction. This is recommended for small sample sizes:  \n",
    "    https://towardsdatascience.com/five-confidence-intervals-for-proportions-that-you-should-know-about-7ff5484c024f\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 2*r + z**2\n",
    "    # B = z*np.sqrt(z**2 + 4*r*(1 - r/n))\n",
    "    B_low=1+z*np.sqrt(z**2 + 4*r*(1 - r/n) + (((4*r)-(2*n)-1)/n))\n",
    "    # if B_low<0:\n",
    "    #     B_low=0\n",
    "    \n",
    "    B_high=1+z*np.sqrt(z**2 + 4*r*(1 - r/n) - (((4*r)-(2*n)+1)/n))\n",
    "    # if B_high>1:\n",
    "    #     B_high=1\n",
    "\n",
    "    C = 2*(n + z**2)\n",
    "    return ((A-B_low)/C, (A+B_high)/C)\n",
    "\n",
    "\n",
    "def sensitivity_and_specificity_with_confidence_intervals(TP, FP, FN, TN, alpha=0.95):\n",
    "    \"\"\"Compute confidence intervals for sensitivity and specificity using Wilson's method. \n",
    "    Based on https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval this calculation is without continuity correction.\n",
    "    For more information about that check on https://www.statskingdom.com/doc_confidence_interval.html\n",
    "    Based on the first link, of the possible approximations, Wilson score interval methods (with or without continuity correction) \n",
    "    have been shown to be the most accurate and the most robust, though some prefer the Agresti–Coull approach for larger sample sizes\n",
    "    Another link for that is https://statisticaloddsandends.wordpress.com/2019/06/09/wilson-score-and-agresti-coull-intervals-for-binomial-proportions/\n",
    "    \n",
    "    This method does not rely on a normal approximation and results in accurate confidence intervals even for small sample sizes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    TP : int\n",
    "        Number of true positives\n",
    "    FP : int \n",
    "        Number of false positives\n",
    "    FN : int\n",
    "        Number of false negatives\n",
    "    TN : int\n",
    "        Number of true negatives\n",
    "    alpha : float, optional\n",
    "        Desired confidence. Defaults to 0.95, which yields a 95% confidence interval. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sensitivity_confidence_interval : Tuple (float, float)\n",
    "        Lower and upper bounds on the alpha confidence interval for sensitivity\n",
    "    PPV_confidence_interval : Tuple (float, float)\n",
    "        Lower and upper bounds on the alpha confidence interval for PPV\n",
    "    F1_confidence_interval : Tuple (float, float)\n",
    "        Lower and upper bounds on the alpha confidence interval for F1 score\n",
    "        \n",
    "    References\n",
    "    ----------\n",
    "    [1] R. G. Newcombe and D. G. Altman, Proportions and their differences, in Statisics\n",
    "    with Confidence: Confidence intervals and statisctical guidelines, 2nd Ed., D. G. Altman, \n",
    "    D. Machin, T. N. Bryant and M. J. Gardner (Eds.), pp. 45-57, BMJ Books, 2000. \n",
    "    [2] E. B. Wilson, Probable inference, the law of succession, and statistical inference,\n",
    "    J Am Stat Assoc 22:209-12, 1927. \n",
    "    \"\"\"\n",
    "    \n",
    "    z = -ndtri((1.0-alpha)/2)\n",
    "    \n",
    "    # Compute sensitivity using method described in [1] \n",
    "    sensitivity_confidence_interval = _proportion_confidence_interval(TP, TP + FN, z)\n",
    "\n",
    "     # Compute PPV\n",
    "    PPV_confidence_interval = _proportion_confidence_interval(TP, TP + FP, z)\n",
    "    \n",
    "    #Compute F1score\n",
    "    F1_confidence_interval = _proportion_confidence_interval(2*TP, 2*TP + (FP+FN), z) #if n=TP+FP+FN used we get nan errors - sample size of proportion should be with 2*TP\n",
    "    # Check also based on https://stats.stackexchange.com/questions/363382/confidence-interval-of-precision-recall-and-f1-score\n",
    "    #It is not a binomial outcome (eg. like accuracy which is num of correct over num of predicted) and so, we probably\n",
    "    #can't apply any number of binomial conf intervals as stated in https://stats.stackexchange.com/questions/563582/calculate-confidence-intervals-on-accuracy-metrics\n",
    "    #Also if data not normally distributed we cannot use simple formulas like those in https://aegis4048.github.io/comprehensive_confidence_intervals_for_python_developers\n",
    "\n",
    "    return sensitivity_confidence_interval, PPV_confidence_interval, F1_confidence_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9f717d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity(TP,FN): #same as recall\n",
    "    return TP/(TP+FN)\n",
    "\n",
    "def PPV(TP,FP): #Same as precision\n",
    "    return TP/(TP+FP)\n",
    "\n",
    "def F1score(TP,FP,FN):\n",
    "    return (2*TP)/(2*TP+(FP+FN))\n",
    "\n",
    "#Metrics with TN in their definition can't be used"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99010481",
   "metadata": {},
   "source": [
    "##### Example of CI calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bea469a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.948718, PPV: 0.649123, F1 score: 0.770833\n",
      "alpha = 0.500000 CI for sensitivity: (0.9040941124792334, 0.9775414611018507)\n",
      "alpha = 0.500000 CI for PPV: (0.5965219235920692, 0.6988783015915245)\n",
      "alpha = 0.500000 CI for F1 score: (0.7352529801795313, 0.8034004651312406)\n"
     ]
    }
   ],
   "source": [
    "for a in [0.5]: #Can also set other values of a to check the CI\n",
    "\n",
    "    sensitivity_confidence_interval, PPV_confidence_interval, F1_confidence_interval\\\n",
    "    = sensitivity_and_specificity_with_confidence_intervals(37, 20, 2, 0, alpha=a) #Here TP, FP, FN, TN were set based on an example below - just for demonstration\n",
    "\n",
    "    print(\"Sensitivity: %f, PPV: %f, F1 score: %f\" %(sensitivity(37,2), PPV(37,20),F1score(37,20,2)))\n",
    "    print(\"alpha = %f CI for sensitivity:\"%a, sensitivity_confidence_interval)\n",
    "    print(\"alpha = %f CI for PPV:\"%a, PPV_confidence_interval)\n",
    "    print(\"alpha = %f CI for F1 score:\"%a, F1_confidence_interval)    \n",
    "    #Confidence intervals of proportions were calculated using the Wilson method (with continuity correction). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c10ed9a",
   "metadata": {},
   "source": [
    "From the intervals above we can conclude that we won't get the same results if we use normal approximations (z=1.96 and mean between lower and upper bound of CI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a527dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get total number of nodules/non-nodules that were detected only by AI/human reader for each of the low/high BMI groups\n",
    "\n",
    "#nodules+Lymph nodes included in the right part of the equations below - if only nodules comment 'ai_lymph_..' and activate comments in reader below\n",
    "FP_nods_low=ai_only_nods_low_30_100+ai_only_nods_low_100_300+ai_only_nods_low_300 +(ai_lymph_low_30_100+ai_lymph_low_100_300+ai_lymph_low_300)\n",
    "FP_nods_high=ai_only_nods_high_30_100+ai_only_nods_high_100_300+ai_only_nods_high_300 +(ai_lymph_high_30_100+ai_lymph_high_100_300+ai_lymph_high_300)\n",
    "\n",
    "FP_nonods_low=ai_nonods_low_30_100+ai_nonods_low_100_300+ai_nonods_low_300\n",
    "FP_nonods_high=ai_nonods_high_30_100+ai_nonods_high_100_300+ai_nonods_high_300\n",
    "\n",
    "FN_nods_low=reader_nods_low_30_100+reader_nods_low_100_300+reader_nods_low_300 #-(reader_lymph_low_30_100+reader_lymph_low_100_300+reader_lymph_low_300)\n",
    "FN_nods_high=reader_nods_high_30_100+reader_nods_high_100_300+reader_nods_high_300 #-(reader_lymph_high_30_100+reader_lymph_high_100_300+reader_lymph_high_300)\n",
    "\n",
    "FN_nonods_low=reader_nonods_low_30_100+reader_nonods_low_100_300+reader_nonods_low_300\n",
    "FN_nonods_high=reader_nonods_high_30_100+reader_nonods_high_100_300+reader_nonods_high_300\n",
    "\n",
    "#Similar only for lymph nodes\n",
    "lymph_reader_low=reader_lymph_low_30_100+reader_lymph_low_100_300+reader_lymph_low_300\n",
    "lymph_reader_high=reader_lymph_high_30_100+reader_lymph_high_100_300+reader_lymph_high_300\n",
    "lymph_AI_low=ai_lymph_low_30_100+ai_lymph_low_100_300+ai_lymph_low_300\n",
    "lymph_AI_high=ai_lymph_high_30_100+ai_lymph_high_100_300+ai_lymph_high_300"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39a1442a",
   "metadata": {},
   "source": [
    "Explanation below assumes that GT is whatever is detected only! For TN, this might be incorrect! We assumed that TP (in REDCap) will always be nodules, even though sometimes this might not be correct.\n",
    "\n",
    "To calculate metrics for AI we consider the following (demonstrated for emphysema - same for non-emphysema cases):\n",
    "\n",
    "1. TP_AI=TP_both+FP_nods_emph (nodules found as nodules) \n",
    "2. FP_AI=FP_nonods_emph (non-nodules found as nodules)\n",
    "3. FN_AI=FN_nods_emph (nodules missed by AI)\n",
    "\n",
    "Similarly, for reader metrics:\n",
    "\n",
    "1. TP_read=TP_both+FN_nods_emph\n",
    "2. FP_read=FN_nonods_emph\n",
    "3. FN_read=FP_nods_emph\n",
    "\n",
    "'AI found and reader found' can be seen from TP in REDCap\n",
    "\n",
    "'AI missed and reader missed' does not exist - assumes that consensus found extra nodules while they just reviewed discrepancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f6499c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual number of nodules among discrepancies is 56\n",
      "From those 39 were detected by the AI only and 17 from reader only\n",
      "\n",
      "\n",
      "Actual number of lymph nodes among discrepancies is 57\n",
      "From those 14 were detected by the AI only and 43 from reader only\n"
     ]
    }
   ],
   "source": [
    "AI_found_lymph=lymph_AI_low+lymph_AI_high\n",
    "read_found_lymph=lymph_reader_low+lymph_reader_high\n",
    "\n",
    "reader_found_only=FN_nods_low+FN_nods_high - read_found_lymph\n",
    "AI_found_only=FP_nods_low+FP_nods_high-AI_found_lymph\n",
    "print(\"Actual number of nodules among discrepancies is\",reader_found_only+AI_found_only)\n",
    "print(\"From those {} were detected by the AI only and {} from reader only\".format(AI_found_only,reader_found_only))\n",
    "print(\"\\n\")\n",
    "print(\"Actual number of lymph nodes among discrepancies is\",read_found_lymph+AI_found_lymph)\n",
    "print(\"From those {} were detected by the AI only and {} from reader only\".format(AI_found_lymph,read_found_lymph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low BMI - Main analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65934db3",
   "metadata": {},
   "source": [
    "#### Table only for nodules (+atypical PFNs) - lymph nodes (typical PFNs and bronchovascular) not included in calculations (considered as non-existent). Statistical tests based on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fe4f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low BMI numbers\n",
      "TP_AI 81\n",
      "FP_AI 96\n",
      "FN_AI 4\n",
      "TP_read 68\n",
      "FP_read 28\n",
      "FN_read 17\n",
      "TP_both 64\n"
     ]
    }
   ],
   "source": [
    "TP_AI=TP_low+FP_nods_low-(TP_lymph_low) -lymph_AI_low #'FP_nods' include lymph and that's why we subtract 'lymph_AI_emph'\n",
    "FP_AI=FP_nonods_low\n",
    "FN_AI=TP_read_only=FN_nods_low-lymph_reader_low #nodules detected only by the reader, excluding lymph nodes\n",
    "\n",
    "TP_read=TP_low+FN_nods_low-(TP_lymph_low) - lymph_reader_low\n",
    "FP_read=FN_nonods_low\n",
    "FN_read=TP_AI_only=FP_nods_low-lymph_AI_low #nodules detected only by AI, excluding lymph nodes\n",
    "\n",
    "TP_both=TP_low-(TP_lymph_low) #Common nodules detected by both AI and reader\n",
    "\n",
    "#Print the above\n",
    "print(\"Low BMI numbers\")\n",
    "print(\"TP_AI\",TP_AI)\n",
    "print(\"FP_AI\",FP_AI)\n",
    "print(\"FN_AI\",FN_AI)\n",
    "print(\"TP_read\",TP_read)\n",
    "print(\"FP_read\",FP_read)\n",
    "print(\"FN_read\",FN_read)\n",
    "print(\"TP_both\",TP_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5087dd87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensitivity (95% CI)</th>\n",
       "      <th>PPV (95% CI)</th>\n",
       "      <th>F1 score (95% CI)</th>\n",
       "      <th>nodules detected</th>\n",
       "      <th>non-nodules detected</th>\n",
       "      <th>nodules missed</th>\n",
       "      <th>All findings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AI, low</th>\n",
       "      <td>0.95 (0.88, 0.98)</td>\n",
       "      <td>0.46 (0.38, 0.53)</td>\n",
       "      <td>0.62 (0.56, 0.68)</td>\n",
       "      <td>81 (44.8%)</td>\n",
       "      <td>96 (53.0%)</td>\n",
       "      <td>4 (2.2%)</td>\n",
       "      <td>181 (100%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reader, low</th>\n",
       "      <td>0.8 (0.7, 0.88)</td>\n",
       "      <td>0.71 (0.61, 0.79)</td>\n",
       "      <td>0.75 (0.68, 0.81)</td>\n",
       "      <td>68 (60.2%)</td>\n",
       "      <td>28 (24.8%)</td>\n",
       "      <td>17 (15.0%)</td>\n",
       "      <td>113 (100%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>149</td>\n",
       "      <td>124</td>\n",
       "      <td>21</td>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sensitivity (95% CI)       PPV (95% CI)  F1 score (95% CI)  \\\n",
       "AI, low        0.95 (0.88, 0.98)  0.46 (0.38, 0.53)  0.62 (0.56, 0.68)   \n",
       "reader, low      0.8 (0.7, 0.88)  0.71 (0.61, 0.79)  0.75 (0.68, 0.81)   \n",
       "Total                                                                    \n",
       "\n",
       "            nodules detected non-nodules detected nodules missed All findings  \n",
       "AI, low           81 (44.8%)           96 (53.0%)       4 (2.2%)   181 (100%)  \n",
       "reader, low       68 (60.2%)           28 (24.8%)     17 (15.0%)   113 (100%)  \n",
       "Total                    149                  124             21          294  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Two tables: one for low and one for high BMI (below), having also percentages.\n",
    "#Assessing detection performance for low/high groups - For nodules only, we treat lymph nodes as non-existent\n",
    "\n",
    "df_all_new=pd.DataFrame(columns=['sensitivity (95% CI)','PPV (95% CI)','F1 score (95% CI)','nodules detected','non-nodules detected','nodules missed'],\n",
    "                        index=['AI, low', 'reader, low'] )\n",
    "\n",
    "#AI nodules only\n",
    "df_all_new.iloc[0,0]=np.round(sensitivity(TP_AI,FN_AI),2)\n",
    "df_all_new.iloc[0,1]=np.round(PPV(TP_AI,FP_AI),2)\n",
    "df_all_new.iloc[0,2]=np.round(F1score(TP_AI,FP_AI,FN_AI),2)\n",
    "df_all_new.iloc[0,3]=TP_AI\n",
    "df_all_new.iloc[0,4]=FP_AI\n",
    "df_all_new.iloc[0,5]=FN_AI\n",
    "\n",
    "#Calculate CIs for sensitivity, PPV, and F1score\n",
    "sensitivity_confidence_interval_AI, PPV_confidence_interval_AI, F1_confidence_interval_AI \\\n",
    "= sensitivity_and_specificity_with_confidence_intervals(TP_AI, FP_AI, FN_AI, 0, alpha=0.95)\n",
    "\n",
    "#Round CIs to 2 digits\n",
    "ci_sens_ai=[np.round(x,2) for x in sensitivity_confidence_interval_AI]\n",
    "ci_ppv_ai=[np.round(x,2) for x in PPV_confidence_interval_AI]\n",
    "ci_f1_ai=[np.round(x,2) for x in F1_confidence_interval_AI]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[0]=str(df_all_new['sensitivity (95% CI)'].iloc[0])+' '+str(tuple(ci_sens_ai))\n",
    "df_all_new['PPV (95% CI)'].iloc[0]=str(df_all_new['PPV (95% CI)'].iloc[0])+' '+str(tuple(ci_ppv_ai))\n",
    "df_all_new['F1 score (95% CI)'].iloc[0]=str(df_all_new['F1 score (95% CI)'].iloc[0])+' '+str(tuple(ci_f1_ai))\n",
    "\n",
    "\n",
    "#Reader nodules only\n",
    "df_all_new.iloc[1,0]=np.round(sensitivity(TP_read,FN_read),2)\n",
    "df_all_new.iloc[1,1]=np.round(PPV(TP_read,FP_read),2)\n",
    "df_all_new.iloc[1,2]=np.round(F1score(TP_read,FP_read,FN_read),2)\n",
    "df_all_new.iloc[1,3]=TP_read\n",
    "df_all_new.iloc[1,4]=FP_read\n",
    "df_all_new.iloc[1,5]=FN_read\n",
    "\n",
    "sensitivity_confidence_interval_read, PPV_confidence_interval_read, F1_confidence_interval_read\\\n",
    "    = sensitivity_and_specificity_with_confidence_intervals(TP_read, FP_read, FN_read, 0, alpha=0.95)\n",
    "\n",
    "ci_sens_read=[np.round(x,2) for x in sensitivity_confidence_interval_read]\n",
    "ci_ppv_read=[np.round(x,2) for x in PPV_confidence_interval_read]\n",
    "ci_f1_read=[np.round(x,2) for x in F1_confidence_interval_read]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[1]=str(df_all_new['sensitivity (95% CI)'].iloc[1])+' '+str(tuple(ci_sens_read))\n",
    "df_all_new['PPV (95% CI)'].iloc[1]=str(df_all_new['PPV (95% CI)'].iloc[1])+' '+str(tuple(ci_ppv_read))\n",
    "df_all_new['F1 score (95% CI)'].iloc[1]=str(df_all_new['F1 score (95% CI)'].iloc[1])+' '+str(tuple(ci_f1_read))\n",
    "\n",
    "\n",
    "df_all_new['All findings']=df_all_new['nodules detected']+df_all_new['non-nodules detected']+df_all_new['nodules missed']\n",
    "df_all_new.loc['Total']= df_all_new.sum()\n",
    "df_all_new.loc['Total'].iloc[0:3]=''\n",
    "\n",
    "all_findings=df_all_new.iloc[:-1,3:-1].sum().sum()\n",
    "\n",
    "for i in range(2): #Add percentages to df\n",
    "    row_all=np.sum(df_all_new.iloc[i][3:6].values) #get all values for a given row\n",
    "\n",
    "    percentage_tp=np.round((df_all_new.iloc[i][3]/row_all)*100,1) #% of TP\n",
    "    df_all_new['nodules detected'].iloc[i]=str(df_all_new.iloc[i][3])+' ('+str(percentage_tp)+'%)'\n",
    "    percentage_fp=np.round((df_all_new.iloc[i][4]/row_all)*100,1) #% of FP\n",
    "    df_all_new['non-nodules detected'].iloc[i]=str(df_all_new.iloc[i][4])+' ('+str(percentage_fp)+'%)'\n",
    "    percentage_fn=np.round((df_all_new.iloc[i][5]/row_all)*100,1) #% of FN\n",
    "    df_all_new['nodules missed'].iloc[i]=str(df_all_new.iloc[i][5])+' ('+str(percentage_fn)+'%)'\n",
    "\n",
    "    df_all_new['All findings'].iloc[i]=str(df_all_new.iloc[i][6])+' (100%)'\n",
    "\n",
    "df_all_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3814b81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_new.to_excel('nodules_atypical_only_low.xlsx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "191392d0",
   "metadata": {},
   "source": [
    "#### McNemar' test\n",
    "\n",
    "- If we want it with continuity correction we should use 'exact=False, correction=False'. We can compare when it's not applied to see if these values are on different sides of the traditional 0.05 cutoff. If they are, we would have to check the 'exact=True' method to decide which to keep (no corrections at all).  Taken from https://cran.r-project.org/web/packages/exact2x2/vignettes/exactMcNemar.pdf\n",
    "- Continuity corrections no longer used based on https://stats.stackexchange.com/questions/6448/continuity-correction-for-pearson-and-mcnemars-chi-square-test but statistician suggested it due to small sample size\n",
    "- McNemar's test is used when we want to know whether there is a statistically significant difference in the proportion of nodules detected by AI and reader (paraphrased from https://www.geeksforgeeks.org/how-to-perform-mcnemars-test-in-python/).\n",
    "- Other useful sources: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8182550/ (paper used it for similar topic), https://stats.stackexchange.com/questions/358101/statistical-significance-p-value-for-comparing-two-classifiers-with-respect-to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc30f81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "McNemar's test (nodules only), AI_vs_Reader with continuity correction (not exact) p value is 0.00882876095281863\n",
      "For FP findings, with continuity correction (not exact) p value is 1.7792323833716621e-09\n"
     ]
    }
   ],
   "source": [
    "#McNemar's test to compare Reader vs AI (using consensus panel)\n",
    "#Below format is: [[Both AI found and reader found, reader missed and AI found], [Reader found and AI missed, 0]]\n",
    "\n",
    "#For nodules\n",
    "data=[[TP_both, FN_read],\n",
    "        [FN_AI,0]]\n",
    "# print(data)\n",
    "\n",
    "# McNemar's Test without continuity correction\n",
    "print(\"McNemar's test (nodules only), AI_vs_Reader with continuity correction (not exact) p value is\",mcnemar(data, exact=False,correction=True).pvalue) \n",
    "\n",
    "\n",
    "#For FPs\n",
    "data=[[0, FP_AI], \n",
    "        [FP_read, 0]]\n",
    "# print(data)\n",
    "\n",
    "# McNemar's Test without continuity correction\n",
    "print(\"For FP findings, with continuity correction (not exact) p value is\",mcnemar(data, exact=False,correction=True).pvalue)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a83cee28",
   "metadata": {},
   "source": [
    "#### Cohen's Kappa\n",
    "\n",
    "- According to https://en.wikipedia.org/wiki/Fleiss%27_kappa, we must use Fleiss kappa when assessing the agreement between three or more raters or the intra-rater reliability (for one appraiser versus themself). To calculate this, the fleiss_kappa() function from the statsmodels library can be used. Cohen's kappa can be used for two readers and this is what we use below (https://www.statology.org/cohens-kappa-python/).We should only have 0 or 1 labels since otherwise it is considered as a multiclass problem\n",
    "- Other useful sources https://www.statology.org/cohens-kappa-statistic/, https://vitalflux.com/cohen-kappa-score-python-example-machine-learning/\n",
    "- Based on the last one, in the contigency table we have reader 1 (actual results) horizontally and reader 2 (predictions) vertically. For this to be true, reasonable to assume reader 1 is GT by radiologists and reader 2 either reader or AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2422ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #AI vs GT, TP_both included\n",
    "# # Table looks like below:\n",
    "# #              GT\n",
    "# #             Yes                       No\n",
    "# # AI   Yes    TP_both+TP_AI_only      FP_AI\n",
    "# #      No     FN_AI                   0\n",
    "\n",
    "# list_of_lists=[[1 for x in range(TP_both)],[1 for x in range(TP_AI_only)],[0 for x in range(FP_AI) ],[1 for x in range(FN_AI) ] ] \n",
    "# rater_GT=[item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# list_of_lists=[[1 for x in range(TP_both)], [1 for x in range(TP_AI_only)],[1 for x in range(FP_AI) ],[0 for x in range(FN_AI) ]]\n",
    "# rater_AI=[item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# print(\"AI vs consensus (for nodules only), kappa is \",cohen_kappa_score(rater_GT, rater_AI))\n",
    "\n",
    "# #Reader vs GT, TP_both included\n",
    "# # Table looks like below:\n",
    "# #                    GT\n",
    "# #                   Yes                       No\n",
    "# # Reader   Yes    TP_both+TP_read_only      FP_read\n",
    "# #           No     FN_read                   0\n",
    "\n",
    "# list_of_lists=[[1 for x in range(TP_both)],[1 for x in range(TP_read_only)],[0 for x in range(FP_read) ],[1 for x in range(FN_read) ] ] \n",
    "# rater_GT=[item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# list_of_lists=[[1 for x in range(TP_both)],[1 for x in range(TP_read_only)],[1 for x in range(FP_read) ],[0 for x in range(FN_read) ]]\n",
    "# rater_read=[item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# print(\"Reader vs consensus (for non-nodules only), kappa is \",cohen_kappa_score(rater_GT, rater_read))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High BMI - Main analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bdda5563",
   "metadata": {},
   "source": [
    "#### Table only for nodules (+atypical PFNs) - lymph nodes (typical PFNs and bronchovascular) not included in calculations (considered as non-existent). Statistical tests based on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9bf97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High BMI numbers\n",
      "TP_AI 77\n",
      "FP_AI 53\n",
      "FN_AI 13\n",
      "TP_read 68\n",
      "FP_read 9\n",
      "FN_read 22\n",
      "TP_both 55\n"
     ]
    }
   ],
   "source": [
    "TP_AI= TP_high+FP_nods_high -(TP_lymph_high) -lymph_AI_high #'FP_nods' include lymph and that's why we subtract 'lymph_AI_high'\n",
    "FP_AI=FP_nonods_high\n",
    "FN_AI=TP_read_only=FN_nods_high-lymph_reader_high #nodules detected only by the reader, excluding lymph nodes\n",
    "\n",
    "TP_read=TP_high+FN_nods_high -(TP_lymph_high) - lymph_reader_high\n",
    "FP_read=FN_nonods_high\n",
    "FN_read=TP_AI_only=FP_nods_high-lymph_AI_high #nodules detected only by AI, excluding lymph nodes\n",
    "\n",
    "TP_both=TP_high-(TP_lymph_high) #Common nodules detected by both AI and reader\n",
    "\n",
    "#Print the above\n",
    "print(\"High BMI numbers\")\n",
    "print(\"TP_AI\",TP_AI)\n",
    "print(\"FP_AI\",FP_AI)\n",
    "print(\"FN_AI\",FN_AI)\n",
    "print(\"TP_read\",TP_read)\n",
    "print(\"FP_read\",FP_read)\n",
    "print(\"FN_read\",FN_read)\n",
    "print(\"TP_both\",TP_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d732795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensitivity (95% CI)</th>\n",
       "      <th>PPV (95% CI)</th>\n",
       "      <th>F1 score (95% CI)</th>\n",
       "      <th>nodules detected</th>\n",
       "      <th>non-nodules detected</th>\n",
       "      <th>nodules missed</th>\n",
       "      <th>All findings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AI, high</th>\n",
       "      <td>0.86 (0.76, 0.92)</td>\n",
       "      <td>0.59 (0.5, 0.68)</td>\n",
       "      <td>0.7 (0.63, 0.76)</td>\n",
       "      <td>77 (53.8%)</td>\n",
       "      <td>53 (37.1%)</td>\n",
       "      <td>13 (9.1%)</td>\n",
       "      <td>143 (100%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reader, high</th>\n",
       "      <td>0.76 (0.65, 0.84)</td>\n",
       "      <td>0.88 (0.78, 0.94)</td>\n",
       "      <td>0.81 (0.75, 0.87)</td>\n",
       "      <td>68 (68.7%)</td>\n",
       "      <td>9 (9.1%)</td>\n",
       "      <td>22 (22.2%)</td>\n",
       "      <td>99 (100%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>145</td>\n",
       "      <td>62</td>\n",
       "      <td>35</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             sensitivity (95% CI)       PPV (95% CI)  F1 score (95% CI)  \\\n",
       "AI, high        0.86 (0.76, 0.92)   0.59 (0.5, 0.68)   0.7 (0.63, 0.76)   \n",
       "reader, high    0.76 (0.65, 0.84)  0.88 (0.78, 0.94)  0.81 (0.75, 0.87)   \n",
       "Total                                                                     \n",
       "\n",
       "             nodules detected non-nodules detected nodules missed All findings  \n",
       "AI, high           77 (53.8%)           53 (37.1%)      13 (9.1%)   143 (100%)  \n",
       "reader, high       68 (68.7%)             9 (9.1%)     22 (22.2%)    99 (100%)  \n",
       "Total                     145                   62             35          242  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second part of table split for high BMI\n",
    "    \n",
    "#Assessing detection performance - For nodules only, we treat lymph nodes as non-existent\n",
    "df_all_new=pd.DataFrame(columns=['sensitivity (95% CI)','PPV (95% CI)','F1 score (95% CI)','nodules detected','non-nodules detected','nodules missed'],\n",
    "                        index=['AI, high', 'reader, high'])\n",
    "\n",
    "#AI nodules only\n",
    "df_all_new.iloc[0,0]=np.round(sensitivity(TP_AI,FN_AI),2)\n",
    "df_all_new.iloc[0,1]=np.round(PPV(TP_AI,FP_AI),2)\n",
    "df_all_new.iloc[0,2]=np.round(F1score(TP_AI,FP_AI,FN_AI),2)\n",
    "df_all_new.iloc[0,3]=TP_AI\n",
    "df_all_new.iloc[0,4]=FP_AI\n",
    "df_all_new.iloc[0,5]=FN_AI\n",
    "\n",
    "#Calculate CIs for sensitivity, PPV, and F1score\n",
    "sensitivity_confidence_interval_AI, PPV_confidence_interval_AI, F1_confidence_interval_AI \\\n",
    "= sensitivity_and_specificity_with_confidence_intervals(TP_AI, FP_AI, FN_AI, 0, alpha=0.95)\n",
    "\n",
    "#Round CIs to 2 digits\n",
    "ci_sens_ai=[np.round(x,2) for x in sensitivity_confidence_interval_AI]\n",
    "ci_ppv_ai=[np.round(x,2) for x in PPV_confidence_interval_AI]\n",
    "ci_f1_ai=[np.round(x,2) for x in F1_confidence_interval_AI]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[0]=str(df_all_new['sensitivity (95% CI)'].iloc[0])+' '+str(tuple(ci_sens_ai))\n",
    "df_all_new['PPV (95% CI)'].iloc[0]=str(df_all_new['PPV (95% CI)'].iloc[0])+' '+str(tuple(ci_ppv_ai))\n",
    "df_all_new['F1 score (95% CI)'].iloc[0]=str(df_all_new['F1 score (95% CI)'].iloc[0])+' '+str(tuple(ci_f1_ai))\n",
    "\n",
    "#Reader nodules only\n",
    "df_all_new.iloc[1,0]=np.round(sensitivity(TP_read,FN_read),2)\n",
    "df_all_new.iloc[1,1]=np.round(PPV(TP_read,FP_read),2)\n",
    "df_all_new.iloc[1,2]=np.round(F1score(TP_read,FP_read,FN_read),2)\n",
    "df_all_new.iloc[1,3]=TP_read\n",
    "df_all_new.iloc[1,4]=FP_read\n",
    "df_all_new.iloc[1,5]=FN_read\n",
    "\n",
    "sensitivity_confidence_interval_read, PPV_confidence_interval_read, F1_confidence_interval_read\\\n",
    "    = sensitivity_and_specificity_with_confidence_intervals(TP_read, FP_read, FN_read, 0, alpha=0.95)\n",
    "\n",
    "ci_sens_read=[np.round(x,2) for x in sensitivity_confidence_interval_read]\n",
    "ci_ppv_read=[np.round(x,2) for x in PPV_confidence_interval_read]\n",
    "ci_f1_read=[np.round(x,2) for x in F1_confidence_interval_read]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[1]=str(df_all_new['sensitivity (95% CI)'].iloc[1])+' '+str(tuple(ci_sens_read))\n",
    "df_all_new['PPV (95% CI)'].iloc[1]=str(df_all_new['PPV (95% CI)'].iloc[1])+' '+str(tuple(ci_ppv_read))\n",
    "df_all_new['F1 score (95% CI)'].iloc[1]=str(df_all_new['F1 score (95% CI)'].iloc[1])+' '+str(tuple(ci_f1_read))\n",
    "\n",
    "\n",
    "df_all_new['All findings']=df_all_new['nodules detected']+df_all_new['non-nodules detected']+df_all_new['nodules missed']\n",
    "df_all_new.loc['Total']= df_all_new.sum()\n",
    "df_all_new.loc['Total'].iloc[0:3]=''\n",
    "\n",
    "all_findings=df_all_new.iloc[:-1,3:-1].sum().sum()\n",
    "\n",
    "for i in range(2): #Add percentages to df\n",
    "    row_all=np.sum(df_all_new.iloc[i][3:6].values)\n",
    "\n",
    "    percentage_fp=np.round((df_all_new.iloc[i][4]/row_all)*100,1) \n",
    "    df_all_new['non-nodules detected'].iloc[i]=str(df_all_new.iloc[i][4])+' ('+str(percentage_fp)+'%)'\n",
    "    percentage_tp=np.round((df_all_new.iloc[i][3]/row_all)*100,1) \n",
    "    df_all_new['nodules detected'].iloc[i]=str(df_all_new.iloc[i][3])+' ('+str(percentage_tp)+'%)'\n",
    "    percentage_fn=np.round((df_all_new.iloc[i][5]/row_all)*100,1) \n",
    "    df_all_new['nodules missed'].iloc[i]=str(df_all_new.iloc[i][5])+' ('+str(percentage_fn)+'%)'\n",
    "\n",
    "    df_all_new['All findings'].iloc[i]=str(df_all_new.iloc[i][6])+' (100%)'\n",
    "\n",
    "df_all_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9daecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_new.to_excel('nodules_atypical_only_high.xlsx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff08457d",
   "metadata": {},
   "source": [
    "##### McNemar's test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55b28ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For nodules only (AI vs reader) with continuity correction (not exact) p value is 0.17629637444050728\n",
      "For FP findings, with continuity correction (not exact) p value is 4.7344548308441886e-08\n"
     ]
    }
   ],
   "source": [
    "#For nodules\n",
    "data=[[TP_both, FN_read],\n",
    "        [FN_AI,0]]\n",
    "# print(data)\n",
    "\n",
    "# McNemar's Test without continuity correction\n",
    "print(\"For nodules only (AI vs reader) with continuity correction (not exact) p value is\",mcnemar(data, exact=False,correction=True).pvalue)\n",
    "\n",
    "\n",
    "#For FPs\n",
    "data=[[0, FP_AI], \n",
    "        [FP_read, 0]]\n",
    "# print(data)\n",
    "\n",
    "# McNemar's Test without continuity correction\n",
    "print(\"For FP findings, with continuity correction (not exact) p value is\",mcnemar(data, exact=False,correction=True).pvalue)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28f43a1e",
   "metadata": {},
   "source": [
    "##### Cohen's Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a43b8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #AI vs GT, TP_both included - \n",
    "# list_of_lists=[[1 for x in range(TP_both)],[1 for x in range(TP_AI_only)],[0 for x in range(FP_AI) ],[1 for x in range(FN_AI) ] ] \n",
    "# rater_GT=[item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# list_of_lists=[[1 for x in range(TP_both)], [1 for x in range(TP_AI_only)],[1 for x in range(FP_AI) ],[0 for x in range(FN_AI) ]]\n",
    "# rater_AI=[item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# print(\"AI vs consensus (for nodules only), kappa is \",cohen_kappa_score(rater_GT, rater_AI))\n",
    "\n",
    "# #Reader vs GT, TP_both included\n",
    "# list_of_lists=[[1 for x in range(TP_both)],[1 for x in range(TP_read_only)],[0 for x in range(FP_read) ],[1 for x in range(FN_read) ] ] \n",
    "# rater_GT=[item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# list_of_lists=[[1 for x in range(TP_both)],[1 for x in range(TP_read_only)],[1 for x in range(FP_read) ],[0 for x in range(FN_read) ]]\n",
    "# rater_read=[item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# print(\"Reader vs consensus (for non-nodules only), kappa is \",cohen_kappa_score(rater_GT, rater_read))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b661eaa",
   "metadata": {},
   "source": [
    "#### Comparison of volume subgroups for low BMI - Statistical tests based on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bbeb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low BMI numbers\n",
      "TP_AI_100 62\n",
      "FP_AI_100 26\n",
      "FN_AI_100 4\n",
      "TP_read_100 53\n",
      "FP_read_100 22\n",
      "FN_read_100 13\n",
      "TP_AI_100_300 14\n",
      "FP_AI_100_300 43\n",
      "FN_AI_100_300 0\n",
      "TP_read_100_300 11\n",
      "FP_read_100_300 5\n",
      "FN_read_100_300 3\n",
      "TP_AI_300 5\n",
      "FP_AI_300 27\n",
      "FN_AI_300 0\n",
      "TP_read_300 4\n",
      "FP_read_300 1\n",
      "FN_read_300 1\n",
      "TP_both_100 49\n",
      "TP_both_100_300 11\n",
      "TP_both_300 4\n"
     ]
    }
   ],
   "source": [
    "TP_AI_100=TP_low_30_100+ai_only_nods_low_30_100-(TP_lymph_low_30_100)# -ai_lymph_low_30_100  \n",
    "FP_AI_100=ai_nonods_low_30_100\n",
    "FN_AI_100=reader_nods_low_30_100-reader_lymph_low_30_100 #nodules of reader excluding lymph nodes\n",
    "\n",
    "TP_read_100=TP_low_30_100+reader_nods_low_30_100-(TP_lymph_low_30_100) - reader_lymph_low_30_100\n",
    "FP_read_100=reader_nonods_low_30_100\n",
    "FN_read_100=ai_only_nods_low_30_100#-ai_lymph_low_30_100 #nodules of AI excluding lymph nodes\n",
    "\n",
    "TP_AI_100_300=TP_low_100_300+ai_only_nods_low_100_300-(TP_lymph_low_100_300)# -(ai_lymph_low_100_300)\n",
    "FP_AI_100_300=ai_nonods_low_100_300\n",
    "FN_AI_100_300=reader_nods_low_100_300-(reader_lymph_low_100_300) #nodules of reader excluding lymph nodes\n",
    "\n",
    "TP_read_100_300=TP_low_100_300+reader_nods_low_100_300-(TP_lymph_low_100_300) - (reader_lymph_low_100_300)\n",
    "FP_read_100_300=reader_nonods_low_100_300\n",
    "FN_read_100_300=ai_only_nods_low_100_300#-(ai_lymph_low_100_300) #nodules of AI excluding lymph nodes\n",
    "\n",
    "TP_AI_300=TP_low_300+ai_only_nods_low_300-(TP_lymph_low_300)# -(ai_lymph_low_300)\n",
    "FP_AI_300=ai_nonods_low_300\n",
    "FN_AI_300=reader_nods_low_300-(reader_lymph_low_300) #nodules of reader excluding lymph nodes\n",
    "\n",
    "TP_read_300=TP_low_300+reader_nods_low_300-(TP_lymph_low_300) - (reader_lymph_low_300)\n",
    "FP_read_300=reader_nonods_low_300\n",
    "FN_read_300=ai_only_nods_low_300#-(ai_lymph_low_300) #nodules of AI excluding lymph nodes\n",
    "\n",
    "TP_both_100=TP_low_30_100-(TP_lymph_low_30_100) \n",
    "TP_both_100_300=TP_low_100_300-(TP_lymph_low_100_300)\n",
    "TP_both_300=TP_low_300-(TP_lymph_low_300)\n",
    "\n",
    "#Print the above\n",
    "print(\"Low BMI numbers\")\n",
    "print(\"TP_AI_100\",TP_AI_100)\n",
    "print(\"FP_AI_100\",FP_AI_100)\n",
    "print(\"FN_AI_100\",FN_AI_100)\n",
    "print(\"TP_read_100\",TP_read_100)\n",
    "print(\"FP_read_100\",FP_read_100)\n",
    "print(\"FN_read_100\",FN_read_100)\n",
    "print(\"TP_AI_100_300\",TP_AI_100_300)\n",
    "print(\"FP_AI_100_300\",FP_AI_100_300)\n",
    "print(\"FN_AI_100_300\",FN_AI_100_300)\n",
    "print(\"TP_read_100_300\",TP_read_100_300)\n",
    "print(\"FP_read_100_300\",FP_read_100_300)\n",
    "print(\"FN_read_100_300\",FN_read_100_300)\n",
    "print(\"TP_AI_300\",TP_AI_300)\n",
    "print(\"FP_AI_300\",FP_AI_300)\n",
    "print(\"FN_AI_300\",FN_AI_300)\n",
    "print(\"TP_read_300\",TP_read_300)\n",
    "print(\"FP_read_300\",FP_read_300)\n",
    "print(\"FN_read_300\",FN_read_300)\n",
    "print(\"TP_both_100\",TP_both_100)\n",
    "print(\"TP_both_100_300\",TP_both_100_300)\n",
    "print(\"TP_both_300\",TP_both_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40466b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensitivity (95% CI)</th>\n",
       "      <th>PPV (95% CI)</th>\n",
       "      <th>F1 score (95% CI)</th>\n",
       "      <th>nodules correctly detected</th>\n",
       "      <th>non-nodules incorrectly detected</th>\n",
       "      <th>nodules missed</th>\n",
       "      <th>All findings</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GT by radiologists for discrepancies</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AI, low 30-100mm3</th>\n",
       "      <td>0.94 (0.84, 0.98)</td>\n",
       "      <td>0.7 (0.6, 0.79)</td>\n",
       "      <td>0.81 (0.73, 0.86)</td>\n",
       "      <td>62 (34.3%)</td>\n",
       "      <td>26 (14.4%)</td>\n",
       "      <td>4 (2.2%)</td>\n",
       "      <td>92 (50.8%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AI, low 100-300mm3</th>\n",
       "      <td>1.0 (0.73, 0.99)</td>\n",
       "      <td>0.25 (0.15, 0.38)</td>\n",
       "      <td>0.39 (0.28, 0.52)</td>\n",
       "      <td>14 (7.7%)</td>\n",
       "      <td>43 (23.8%)</td>\n",
       "      <td>0 (0.0%)</td>\n",
       "      <td>57 (31.5%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AI, low 300+mm3</th>\n",
       "      <td>1.0 (0.46, 0.98)</td>\n",
       "      <td>0.16 (0.06, 0.34)</td>\n",
       "      <td>0.27 (0.14, 0.44)</td>\n",
       "      <td>5 (2.8%)</td>\n",
       "      <td>27 (14.9%)</td>\n",
       "      <td>0 (0.0%)</td>\n",
       "      <td>32 (17.7%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>181 (100%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reader, low 30-100mm3</th>\n",
       "      <td>0.8 (0.68, 0.89)</td>\n",
       "      <td>0.71 (0.59, 0.8)</td>\n",
       "      <td>0.75 (0.67, 0.82)</td>\n",
       "      <td>53 (46.9%)</td>\n",
       "      <td>22 (19.5%)</td>\n",
       "      <td>13 (11.5%)</td>\n",
       "      <td>88 (77.9%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reader, low 100-300mm3</th>\n",
       "      <td>0.79 (0.49, 0.94)</td>\n",
       "      <td>0.69 (0.41, 0.88)</td>\n",
       "      <td>0.73 (0.54, 0.87)</td>\n",
       "      <td>11 (9.7%)</td>\n",
       "      <td>5 (4.4%)</td>\n",
       "      <td>3 (2.7%)</td>\n",
       "      <td>19 (16.8%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reader, low 300+mm3</th>\n",
       "      <td>0.8 (0.3, 0.99)</td>\n",
       "      <td>0.8 (0.3, 0.99)</td>\n",
       "      <td>0.8 (0.44, 0.96)</td>\n",
       "      <td>4 (3.5%)</td>\n",
       "      <td>1 (0.9%)</td>\n",
       "      <td>1 (0.9%)</td>\n",
       "      <td>6 (5.3%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>107 (100%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     sensitivity (95% CI)       PPV (95% CI)  \\\n",
       "GT by radiologists for discrepancies                                           \n",
       "AI, low 30-100mm3                       0.94 (0.84, 0.98)    0.7 (0.6, 0.79)   \n",
       "AI, low 100-300mm3                       1.0 (0.73, 0.99)  0.25 (0.15, 0.38)   \n",
       "AI, low 300+mm3                          1.0 (0.46, 0.98)  0.16 (0.06, 0.34)   \n",
       "                                                                               \n",
       "reader, low 30-100mm3                    0.8 (0.68, 0.89)   0.71 (0.59, 0.8)   \n",
       "reader, low 100-300mm3                  0.79 (0.49, 0.94)  0.69 (0.41, 0.88)   \n",
       "reader, low 300+mm3                       0.8 (0.3, 0.99)    0.8 (0.3, 0.99)   \n",
       "                                                                               \n",
       "\n",
       "                                      F1 score (95% CI)  \\\n",
       "GT by radiologists for discrepancies                      \n",
       "AI, low 30-100mm3                     0.81 (0.73, 0.86)   \n",
       "AI, low 100-300mm3                    0.39 (0.28, 0.52)   \n",
       "AI, low 300+mm3                       0.27 (0.14, 0.44)   \n",
       "                                                          \n",
       "reader, low 30-100mm3                 0.75 (0.67, 0.82)   \n",
       "reader, low 100-300mm3                0.73 (0.54, 0.87)   \n",
       "reader, low 300+mm3                    0.8 (0.44, 0.96)   \n",
       "                                                          \n",
       "\n",
       "                                     nodules correctly detected  \\\n",
       "GT by radiologists for discrepancies                              \n",
       "AI, low 30-100mm3                                    62 (34.3%)   \n",
       "AI, low 100-300mm3                                    14 (7.7%)   \n",
       "AI, low 300+mm3                                        5 (2.8%)   \n",
       "                                                                  \n",
       "reader, low 30-100mm3                                53 (46.9%)   \n",
       "reader, low 100-300mm3                                11 (9.7%)   \n",
       "reader, low 300+mm3                                    4 (3.5%)   \n",
       "                                                                  \n",
       "\n",
       "                                     non-nodules incorrectly detected  \\\n",
       "GT by radiologists for discrepancies                                    \n",
       "AI, low 30-100mm3                                          26 (14.4%)   \n",
       "AI, low 100-300mm3                                         43 (23.8%)   \n",
       "AI, low 300+mm3                                            27 (14.9%)   \n",
       "                                                                        \n",
       "reader, low 30-100mm3                                      22 (19.5%)   \n",
       "reader, low 100-300mm3                                       5 (4.4%)   \n",
       "reader, low 300+mm3                                          1 (0.9%)   \n",
       "                                                                        \n",
       "\n",
       "                                     nodules missed All findings  \n",
       "GT by radiologists for discrepancies                              \n",
       "AI, low 30-100mm3                          4 (2.2%)   92 (50.8%)  \n",
       "AI, low 100-300mm3                         0 (0.0%)   57 (31.5%)  \n",
       "AI, low 300+mm3                            0 (0.0%)   32 (17.7%)  \n",
       "                                                      181 (100%)  \n",
       "reader, low 30-100mm3                    13 (11.5%)   88 (77.9%)  \n",
       "reader, low 100-300mm3                     3 (2.7%)   19 (16.8%)  \n",
       "reader, low 300+mm3                        1 (0.9%)     6 (5.3%)  \n",
       "                                                      107 (100%)  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For low BMI comparison between reader and AI for volume subgroups\n",
    "df_all_new=pd.DataFrame(columns=['sensitivity (95% CI)','PPV (95% CI)','F1 score (95% CI)','nodules correctly detected','non-nodules incorrectly detected','nodules missed'], \n",
    "                        index=['AI, low 30-100mm3', 'AI, low 100-300mm3','AI, low 300+mm3','',\n",
    "                               'reader, low 30-100mm3', 'reader, low 100-300mm3','reader, low 300+mm3',''\n",
    "                              ])\n",
    "\n",
    "df_all_new.index.name = 'GT by radiologists for discrepancies' \n",
    "\n",
    "df_all_new.iloc[0,0]=np.round(sensitivity(TP_AI_100,FN_AI_100),2)\n",
    "df_all_new.iloc[0,1]=np.round(PPV(TP_AI_100,FP_AI_100),2)\n",
    "df_all_new.iloc[0,2]=np.round(F1score(TP_AI_100,FP_AI_100,FN_AI_100),2)\n",
    "df_all_new.iloc[0,3]=TP_AI_100\n",
    "df_all_new.iloc[0,4]=FP_AI_100\n",
    "df_all_new.iloc[0,5]=FN_AI_100\n",
    "\n",
    "sensitivity_confidence_interval_AI, PPV_confidence_interval_AI, F1_confidence_interval_AI \\\n",
    "= sensitivity_and_specificity_with_confidence_intervals(TP_AI_100, FP_AI_100, FN_AI_100, 0, alpha=0.95)\n",
    "\n",
    "ci_sens_ai=[np.round(x,2) for x in sensitivity_confidence_interval_AI]\n",
    "ci_ppv_ai=[np.round(x,2) for x in PPV_confidence_interval_AI]\n",
    "ci_f1_ai=[np.round(x,2) for x in F1_confidence_interval_AI]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[0]=str(df_all_new['sensitivity (95% CI)'].iloc[0])+' '+str(tuple(ci_sens_ai))\n",
    "df_all_new['PPV (95% CI)'].iloc[0]=str(df_all_new['PPV (95% CI)'].iloc[0])+' '+str(tuple(ci_ppv_ai))\n",
    "df_all_new['F1 score (95% CI)'].iloc[0]=str(df_all_new['F1 score (95% CI)'].iloc[0])+' '+str(tuple(ci_f1_ai))\n",
    "\n",
    "\n",
    "df_all_new.iloc[4,0]=np.round(sensitivity(TP_read_100,FN_read_100),2)\n",
    "df_all_new.iloc[4,1]=np.round(PPV(TP_read_100,FP_read_100),2)\n",
    "df_all_new.iloc[4,2]=np.round(F1score(TP_read_100,FP_read_100,FN_read_100),2)\n",
    "df_all_new.iloc[4,3]=TP_read_100\n",
    "df_all_new.iloc[4,4]=FP_read_100\n",
    "df_all_new.iloc[4,5]=FN_read_100\n",
    "\n",
    "sensitivity_confidence_interval_AI, PPV_confidence_interval_AI, F1_confidence_interval_AI \\\n",
    "= sensitivity_and_specificity_with_confidence_intervals(TP_read_100, FP_read_100, FN_read_100, 0, alpha=0.95)\n",
    "\n",
    "ci_sens_ai=[np.round(x,2) for x in sensitivity_confidence_interval_AI]\n",
    "ci_ppv_ai=[np.round(x,2) for x in PPV_confidence_interval_AI]\n",
    "ci_f1_ai=[np.round(x,2) for x in F1_confidence_interval_AI]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[4]=str(df_all_new['sensitivity (95% CI)'].iloc[4])+' '+str(tuple(ci_sens_ai))\n",
    "df_all_new['PPV (95% CI)'].iloc[4]=str(df_all_new['PPV (95% CI)'].iloc[4])+' '+str(tuple(ci_ppv_ai))\n",
    "df_all_new['F1 score (95% CI)'].iloc[4]=str(df_all_new['F1 score (95% CI)'].iloc[4])+' '+str(tuple(ci_f1_ai))\n",
    "\n",
    "\n",
    "\n",
    "df_all_new.iloc[1,0]=np.round(sensitivity(TP_AI_100_300,FN_AI_100_300),2)\n",
    "df_all_new.iloc[1,1]=np.round(PPV(TP_AI_100_300,FP_AI_100_300),2)\n",
    "df_all_new.iloc[1,2]=np.round(F1score(TP_AI_100_300,FP_AI_100_300,FN_AI_100_300),2)\n",
    "df_all_new.iloc[1,3]=TP_AI_100_300\n",
    "df_all_new.iloc[1,4]=FP_AI_100_300\n",
    "df_all_new.iloc[1,5]=FN_AI_100_300\n",
    "\n",
    "sensitivity_confidence_interval_AI, PPV_confidence_interval_AI, F1_confidence_interval_AI \\\n",
    "= sensitivity_and_specificity_with_confidence_intervals(TP_AI_100_300, FP_AI_100_300, FN_AI_100_300, 0, alpha=0.95)\n",
    "\n",
    "ci_sens_ai=[np.round(x,2) for x in sensitivity_confidence_interval_AI]\n",
    "ci_ppv_ai=[np.round(x,2) for x in PPV_confidence_interval_AI]\n",
    "ci_f1_ai=[np.round(x,2) for x in F1_confidence_interval_AI]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[1]=str(df_all_new['sensitivity (95% CI)'].iloc[1])+' '+str(tuple(ci_sens_ai))\n",
    "df_all_new['PPV (95% CI)'].iloc[1]=str(df_all_new['PPV (95% CI)'].iloc[1])+' '+str(tuple(ci_ppv_ai))\n",
    "df_all_new['F1 score (95% CI)'].iloc[1]=str(df_all_new['F1 score (95% CI)'].iloc[1])+' '+str(tuple(ci_f1_ai))\n",
    "\n",
    "\n",
    "\n",
    "df_all_new.iloc[5,0]=np.round(sensitivity(TP_read_100_300,FN_read_100_300),2)\n",
    "df_all_new.iloc[5,1]=np.round(PPV(TP_read_100_300,FP_read_100_300),2)\n",
    "df_all_new.iloc[5,2]=np.round(F1score(TP_read_100_300,FP_read_100_300,FN_read_100_300),2)\n",
    "df_all_new.iloc[5,3]=TP_read_100_300\n",
    "df_all_new.iloc[5,4]=FP_read_100_300\n",
    "df_all_new.iloc[5,5]=FN_read_100_300\n",
    "\n",
    "sensitivity_confidence_interval_AI, PPV_confidence_interval_AI, F1_confidence_interval_AI \\\n",
    "= sensitivity_and_specificity_with_confidence_intervals(TP_read_100_300, FP_read_100_300, FN_read_100_300, 0, alpha=0.95)\n",
    "\n",
    "ci_sens_ai=[np.round(x,2) for x in sensitivity_confidence_interval_AI]\n",
    "ci_ppv_ai=[np.round(x,2) for x in PPV_confidence_interval_AI]\n",
    "ci_f1_ai=[np.round(x,2) for x in F1_confidence_interval_AI]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[5]=str(df_all_new['sensitivity (95% CI)'].iloc[5])+' '+str(tuple(ci_sens_ai))\n",
    "df_all_new['PPV (95% CI)'].iloc[5]=str(df_all_new['PPV (95% CI)'].iloc[5])+' '+str(tuple(ci_ppv_ai))\n",
    "df_all_new['F1 score (95% CI)'].iloc[5]=str(df_all_new['F1 score (95% CI)'].iloc[5])+' '+str(tuple(ci_f1_ai))\n",
    "\n",
    "\n",
    "df_all_new.iloc[2,0]=np.round(sensitivity(TP_AI_300,FN_AI_300),2)\n",
    "df_all_new.iloc[2,1]=np.round(PPV(TP_AI_300,FP_AI_300),2)\n",
    "df_all_new.iloc[2,2]=np.round(F1score(TP_AI_300,FP_AI_300,FN_AI_300),2)\n",
    "df_all_new.iloc[2,3]=TP_AI_300\n",
    "df_all_new.iloc[2,4]=FP_AI_300\n",
    "df_all_new.iloc[2,5]=FN_AI_300\n",
    "\n",
    "sensitivity_confidence_interval_AI, PPV_confidence_interval_AI, F1_confidence_interval_AI \\\n",
    "= sensitivity_and_specificity_with_confidence_intervals(TP_AI_300, FP_AI_300, FN_AI_300, 0, alpha=0.95)\n",
    "\n",
    "ci_sens_ai=[np.round(x,2) for x in sensitivity_confidence_interval_AI]\n",
    "ci_ppv_ai=[np.round(x,2) for x in PPV_confidence_interval_AI]\n",
    "ci_f1_ai=[np.round(x,2) for x in F1_confidence_interval_AI]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[2]=str(df_all_new['sensitivity (95% CI)'].iloc[2])+' '+str(tuple(ci_sens_ai))\n",
    "df_all_new['PPV (95% CI)'].iloc[2]=str(df_all_new['PPV (95% CI)'].iloc[2])+' '+str(tuple(ci_ppv_ai))\n",
    "df_all_new['F1 score (95% CI)'].iloc[2]=str(df_all_new['F1 score (95% CI)'].iloc[2])+' '+str(tuple(ci_f1_ai))\n",
    "\n",
    "\n",
    "df_all_new.iloc[6,0]=np.round(sensitivity(TP_read_300,FN_read_300),2)\n",
    "df_all_new.iloc[6,1]=np.round(PPV(TP_read_300,FP_read_300),2)\n",
    "df_all_new.iloc[6,2]=np.round(F1score(TP_read_300,FP_read_300,FN_read_300),2)\n",
    "df_all_new.iloc[6,3]=TP_read_300\n",
    "df_all_new.iloc[6,4]=FP_read_300\n",
    "df_all_new.iloc[6,5]=FN_read_300\n",
    "\n",
    "sensitivity_confidence_interval_AI, PPV_confidence_interval_AI, F1_confidence_interval_AI \\\n",
    "= sensitivity_and_specificity_with_confidence_intervals(TP_read_300, FP_read_300, FN_read_300, 0, alpha=0.95)\n",
    "\n",
    "ci_sens_ai=[np.round(x,2) for x in sensitivity_confidence_interval_AI]\n",
    "ci_ppv_ai=[np.round(x,2) for x in PPV_confidence_interval_AI]\n",
    "ci_f1_ai=[np.round(x,2) for x in F1_confidence_interval_AI]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[6]=str(df_all_new['sensitivity (95% CI)'].iloc[6])+' '+str(tuple(ci_sens_ai))\n",
    "df_all_new['PPV (95% CI)'].iloc[6]=str(df_all_new['PPV (95% CI)'].iloc[6])+' '+str(tuple(ci_ppv_ai))\n",
    "df_all_new['F1 score (95% CI)'].iloc[6]=str(df_all_new['F1 score (95% CI)'].iloc[6])+' '+str(tuple(ci_f1_ai))\n",
    "\n",
    "\n",
    "df_all_new.iloc[3,0]=0\n",
    "df_all_new.iloc[3,1]=0\n",
    "df_all_new.iloc[3,2]=0\n",
    "df_all_new.iloc[3,3]=0\n",
    "df_all_new.iloc[3,4]=0\n",
    "df_all_new.iloc[3,5]=0\n",
    "\n",
    "\n",
    "AI_all=np.sum(df_all_new.iloc[0:3,3:].values)\n",
    "reader_all=np.sum(df_all_new.iloc[4:7,3:].values)\n",
    "\n",
    "df_all_new['All findings']=df_all_new['nodules correctly detected']+df_all_new['non-nodules incorrectly detected']+df_all_new['nodules missed']\n",
    "\n",
    "df_all_new.iloc[7,0]=''\n",
    "df_all_new.iloc[7,1]=''\n",
    "df_all_new.iloc[7,2]=''\n",
    "df_all_new.iloc[7,3]=''\n",
    "df_all_new.iloc[7,4]=''\n",
    "df_all_new.iloc[7,5]=''\n",
    "df_all_new.iloc[7,6]=np.sum(df_all_new['All findings'].iloc[4:6])\n",
    "\n",
    "df_all_new.iloc[3,0]=''\n",
    "df_all_new.iloc[3,1]=''\n",
    "df_all_new.iloc[3,2]=''\n",
    "df_all_new.iloc[3,3]=''\n",
    "df_all_new.iloc[3,4]=''\n",
    "df_all_new.iloc[3,5]=''\n",
    "df_all_new.iloc[3,6]=np.sum(df_all_new['All findings'].iloc[0:3])\n",
    "\n",
    "for i in range(7):\n",
    "    if i!=3:\n",
    "        if i<3:\n",
    "            sum_all=AI_all\n",
    "        elif i>3:\n",
    "            sum_all=reader_all\n",
    "            \n",
    "        percentage_tp=np.round((df_all_new.iloc[i][3]/sum_all)*100,1) \n",
    "        df_all_new['nodules correctly detected'].iloc[i]=str(df_all_new.iloc[i][3])+' ('+str(percentage_tp)+'%)'\n",
    "\n",
    "        percentage_fp=np.round((df_all_new.iloc[i][4]/sum_all)*100,1) \n",
    "        df_all_new['non-nodules incorrectly detected'].iloc[i]=str(df_all_new.iloc[i][4])+' ('+str(percentage_fp)+'%)'\n",
    "\n",
    "        percentage_fn=np.round((df_all_new.iloc[i][5]/sum_all)*100,1) \n",
    "        df_all_new['nodules missed'].iloc[i]=str(df_all_new.iloc[i][5])+' ('+str(percentage_fn)+'%)'\n",
    "\n",
    "        df_all_new['All findings'].iloc[i]=str(df_all_new.iloc[i][6])+' ('+str(np.round(100*df_all_new.iloc[i][6]/sum_all,1))+'%)'\n",
    "\n",
    "    \n",
    "df_all_new['All findings'].iloc[3]=str(df_all_new.iloc[3][6])+' (100%)'\n",
    "df_all_new['All findings'].iloc[7]=str(df_all_new.iloc[7][6])+' (100%)'\n",
    "\n",
    "df_all_new #Detection performance comparison for nodules only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb84c085",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_new.to_excel('nodules_only_volumes_low_all.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bcefd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For nodules only (AI vs reader) of 30-100mm3 with continuity correction (not exact) p value is 0.052345063273163295\n",
      "For FP findings of 30-100mm3, with continuity correction (not exact) p value is 0.6650055421020291\n",
      "\n",
      "\n",
      "For nodules only (AI vs reader) of 100-300mm3 with continuity correction (not exact) p value is 0.24821307898992026\n",
      "For FP findings of 100-300mm3, with continuity correction (not exact) p value is 9.269569613019149e-08\n",
      "\n",
      "\n",
      "For nodules only (AI vs reader) of 300+mm3 with continuity correction (not exact) p value is 1.0\n",
      "For FP findings of 300+mm3, with continuity correction (not exact) p value is 2.306187599291998e-06\n"
     ]
    }
   ],
   "source": [
    "data=[[TP_both_100, FN_read_100],\n",
    "        [FN_AI_100,0]]\n",
    "# print(data)\n",
    "\n",
    "# McNemar's Test without continuity correction\n",
    "print(\"For nodules only (AI vs reader) of 30-100mm3 with continuity correction (not exact) p value is\",mcnemar(data, exact=False,correction=True).pvalue)\n",
    "\n",
    "#For FPs\n",
    "data=[[0, FP_AI_100], \n",
    "        [FP_read_100, 0]]\n",
    "# print(data)\n",
    "\n",
    "# McNemar's Test without continuity correction\n",
    "print(\"For FP findings of 30-100mm3, with continuity correction (not exact) p value is\",mcnemar(data, exact=False,correction=True).pvalue)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "data=[[TP_both_100_300,FN_read_100_300], \n",
    "        [FN_AI_100_300, 0]]\n",
    "# print(data)\n",
    "\n",
    "# McNemar's Test without continuity correction\n",
    "print(\"For nodules only (AI vs reader) of 100-300mm3 with continuity correction (not exact) p value is\",mcnemar(data, exact=False,correction=True).pvalue)\n",
    "\n",
    "#For FPs\n",
    "data=[[0, FP_AI_100_300], \n",
    "        [FP_read_100_300, 0]]\n",
    "# print(data)\n",
    "\n",
    "# McNemar's Test without continuity correction\n",
    "print(\"For FP findings of 100-300mm3, with continuity correction (not exact) p value is\",mcnemar(data, exact=False,correction=True).pvalue)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "data=[[TP_both_300,FN_read_300], \n",
    "        [FN_AI_300, 0]]\n",
    "# print(data)\n",
    "\n",
    "# McNemar's Test without continuity correction\n",
    "print(\"For nodules only (AI vs reader) of 300+mm3 with continuity correction (not exact) p value is\",mcnemar(data, exact=False,correction=True).pvalue)\n",
    "\n",
    "#For FPs\n",
    "data=[[0, FP_AI_300], \n",
    "        [FP_read_300, 0]]\n",
    "# print(data)\n",
    "\n",
    "# McNemar's Test without continuity correction\n",
    "print(\"For FP findings of 300+mm3, with continuity correction (not exact) p value is\",mcnemar(data, exact=False,correction=True).pvalue)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20b61338",
   "metadata": {},
   "source": [
    "#### Comparison of volume subgroups for high BMI - Statistical tests based on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9520fd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High BMI numbers\n",
      "TP_AI_100 58\n",
      "FP_AI_100 11\n",
      "FN_AI_100 11\n",
      "TP_read_100 51\n",
      "FP_read_100 8\n",
      "FN_read_100 18\n",
      "TP_AI_100_300 15\n",
      "FP_AI_100_300 27\n",
      "FN_AI_100_300 2\n",
      "TP_read_100_300 13\n",
      "FP_read_100_300 1\n",
      "FN_read_100_300 4\n",
      "TP_AI_300 4\n",
      "FP_AI_300 15\n",
      "FN_AI_300 0\n",
      "TP_read_300 4\n",
      "FP_read_300 0\n",
      "FN_read_300 0\n",
      "TP_both_100 40\n",
      "TP_both_100_300 11\n",
      "TP_both_300 4\n"
     ]
    }
   ],
   "source": [
    "TP_AI_100=TP_high_30_100+ai_only_nods_high_30_100-(TP_lymph_high_30_100) #-ai_lymph_high_30_100\n",
    "FP_AI_100=ai_nonods_high_30_100\n",
    "FN_AI_100=reader_nods_high_30_100-reader_lymph_high_30_100 #nodules of reader excluding lymph nodes\n",
    "\n",
    "TP_read_100=TP_high_30_100+reader_nods_high_30_100-(TP_lymph_high_30_100) - reader_lymph_high_30_100\n",
    "FP_read_100=reader_nonods_high_30_100\n",
    "FN_read_100=ai_only_nods_high_30_100#-ai_lymph_high_30_100 #nodules of AI excluding lymph nodes\n",
    "\n",
    "TP_AI_100_300=TP_high_100_300+ai_only_nods_high_100_300-(TP_lymph_high_100_300)# -(ai_lymph_high_100_300)\n",
    "FP_AI_100_300=ai_nonods_high_100_300\n",
    "FN_AI_100_300=reader_nods_high_100_300-(reader_lymph_high_100_300) #nodules of reader excluding lymph nodes\n",
    "\n",
    "TP_read_100_300=TP_high_100_300+reader_nods_high_100_300-(TP_lymph_high_100_300) - (reader_lymph_high_100_300)\n",
    "FP_read_100_300=reader_nonods_high_100_300\n",
    "FN_read_100_300=ai_only_nods_high_100_300#-(ai_lymph_high_100_300) #nodules of AI excluding lymph nodes\n",
    "\n",
    "TP_AI_300=TP_high_300+ai_only_nods_high_300-(TP_lymph_high_300) #-(ai_lymph_high_300)\n",
    "FP_AI_300=ai_nonods_high_300\n",
    "FN_AI_300=reader_nods_high_300-(reader_lymph_high_300) #nodules of reader excluding lymph nodes\n",
    "\n",
    "TP_read_300=TP_high_300+reader_nods_high_300-(TP_lymph_high_300) - (reader_lymph_high_300)\n",
    "FP_read_300=reader_nonods_high_300\n",
    "FN_read_300=ai_only_nods_high_300#-(ai_lymph_high_300) #nodules of AI excluding lymph nodes\n",
    "\n",
    "\n",
    "TP_both_100=TP_high_30_100-(TP_lymph_high_30_100)\n",
    "TP_both_100_300=TP_high_100_300-(TP_lymph_high_100_300)\n",
    "TP_both_300=TP_high_300-(TP_lymph_high_300)\n",
    "\n",
    "#Print the above\n",
    "print(\"High BMI numbers\")\n",
    "print(\"TP_AI_100\",TP_AI_100)\n",
    "print(\"FP_AI_100\",FP_AI_100)\n",
    "print(\"FN_AI_100\",FN_AI_100)\n",
    "print(\"TP_read_100\",TP_read_100)\n",
    "print(\"FP_read_100\",FP_read_100)\n",
    "print(\"FN_read_100\",FN_read_100)\n",
    "print(\"TP_AI_100_300\",TP_AI_100_300)\n",
    "print(\"FP_AI_100_300\",FP_AI_100_300)\n",
    "print(\"FN_AI_100_300\",FN_AI_100_300)\n",
    "print(\"TP_read_100_300\",TP_read_100_300)\n",
    "print(\"FP_read_100_300\",FP_read_100_300)\n",
    "print(\"FN_read_100_300\",FN_read_100_300)\n",
    "print(\"TP_AI_300\",TP_AI_300)\n",
    "print(\"FP_AI_300\",FP_AI_300)\n",
    "print(\"FN_AI_300\",FN_AI_300)\n",
    "print(\"TP_read_300\",TP_read_300)\n",
    "print(\"FP_read_300\",FP_read_300)\n",
    "print(\"FN_read_300\",FN_read_300)\n",
    "print(\"TP_both_100\",TP_both_100)\n",
    "print(\"TP_both_100_300\",TP_both_100_300)\n",
    "print(\"TP_both_300\",TP_both_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e70c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensitivity (95% CI)</th>\n",
       "      <th>PPV (95% CI)</th>\n",
       "      <th>F1 score (95% CI)</th>\n",
       "      <th>nodules correctly detected</th>\n",
       "      <th>non-nodules incorrectly detected</th>\n",
       "      <th>nodules missed</th>\n",
       "      <th>All findings</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GT by radiologists for discrepancies</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AI, high 30-100mm3</th>\n",
       "      <td>0.84 (0.73, 0.91)</td>\n",
       "      <td>0.84 (0.73, 0.91)</td>\n",
       "      <td>0.84 (0.77, 0.9)</td>\n",
       "      <td>58 (40.6%)</td>\n",
       "      <td>11 (7.7%)</td>\n",
       "      <td>11 (7.7%)</td>\n",
       "      <td>80 (55.9%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AI, high 100-300mm3</th>\n",
       "      <td>0.88 (0.62, 0.98)</td>\n",
       "      <td>0.36 (0.22, 0.52)</td>\n",
       "      <td>0.51 (0.38, 0.64)</td>\n",
       "      <td>15 (10.5%)</td>\n",
       "      <td>27 (18.9%)</td>\n",
       "      <td>2 (1.4%)</td>\n",
       "      <td>44 (30.8%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AI, high 300+mm3</th>\n",
       "      <td>1.0 (0.4, 0.98)</td>\n",
       "      <td>0.21 (0.07, 0.46)</td>\n",
       "      <td>0.35 (0.17, 0.57)</td>\n",
       "      <td>4 (2.8%)</td>\n",
       "      <td>15 (10.5%)</td>\n",
       "      <td>0 (0.0%)</td>\n",
       "      <td>19 (13.3%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>143 (100%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reader, high 30-100mm3</th>\n",
       "      <td>0.74 (0.62, 0.83)</td>\n",
       "      <td>0.86 (0.74, 0.94)</td>\n",
       "      <td>0.8 (0.71, 0.86)</td>\n",
       "      <td>51 (51.5%)</td>\n",
       "      <td>8 (8.1%)</td>\n",
       "      <td>18 (18.2%)</td>\n",
       "      <td>77 (77.8%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reader, high 100-300mm3</th>\n",
       "      <td>0.76 (0.5, 0.92)</td>\n",
       "      <td>0.93 (0.64, 1.0)</td>\n",
       "      <td>0.84 (0.66, 0.94)</td>\n",
       "      <td>13 (13.1%)</td>\n",
       "      <td>1 (1.0%)</td>\n",
       "      <td>4 (4.0%)</td>\n",
       "      <td>18 (18.2%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reader, high 300+mm3</th>\n",
       "      <td>1.0 (0.4, 0.98)</td>\n",
       "      <td>1.0 (0.4, 0.98)</td>\n",
       "      <td>1.0 (0.6, 0.99)</td>\n",
       "      <td>4 (4.0%)</td>\n",
       "      <td>0 (0.0%)</td>\n",
       "      <td>0 (0.0%)</td>\n",
       "      <td>4 (4.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>95 (100%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     sensitivity (95% CI)       PPV (95% CI)  \\\n",
       "GT by radiologists for discrepancies                                           \n",
       "AI, high 30-100mm3                      0.84 (0.73, 0.91)  0.84 (0.73, 0.91)   \n",
       "AI, high 100-300mm3                     0.88 (0.62, 0.98)  0.36 (0.22, 0.52)   \n",
       "AI, high 300+mm3                          1.0 (0.4, 0.98)  0.21 (0.07, 0.46)   \n",
       "                                                                               \n",
       "reader, high 30-100mm3                  0.74 (0.62, 0.83)  0.86 (0.74, 0.94)   \n",
       "reader, high 100-300mm3                  0.76 (0.5, 0.92)   0.93 (0.64, 1.0)   \n",
       "reader, high 300+mm3                      1.0 (0.4, 0.98)    1.0 (0.4, 0.98)   \n",
       "                                                                               \n",
       "\n",
       "                                      F1 score (95% CI)  \\\n",
       "GT by radiologists for discrepancies                      \n",
       "AI, high 30-100mm3                     0.84 (0.77, 0.9)   \n",
       "AI, high 100-300mm3                   0.51 (0.38, 0.64)   \n",
       "AI, high 300+mm3                      0.35 (0.17, 0.57)   \n",
       "                                                          \n",
       "reader, high 30-100mm3                 0.8 (0.71, 0.86)   \n",
       "reader, high 100-300mm3               0.84 (0.66, 0.94)   \n",
       "reader, high 300+mm3                    1.0 (0.6, 0.99)   \n",
       "                                                          \n",
       "\n",
       "                                     nodules correctly detected  \\\n",
       "GT by radiologists for discrepancies                              \n",
       "AI, high 30-100mm3                                   58 (40.6%)   \n",
       "AI, high 100-300mm3                                  15 (10.5%)   \n",
       "AI, high 300+mm3                                       4 (2.8%)   \n",
       "                                                                  \n",
       "reader, high 30-100mm3                               51 (51.5%)   \n",
       "reader, high 100-300mm3                              13 (13.1%)   \n",
       "reader, high 300+mm3                                   4 (4.0%)   \n",
       "                                                                  \n",
       "\n",
       "                                     non-nodules incorrectly detected  \\\n",
       "GT by radiologists for discrepancies                                    \n",
       "AI, high 30-100mm3                                          11 (7.7%)   \n",
       "AI, high 100-300mm3                                        27 (18.9%)   \n",
       "AI, high 300+mm3                                           15 (10.5%)   \n",
       "                                                                        \n",
       "reader, high 30-100mm3                                       8 (8.1%)   \n",
       "reader, high 100-300mm3                                      1 (1.0%)   \n",
       "reader, high 300+mm3                                         0 (0.0%)   \n",
       "                                                                        \n",
       "\n",
       "                                     nodules missed All findings  \n",
       "GT by radiologists for discrepancies                              \n",
       "AI, high 30-100mm3                        11 (7.7%)   80 (55.9%)  \n",
       "AI, high 100-300mm3                        2 (1.4%)   44 (30.8%)  \n",
       "AI, high 300+mm3                           0 (0.0%)   19 (13.3%)  \n",
       "                                                      143 (100%)  \n",
       "reader, high 30-100mm3                   18 (18.2%)   77 (77.8%)  \n",
       "reader, high 100-300mm3                    4 (4.0%)   18 (18.2%)  \n",
       "reader, high 300+mm3                       0 (0.0%)     4 (4.0%)  \n",
       "                                                       95 (100%)  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For non-emphysema only comparison between reader and AI for volume subgroups\n",
    "df_all_new=pd.DataFrame(columns=['sensitivity (95% CI)','PPV (95% CI)','F1 score (95% CI)','nodules correctly detected','non-nodules incorrectly detected','nodules missed'], \n",
    "                        index=['AI, high 30-100mm3', 'AI, high 100-300mm3','AI, high 300+mm3','',\n",
    "                               'reader, high 30-100mm3','reader, high 100-300mm3', 'reader, high 300+mm3',''\n",
    "                              ])\n",
    "\n",
    "df_all_new.index.name = 'GT by radiologists for discrepancies' \n",
    "\n",
    "\n",
    "df_all_new.iloc[0,0]=np.round(sensitivity(TP_AI_100,FN_AI_100),2)\n",
    "df_all_new.iloc[0,1]=np.round(PPV(TP_AI_100,FP_AI_100),2)\n",
    "df_all_new.iloc[0,2]=np.round(F1score(TP_AI_100,FP_AI_100,FN_AI_100),2)\n",
    "df_all_new.iloc[0,3]=TP_AI_100\n",
    "df_all_new.iloc[0,4]=FP_AI_100\n",
    "df_all_new.iloc[0,5]=FN_AI_100\n",
    "\n",
    "sensitivity_confidence_interval_AI, PPV_confidence_interval_AI, F1_confidence_interval_AI \\\n",
    "= sensitivity_and_specificity_with_confidence_intervals(TP_AI_100, FP_AI_100, FN_AI_100, 0, alpha=0.95)\n",
    "\n",
    "ci_sens_ai=[np.round(x,2) for x in sensitivity_confidence_interval_AI]\n",
    "ci_ppv_ai=[np.round(x,2) for x in PPV_confidence_interval_AI]\n",
    "ci_f1_ai=[np.round(x,2) for x in F1_confidence_interval_AI]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[0]=str(df_all_new['sensitivity (95% CI)'].iloc[0])+' '+str(tuple(ci_sens_ai))\n",
    "df_all_new['PPV (95% CI)'].iloc[0]=str(df_all_new['PPV (95% CI)'].iloc[0])+' '+str(tuple(ci_ppv_ai))\n",
    "df_all_new['F1 score (95% CI)'].iloc[0]=str(df_all_new['F1 score (95% CI)'].iloc[0])+' '+str(tuple(ci_f1_ai))\n",
    "\n",
    "\n",
    "df_all_new.iloc[4,0]=np.round(sensitivity(TP_read_100,FN_read_100),2)\n",
    "df_all_new.iloc[4,1]=np.round(PPV(TP_read_100,FP_read_100),2)\n",
    "df_all_new.iloc[4,2]=np.round(F1score(TP_read_100,FP_read_100,FN_read_100),2)\n",
    "df_all_new.iloc[4,3]=TP_read_100\n",
    "df_all_new.iloc[4,4]=FP_read_100\n",
    "df_all_new.iloc[4,5]=FN_read_100\n",
    "\n",
    "sensitivity_confidence_interval_AI, PPV_confidence_interval_AI, F1_confidence_interval_AI \\\n",
    "= sensitivity_and_specificity_with_confidence_intervals(TP_read_100, FP_read_100, FN_read_100, 0, alpha=0.95)\n",
    "\n",
    "ci_sens_ai=[np.round(x,2) for x in sensitivity_confidence_interval_AI]\n",
    "ci_ppv_ai=[np.round(x,2) for x in PPV_confidence_interval_AI]\n",
    "ci_f1_ai=[np.round(x,2) for x in F1_confidence_interval_AI]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[4]=str(df_all_new['sensitivity (95% CI)'].iloc[4])+' '+str(tuple(ci_sens_ai))\n",
    "df_all_new['PPV (95% CI)'].iloc[4]=str(df_all_new['PPV (95% CI)'].iloc[4])+' '+str(tuple(ci_ppv_ai))\n",
    "df_all_new['F1 score (95% CI)'].iloc[4]=str(df_all_new['F1 score (95% CI)'].iloc[4])+' '+str(tuple(ci_f1_ai))\n",
    "\n",
    "\n",
    "\n",
    "df_all_new.iloc[1,0]=np.round(sensitivity(TP_AI_100_300,FN_AI_100_300),2)\n",
    "df_all_new.iloc[1,1]=np.round(PPV(TP_AI_100_300,FP_AI_100_300),2)\n",
    "df_all_new.iloc[1,2]=np.round(F1score(TP_AI_100_300,FP_AI_100_300,FN_AI_100_300),2)\n",
    "df_all_new.iloc[1,3]=TP_AI_100_300\n",
    "df_all_new.iloc[1,4]=FP_AI_100_300\n",
    "df_all_new.iloc[1,5]=FN_AI_100_300\n",
    "\n",
    "sensitivity_confidence_interval_AI, PPV_confidence_interval_AI, F1_confidence_interval_AI \\\n",
    "= sensitivity_and_specificity_with_confidence_intervals(TP_AI_100_300, FP_AI_100_300, FN_AI_100_300, 0, alpha=0.95)\n",
    "\n",
    "ci_sens_ai=[np.round(x,2) for x in sensitivity_confidence_interval_AI]\n",
    "ci_ppv_ai=[np.round(x,2) for x in PPV_confidence_interval_AI]\n",
    "ci_f1_ai=[np.round(x,2) for x in F1_confidence_interval_AI]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[1]=str(df_all_new['sensitivity (95% CI)'].iloc[1])+' '+str(tuple(ci_sens_ai))\n",
    "df_all_new['PPV (95% CI)'].iloc[1]=str(df_all_new['PPV (95% CI)'].iloc[1])+' '+str(tuple(ci_ppv_ai))\n",
    "df_all_new['F1 score (95% CI)'].iloc[1]=str(df_all_new['F1 score (95% CI)'].iloc[1])+' '+str(tuple(ci_f1_ai))\n",
    "\n",
    "\n",
    "\n",
    "df_all_new.iloc[5,0]=np.round(sensitivity(TP_read_100_300,FN_read_100_300),2)\n",
    "df_all_new.iloc[5,1]=np.round(PPV(TP_read_100_300,FP_read_100_300),2)\n",
    "df_all_new.iloc[5,2]=np.round(F1score(TP_read_100_300,FP_read_100_300,FN_read_100_300),2)\n",
    "df_all_new.iloc[5,3]=TP_read_100_300\n",
    "df_all_new.iloc[5,4]=FP_read_100_300\n",
    "df_all_new.iloc[5,5]=FN_read_100_300\n",
    "\n",
    "sensitivity_confidence_interval_AI, PPV_confidence_interval_AI, F1_confidence_interval_AI \\\n",
    "= sensitivity_and_specificity_with_confidence_intervals(TP_read_100_300, FP_read_100_300, FN_read_100_300, 0, alpha=0.95)\n",
    "\n",
    "ci_sens_ai=[np.round(x,2) for x in sensitivity_confidence_interval_AI]\n",
    "ci_ppv_ai=[np.round(x,2) for x in PPV_confidence_interval_AI]\n",
    "ci_f1_ai=[np.round(x,2) for x in F1_confidence_interval_AI]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[5]=str(df_all_new['sensitivity (95% CI)'].iloc[5])+' '+str(tuple(ci_sens_ai))\n",
    "df_all_new['PPV (95% CI)'].iloc[5]=str(df_all_new['PPV (95% CI)'].iloc[5])+' '+str(tuple(ci_ppv_ai))\n",
    "df_all_new['F1 score (95% CI)'].iloc[5]=str(df_all_new['F1 score (95% CI)'].iloc[5])+' '+str(tuple(ci_f1_ai))\n",
    "\n",
    "\n",
    "df_all_new.iloc[2,0]=np.round(sensitivity(TP_AI_300,FN_AI_300),2)\n",
    "df_all_new.iloc[2,1]=np.round(PPV(TP_AI_300,FP_AI_300),2)\n",
    "df_all_new.iloc[2,2]=np.round(F1score(TP_AI_300,FP_AI_300,FN_AI_300),2)\n",
    "df_all_new.iloc[2,3]=TP_AI_300\n",
    "df_all_new.iloc[2,4]=FP_AI_300\n",
    "df_all_new.iloc[2,5]=FN_AI_300\n",
    "\n",
    "sensitivity_confidence_interval_AI, PPV_confidence_interval_AI, F1_confidence_interval_AI \\\n",
    "= sensitivity_and_specificity_with_confidence_intervals(TP_AI_300, FP_AI_300, FN_AI_300, 0, alpha=0.95)\n",
    "\n",
    "ci_sens_ai=[np.round(x,2) for x in sensitivity_confidence_interval_AI]\n",
    "ci_ppv_ai=[np.round(x,2) for x in PPV_confidence_interval_AI]\n",
    "ci_f1_ai=[np.round(x,2) for x in F1_confidence_interval_AI]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[2]=str(df_all_new['sensitivity (95% CI)'].iloc[2])+' '+str(tuple(ci_sens_ai))\n",
    "df_all_new['PPV (95% CI)'].iloc[2]=str(df_all_new['PPV (95% CI)'].iloc[2])+' '+str(tuple(ci_ppv_ai))\n",
    "df_all_new['F1 score (95% CI)'].iloc[2]=str(df_all_new['F1 score (95% CI)'].iloc[2])+' '+str(tuple(ci_f1_ai))\n",
    "\n",
    "\n",
    "df_all_new.iloc[6,0]=np.round(sensitivity(TP_read_300,FN_read_300),2)\n",
    "df_all_new.iloc[6,1]=np.round(PPV(TP_read_300,FP_read_300),2)\n",
    "df_all_new.iloc[6,2]=np.round(F1score(TP_read_300,FP_read_300,FN_read_300),2)\n",
    "df_all_new.iloc[6,3]=TP_read_300\n",
    "df_all_new.iloc[6,4]=FP_read_300\n",
    "df_all_new.iloc[6,5]=FN_read_300\n",
    "\n",
    "sensitivity_confidence_interval_AI, PPV_confidence_interval_AI, F1_confidence_interval_AI \\\n",
    "= sensitivity_and_specificity_with_confidence_intervals(TP_read_300, FP_read_300, FN_read_300, 0, alpha=0.95)\n",
    "\n",
    "ci_sens_ai=[np.round(x,2) for x in sensitivity_confidence_interval_AI]\n",
    "ci_ppv_ai=[np.round(x,2) for x in PPV_confidence_interval_AI]\n",
    "ci_f1_ai=[np.round(x,2) for x in F1_confidence_interval_AI]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[6]=str(df_all_new['sensitivity (95% CI)'].iloc[6])+' '+str(tuple(ci_sens_ai))\n",
    "df_all_new['PPV (95% CI)'].iloc[6]=str(df_all_new['PPV (95% CI)'].iloc[6])+' '+str(tuple(ci_ppv_ai))\n",
    "df_all_new['F1 score (95% CI)'].iloc[6]=str(df_all_new['F1 score (95% CI)'].iloc[6])+' '+str(tuple(ci_f1_ai))\n",
    "\n",
    "\n",
    "df_all_new.iloc[3,0]=0\n",
    "df_all_new.iloc[3,1]=0\n",
    "df_all_new.iloc[3,2]=0\n",
    "df_all_new.iloc[3,3]=0\n",
    "df_all_new.iloc[3,4]=0\n",
    "df_all_new.iloc[3,5]=0\n",
    "\n",
    "\n",
    "AI_all=np.sum(df_all_new.iloc[0:3,3:].values)\n",
    "reader_all=np.sum(df_all_new.iloc[4:7,3:].values)\n",
    "\n",
    "df_all_new['All findings']=df_all_new['nodules correctly detected']+df_all_new['non-nodules incorrectly detected']+df_all_new['nodules missed']\n",
    "\n",
    "df_all_new.iloc[7,0]=''\n",
    "df_all_new.iloc[7,1]=''\n",
    "df_all_new.iloc[7,2]=''\n",
    "df_all_new.iloc[7,3]=''\n",
    "df_all_new.iloc[7,4]=''\n",
    "df_all_new.iloc[7,5]=''\n",
    "df_all_new.iloc[7,6]=np.sum(df_all_new['All findings'].iloc[4:6])\n",
    "\n",
    "df_all_new.iloc[3,0]=''\n",
    "df_all_new.iloc[3,1]=''\n",
    "df_all_new.iloc[3,2]=''\n",
    "df_all_new.iloc[3,3]=''\n",
    "df_all_new.iloc[3,4]=''\n",
    "df_all_new.iloc[3,5]=''\n",
    "df_all_new.iloc[3,6]=np.sum(df_all_new['All findings'].iloc[0:3])\n",
    "\n",
    "for i in range(7):\n",
    "    if i!=3:\n",
    "        if i<3:\n",
    "            sum_all=AI_all\n",
    "        elif i>3:\n",
    "            sum_all=reader_all\n",
    "            \n",
    "        percentage_tp=np.round((df_all_new.iloc[i][3]/sum_all)*100,1) \n",
    "        df_all_new['nodules correctly detected'].iloc[i]=str(df_all_new.iloc[i][3])+' ('+str(percentage_tp)+'%)'\n",
    "\n",
    "        percentage_fp=np.round((df_all_new.iloc[i][4]/sum_all)*100,1) \n",
    "        df_all_new['non-nodules incorrectly detected'].iloc[i]=str(df_all_new.iloc[i][4])+' ('+str(percentage_fp)+'%)'\n",
    "\n",
    "        percentage_fn=np.round((df_all_new.iloc[i][5]/sum_all)*100,1) \n",
    "        df_all_new['nodules missed'].iloc[i]=str(df_all_new.iloc[i][5])+' ('+str(percentage_fn)+'%)'\n",
    "\n",
    "        df_all_new['All findings'].iloc[i]=str(df_all_new.iloc[i][6])+' ('+str(np.round(100*df_all_new.iloc[i][6]/sum_all,1))+'%)'\n",
    "\n",
    "    \n",
    "df_all_new['All findings'].iloc[3]=str(df_all_new.iloc[3][6])+' (100%)'\n",
    "df_all_new['All findings'].iloc[7]=str(df_all_new.iloc[7][6])+' (100%)'\n",
    "\n",
    "df_all_new #Detection performance comparison for nodules only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc42e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_new.to_excel('nodules_only_volumes_high_all.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52338c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For nodules only (AI vs reader) of 30-100mm3 with continuity correction (not exact) p value is 0.2652053925915044\n",
      "For FP findings of 30-100mm3, with continuity correction (not exact) p value is 0.6463551955394902\n",
      "\n",
      "\n",
      "For nodules only (AI vs reader) of 100-300mm3 with continuity correction (not exact) p value is 0.6830913983096086\n",
      "For FP findings of 100-300mm3, with continuity correction (not exact) p value is 2.306187599291998e-06\n",
      "\n",
      "\n",
      "For nodules only (AI vs reader) of 300+mm3 with continuity correction (not exact) p value is 0.0\n",
      "For FP findings of 300+mm3, with continuity correction (not exact) p value is 0.0003005976074404506\n"
     ]
    }
   ],
   "source": [
    "data=[[TP_both_100, FN_read_100],\n",
    "        [FN_AI_100,0]]\n",
    "# print(data)\n",
    "\n",
    "# McNemar's Test without continuity correction\n",
    "print(\"For nodules only (AI vs reader) of 30-100mm3 with continuity correction (not exact) p value is\",mcnemar(data, exact=False,correction=True).pvalue)\n",
    "\n",
    "#For FPs\n",
    "data=[[0, FP_AI_100], \n",
    "        [FP_read_100, 0]]\n",
    "# print(data)\n",
    "\n",
    "# McNemar's Test without continuity correction\n",
    "print(\"For FP findings of 30-100mm3, with continuity correction (not exact) p value is\",mcnemar(data, exact=False,correction=True).pvalue)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "data=[[TP_both_100_300,FN_read_100_300 ], \n",
    "        [FN_AI_100_300, 0]]\n",
    "# print(data)\n",
    "\n",
    "# McNemar's Test without continuity correction\n",
    "print(\"For nodules only (AI vs reader) of 100-300mm3 with continuity correction (not exact) p value is\",mcnemar(data, exact=False,correction=True).pvalue)\n",
    "\n",
    "#For FPs\n",
    "data=[[0, FP_AI_100_300], \n",
    "        [FP_read_100_300, 0]]\n",
    "# print(data)\n",
    "\n",
    "# McNemar's Test without continuity correction\n",
    "print(\"For FP findings of 100-300mm3, with continuity correction (not exact) p value is\",mcnemar(data, exact=False,correction=True).pvalue)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "data=[[TP_both_300,FN_read_300], \n",
    "        [FN_AI_300, 0]]\n",
    "# print(data)\n",
    "\n",
    "# McNemar's Test without continuity correction\n",
    "print(\"For nodules only (AI vs reader) of 300+mm3 with continuity correction (not exact) p value is\",mcnemar(data, exact=False,correction=True).pvalue)\n",
    "\n",
    "#For FPs\n",
    "data=[[0, FP_AI_300], \n",
    "        [FP_read_300, 0]]\n",
    "# print(data)\n",
    "\n",
    "# McNemar's Test without continuity correction\n",
    "print(\"For FP findings of 300+mm3, with continuity correction (not exact) p value is\",mcnemar(data, exact=False,correction=True).pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44900c3b",
   "metadata": {},
   "source": [
    "Analysis based on volume for subcategories not possible since we only have volume subgroups for TPs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2f1495d",
   "metadata": {},
   "source": [
    "Nodule types - Here e.g. 'atypical_triangular_low_nod_only' instead of 'atypical_triangular_low_lymph'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68e888e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Both High and Low BMI\n",
    "#Further analysis for nodule/lymph node subcategories - Not kept for now\n",
    "#Detailed analysis of what detected or not from both AI and reader for each category in nodules & lymph nodes \n",
    "\n",
    "df_categories=pd.DataFrame(columns=['TP','FP','FN'], #below index with the correct order as above\n",
    "                          index=['pleural nodules',\n",
    "                                 'calcified nodules',\n",
    "                                 'subsolid & ground glass nodules',\n",
    "                                 'other nodules',\n",
    "                                 'cancer',\n",
    "                                 'atypical PFNs'\n",
    "                                ])\n",
    "\n",
    "# df_categories.index.name = 'GT by radiologists for discrepancies'\n",
    "\n",
    "df_categories['FP']=[sum([len(x) for x in pleural_FP_low.values()])+sum([len(x) for x in pleural_FP_high.values()]),\n",
    "                     sum([len(x) for x in calcif_FP_low.values()])+sum([len(x) for x in calcif_FP_high.values()]),\n",
    "                     sum([len(x) for x in sub_ground_FP_low.values()])+sum([len(x) for x in sub_ground_FP_high.values()]),\n",
    "                     sum([len(x) for x in other_nodules_FP_low.values()])+sum([len(x) for x in other_nodules_FP_high.values()]),\n",
    "                     sum([len(x) for x in cancer_FP_low.values()])+sum([len(x) for x in cancer_FP_high.values()]),\n",
    "                     sum([len(x) for x in atyp_FP_low.values()])+sum([len(x) for x in atyp_FP_high.values()])]\n",
    "\n",
    "df_categories['FN']=[sum([len(x) for x in pleural_FN_low.values()])+sum([len(x) for x in pleural_FN_high.values()]),\n",
    "                     sum([len(x) for x in calcif_FN_low.values()])+sum([len(x) for x in calcif_FN_high.values()]),\n",
    "                     sum([len(x) for x in sub_ground_FN_low.values()])+sum([len(x) for x in sub_ground_FN_high.values()]),\n",
    "                     sum([len(x) for x in other_nodules_FN_low.values()])+sum([len(x) for x in other_nodules_FN_high.values()]),\n",
    "                     sum([len(x) for x in cancer_FN_low.values()])+sum([len(x) for x in cancer_FN_high.values()]),\n",
    "                     sum([len(x) for x in atyp_FN_low.values()])+sum([len(x) for x in atyp_FN_high.values()])]\n",
    "\n",
    "df_categories['TP']=[pleural_low_nod_only+pleural_high_nod_only,\n",
    "                     calcified_low_nod_only+calcified_high_nod_only,\n",
    "                     sub_ground_low_nod_only+sub_ground_high_nod_only,\n",
    "                     other_all_low_nod_only+other_all_high_nod_only,\n",
    "                     0,\n",
    "                     atypical_triangular_low_nod_only+atypical_triangular_high_nod_only]\n",
    "\n",
    "df_categories['All findings']=df_categories['FP']+df_categories['FN']+df_categories['TP']\n",
    "\n",
    "df_categories.loc['Total']= df_categories.sum()\n",
    "\n",
    "total_num_discrepancies_with_tp=df_categories.iloc[:-1,:-1].sum().sum() #To be used in next cells for percentages\n",
    "\n",
    "all_findings=df_categories.iloc[:-1,:-1].sum().sum()\n",
    "\n",
    "percentage_fp=np.round((df_categories['FP']/total_num_discrepancies_with_tp)*100,1) \n",
    "df_categories['FP']=[str(value[1])+' ('+str(percentage_fp[index])+'%)' for index,value in enumerate(df_categories['FP'].items())]\n",
    "\n",
    "percentage_fn=np.round((df_categories['FN']/total_num_discrepancies_with_tp)*100,1) \n",
    "df_categories['FN']=[str(value[1])+' ('+str(percentage_fn[index])+'%)' for index,value in enumerate(df_categories['FN'].items())]\n",
    "\n",
    "percentage_fn=np.round((df_categories['TP']/total_num_discrepancies_with_tp)*100,1) \n",
    "df_categories['TP']=[str(value[1])+' ('+str(percentage_fn[index])+'%)' for index,value in enumerate(df_categories['TP'].items())]\n",
    "\n",
    "df_categories['All findings']=[str(val)+' ('+str(np.round(100*val/total_num_discrepancies_with_tp,1))+'%)' for val in df_categories['All findings'].values]\n",
    "\n",
    "#Rename columns\n",
    "df_categories.rename(columns={'FP': 'AI found, reader missed', 'FN': 'AI missed, reader found', 'TP':'Both found'}, inplace=True)\n",
    "\n",
    "df_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3bd357",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categories.to_excel('nodule_types_all.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "424218e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Low BMI only\n",
    "#Further analysis for nodule/lymph node subcategories - Not kept for now\n",
    "#Detailed analysis of what detected or not from both AI and reader for each category in nodules & lymph nodes \n",
    "\n",
    "df_categories=pd.DataFrame(columns=['TP','FP','FN'], #below index with the correct order as above\n",
    "                          index=['pleural nodules',\n",
    "                                 'calcified nodules',\n",
    "                                 'subsolid & ground glass nodules',\n",
    "                                 'other nodules',\n",
    "                                 'cancer',\n",
    "                                 'atypical PFNs'\n",
    "                                ])\n",
    "\n",
    "# df_categories.index.name = 'GT by radiologists for discrepancies'\n",
    "\n",
    "df_categories['FP']=[sum([len(x) for x in pleural_FP_low.values()]),\n",
    "                     sum([len(x) for x in calcif_FP_low.values()]),\n",
    "                     sum([len(x) for x in sub_ground_FP_low.values()]),\n",
    "                     sum([len(x) for x in other_nodules_FP_low.values()]),\n",
    "                     sum([len(x) for x in cancer_FP_low.values()]),\n",
    "                     sum([len(x) for x in atyp_FP_low.values()])]\n",
    "\n",
    "df_categories['FN']=[sum([len(x) for x in pleural_FN_low.values()]),\n",
    "                     sum([len(x) for x in calcif_FN_low.values()]),\n",
    "                     sum([len(x) for x in sub_ground_FN_low.values()]),\n",
    "                     sum([len(x) for x in other_nodules_FN_low.values()]),\n",
    "                     sum([len(x) for x in cancer_FN_low.values()]),\n",
    "                     sum([len(x) for x in atyp_FN_low.values()])]\n",
    "\n",
    "df_categories['TP']=[pleural_low_nod_only,\n",
    "                     calcified_low_nod_only,\n",
    "                     sub_ground_low_nod_only,\n",
    "                     other_all_low_nod_only,\n",
    "                     0,\n",
    "                     atypical_triangular_low_nod_only]\n",
    "\n",
    "df_categories['All findings']=df_categories['FP']+df_categories['FN']+df_categories['TP']\n",
    "\n",
    "df_categories.loc['Total']= df_categories.sum()\n",
    "\n",
    "total_num_discrepancies_with_tp=df_categories.iloc[:-1,:-1].sum().sum() #To be used in next cells for percentages\n",
    "\n",
    "all_findings=df_categories.iloc[:-1,:-1].sum().sum()\n",
    "\n",
    "percentage_fp=np.round((df_categories['FP']/total_num_discrepancies_with_tp)*100,1) \n",
    "df_categories['FP']=[str(value[1])+' ('+str(percentage_fp[index])+'%)' for index,value in enumerate(df_categories['FP'].items())]\n",
    "\n",
    "percentage_fn=np.round((df_categories['FN']/total_num_discrepancies_with_tp)*100,1) \n",
    "df_categories['FN']=[str(value[1])+' ('+str(percentage_fn[index])+'%)' for index,value in enumerate(df_categories['FN'].items())]\n",
    "\n",
    "percentage_fn=np.round((df_categories['TP']/total_num_discrepancies_with_tp)*100,1) \n",
    "df_categories['TP']=[str(value[1])+' ('+str(percentage_fn[index])+'%)' for index,value in enumerate(df_categories['TP'].items())]\n",
    "\n",
    "df_categories['All findings']=[str(val)+' ('+str(np.round(100*val/total_num_discrepancies_with_tp,1))+'%)' for val in df_categories['All findings'].values]\n",
    "\n",
    "#Rename columns\n",
    "df_categories.rename(columns={'FP': 'AI found, reader missed', 'FN': 'AI missed, reader found', 'TP':'Both found'}, inplace=True)\n",
    "\n",
    "df_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16547137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_categories.to_excel('nodule_types_low.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eda76e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#High BMI\n",
    "#Further analysis for nodule/lymph node subcategories - Not kept for now\n",
    "#Detailed analysis of what detected or not from both AI and reader for each category in nodules & lymph nodes \n",
    "\n",
    "df_categories=pd.DataFrame(columns=['TP','FP','FN'], #below index with the correct order as above\n",
    "                          index=['pleural nodules',\n",
    "                                 'calcified nodules',\n",
    "                                 'subsolid & ground glass nodules',\n",
    "                                 'other nodules',\n",
    "                                 'cancer',\n",
    "                                 'atypical PFNs'\n",
    "                                ])\n",
    "\n",
    "# df_categories.index.name = 'GT by radiologists for discrepancies'\n",
    "\n",
    "df_categories['FP']=[sum([len(x) for x in pleural_FP_high.values()]),\n",
    "                     sum([len(x) for x in calcif_FP_high.values()]),\n",
    "                     sum([len(x) for x in sub_ground_FP_high.values()]),\n",
    "                     sum([len(x) for x in other_nodules_FP_high.values()]),\n",
    "                     sum([len(x) for x in cancer_FP_high.values()]),\n",
    "                     sum([len(x) for x in atyp_FP_high.values()])]\n",
    "\n",
    "df_categories['FN']=[sum([len(x) for x in pleural_FN_high.values()]),\n",
    "                     sum([len(x) for x in calcif_FN_high.values()]),\n",
    "                     sum([len(x) for x in sub_ground_FN_high.values()]),\n",
    "                     sum([len(x) for x in other_nodules_FN_high.values()]),\n",
    "                     sum([len(x) for x in cancer_FN_high.values()]),\n",
    "                     sum([len(x) for x in atyp_FN_high.values()])]\n",
    "\n",
    "df_categories['TP']=[pleural_high_nod_only,\n",
    "                     calcified_high_nod_only,\n",
    "                     sub_ground_high_nod_only,\n",
    "                     other_all_high_nod_only,\n",
    "                     0,\n",
    "                     atypical_triangular_high_nod_only]\n",
    "\n",
    "df_categories['All findings']=df_categories['FP']+df_categories['FN']+df_categories['TP']\n",
    "\n",
    "df_categories.loc['Total']= df_categories.sum()\n",
    "\n",
    "total_num_discrepancies_with_tp=df_categories.iloc[:-1,:-1].sum().sum() #To be used in next cells for percentages\n",
    "\n",
    "all_findings=df_categories.iloc[:-1,:-1].sum().sum()\n",
    "\n",
    "percentage_fp=np.round((df_categories['FP']/total_num_discrepancies_with_tp)*100,1) \n",
    "df_categories['FP']=[str(value[1])+' ('+str(percentage_fp[index])+'%)' for index,value in enumerate(df_categories['FP'].items())]\n",
    "\n",
    "percentage_fn=np.round((df_categories['FN']/total_num_discrepancies_with_tp)*100,1) \n",
    "df_categories['FN']=[str(value[1])+' ('+str(percentage_fn[index])+'%)' for index,value in enumerate(df_categories['FN'].items())]\n",
    "\n",
    "percentage_fn=np.round((df_categories['TP']/total_num_discrepancies_with_tp)*100,1) \n",
    "df_categories['TP']=[str(value[1])+' ('+str(percentage_fn[index])+'%)' for index,value in enumerate(df_categories['TP'].items())]\n",
    "\n",
    "df_categories['All findings']=[str(val)+' ('+str(np.round(100*val/total_num_discrepancies_with_tp,1))+'%)' for val in df_categories['All findings'].values]\n",
    "\n",
    "#Rename columns\n",
    "df_categories.rename(columns={'FP': 'AI found, reader missed', 'FN': 'AI missed, reader found', 'TP':'Both found'}, inplace=True)\n",
    "\n",
    "df_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b17fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_categories.to_excel('nodule_types_high.xlsx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb642297",
   "metadata": {},
   "source": [
    "Benign lymph node types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0737c5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Further analysis for nodule/lymph node subcategories - Not kept for now\n",
    "#Detailed analysis of what detected or not from both AI and reader for each category in nodules & lymph nodes \n",
    "\n",
    "df_categories=pd.DataFrame(columns=['TP','FP','FN'], #below index with the correct order as above\n",
    "                          index=['typical PFNs & periphysural lymph nodes',\n",
    "                                 'bronchiovascular lymph nodes'\n",
    "                                ])\n",
    "\n",
    "# df_categories.index.name = 'GT by radiologists for discrepancies'\n",
    "\n",
    "df_categories['FP']=[#sum([len(x) for x in atyp_FP_low.values()])+sum([len(x) for x in atyp_FP_high.values()]),\n",
    "                     sum([len(x) for x in per_FP_low.values()])+sum([len(x) for x in per_FP_high.values()]),\n",
    "                     sum([len(x) for x in bronchioperi_FP_low.values()])+sum([len(x) for x in bronchioperi_FP_high.values()])]\n",
    "\n",
    "df_categories['FN']=[#sum([len(x) for x in atyp_FN_low.values()])+sum([len(x) for x in atyp_FN_high.values()]),\n",
    "                     sum([len(x) for x in per_FN_low.values()])+sum([len(x) for x in per_FN_high.values()]),\n",
    "                     sum([len(x) for x in bronchioperi_FN_low.values()])+sum([len(x) for x in bronchioperi_FN_high.values()])]\n",
    "\n",
    "df_categories['TP']=[#atypical_triangular_low_nod_only+atypical_triangular_high_nod_only,\n",
    "                     per_fisu_low_lymph+per_fisu_high_lymph,\n",
    "                     peri_bronch_low_lymph+peri_bronch_high_lymph]\n",
    "\n",
    "df_categories['All findings']=df_categories['FP']+df_categories['FN']+df_categories['TP']\n",
    "\n",
    "df_categories.loc['Total']= df_categories.sum()\n",
    "\n",
    "total_num_discrepancies_with_tp=df_categories.iloc[:-1,:-1].sum().sum() #To be used in next cells for percentages\n",
    "\n",
    "all_findings=df_categories.iloc[:-1,:-1].sum().sum()\n",
    "\n",
    "percentage_fp=np.round((df_categories['FP']/total_num_discrepancies_with_tp)*100,1) \n",
    "df_categories['FP']=[str(value[1])+' ('+str(percentage_fp[index])+'%)' for index,value in enumerate(df_categories['FP'].items())]\n",
    "\n",
    "percentage_fn=np.round((df_categories['FN']/total_num_discrepancies_with_tp)*100,1) \n",
    "df_categories['FN']=[str(value[1])+' ('+str(percentage_fn[index])+'%)' for index,value in enumerate(df_categories['FN'].items())]\n",
    "\n",
    "percentage_fn=np.round((df_categories['TP']/total_num_discrepancies_with_tp)*100,1) \n",
    "df_categories['TP']=[str(value[1])+' ('+str(percentage_fn[index])+'%)' for index,value in enumerate(df_categories['TP'].items())]\n",
    "\n",
    "df_categories['All findings']=[str(val)+' ('+str(np.round(100*val/total_num_discrepancies_with_tp,1))+'%)' for val in df_categories['All findings'].values]\n",
    "\n",
    "#Rename columns\n",
    "df_categories.rename(columns={'FP': 'AI found, reader missed', 'FN': 'AI missed, reader found', 'TP':'Both found'}, inplace=True)\n",
    "\n",
    "df_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2415d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categories.to_excel('lymph_types_all.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a30a137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Low BMI\n",
    "#Further analysis for nodule/lymph node subcategories - Not kept for now\n",
    "#Detailed analysis of what detected or not from both AI and reader for each category in nodules & lymph nodes \n",
    "\n",
    "df_categories=pd.DataFrame(columns=['TP','FP','FN'], #below index with the correct order as above\n",
    "                          index=[#'atypical PFNs',\n",
    "                                 'typical PFNs & periphysural lymph nodes',\n",
    "                                 'bronchiovascular lymph nodes'\n",
    "                                ])\n",
    "\n",
    "# df_categories.index.name = 'GT by radiologists for discrepancies'\n",
    "\n",
    "df_categories['FP']=[#sum([len(x) for x in atyp_FP_low.values()]),\n",
    "                     sum([len(x) for x in per_FP_low.values()]),\n",
    "                     sum([len(x) for x in bronchioperi_FP_low.values()])]\n",
    "\n",
    "df_categories['FN']=[#sum([len(x) for x in atyp_FN_low.values()]),\n",
    "                     sum([len(x) for x in per_FN_low.values()]),\n",
    "                     sum([len(x) for x in bronchioperi_FN_low.values()])]\n",
    "\n",
    "df_categories['TP']=[#atypical_triangular_low_nod_only,\n",
    "                     per_fisu_low_lymph,\n",
    "                     peri_bronch_low_lymph]\n",
    "\n",
    "df_categories['All findings']=df_categories['FP']+df_categories['FN']+df_categories['TP']\n",
    "\n",
    "df_categories.loc['Total']= df_categories.sum()\n",
    "\n",
    "total_num_discrepancies_with_tp=df_categories.iloc[:-1,:-1].sum().sum() #To be used in next cells for percentages\n",
    "\n",
    "all_findings=df_categories.iloc[:-1,:-1].sum().sum()\n",
    "\n",
    "percentage_fp=np.round((df_categories['FP']/total_num_discrepancies_with_tp)*100,1) \n",
    "df_categories['FP']=[str(value[1])+' ('+str(percentage_fp[index])+'%)' for index,value in enumerate(df_categories['FP'].items())]\n",
    "\n",
    "percentage_fn=np.round((df_categories['FN']/total_num_discrepancies_with_tp)*100,1) \n",
    "df_categories['FN']=[str(value[1])+' ('+str(percentage_fn[index])+'%)' for index,value in enumerate(df_categories['FN'].items())]\n",
    "\n",
    "percentage_fn=np.round((df_categories['TP']/total_num_discrepancies_with_tp)*100,1) \n",
    "df_categories['TP']=[str(value[1])+' ('+str(percentage_fn[index])+'%)' for index,value in enumerate(df_categories['TP'].items())]\n",
    "\n",
    "df_categories['All findings']=[str(val)+' ('+str(np.round(100*val/total_num_discrepancies_with_tp,1))+'%)' for val in df_categories['All findings'].values]\n",
    "\n",
    "#Rename columns\n",
    "df_categories.rename(columns={'FP': 'AI found, reader missed', 'FN': 'AI missed, reader found', 'TP':'Both found'}, inplace=True)\n",
    "\n",
    "df_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c10fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_categories.to_excel('lymph_types_low.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3acfaed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#High BMI\n",
    "#Further analysis for nodule/lymph node subcategories - Not kept for now\n",
    "#Detailed analysis of what detected or not from both AI and reader for each category in nodules & lymph nodes \n",
    "\n",
    "df_categories=pd.DataFrame(columns=['TP','FP','FN'], #below index with the correct order as above\n",
    "                          index=[#'atypical PFNs',\n",
    "                                 'typical PFNs & periphysural lymph nodes',\n",
    "                                 'bronchiovascular lymph nodes'\n",
    "                                ])\n",
    "\n",
    "# df_categories.index.name = 'GT by radiologists for discrepancies'\n",
    "\n",
    "df_categories['FP']=[#sum([len(x) for x in atyp_FP_high.values()]),\n",
    "                     sum([len(x) for x in per_FP_high.values()]),\n",
    "                     sum([len(x) for x in bronchioperi_FP_high.values()])]\n",
    "\n",
    "df_categories['FN']=[#sum([len(x) for x in atyp_FN_high.values()]),\n",
    "                     sum([len(x) for x in per_FN_high.values()]),\n",
    "                     sum([len(x) for x in bronchioperi_FN_high.values()])]\n",
    "\n",
    "df_categories['TP']=[#atypical_triangular_high_nod_only,\n",
    "                    per_fisu_high_lymph,\n",
    "                     peri_bronch_high_lymph]\n",
    "\n",
    "df_categories['All findings']=df_categories['FP']+df_categories['FN']+df_categories['TP']\n",
    "\n",
    "df_categories.loc['Total']= df_categories.sum()\n",
    "\n",
    "total_num_discrepancies_with_tp=df_categories.iloc[:-1,:-1].sum().sum() #To be used in next cells for percentages\n",
    "\n",
    "all_findings=df_categories.iloc[:-1,:-1].sum().sum()\n",
    "\n",
    "percentage_fp=np.round((df_categories['FP']/total_num_discrepancies_with_tp)*100,1) \n",
    "df_categories['FP']=[str(value[1])+' ('+str(percentage_fp[index])+'%)' for index,value in enumerate(df_categories['FP'].items())]\n",
    "\n",
    "percentage_fn=np.round((df_categories['FN']/total_num_discrepancies_with_tp)*100,1) \n",
    "df_categories['FN']=[str(value[1])+' ('+str(percentage_fn[index])+'%)' for index,value in enumerate(df_categories['FN'].items())]\n",
    "\n",
    "percentage_fn=np.round((df_categories['TP']/total_num_discrepancies_with_tp)*100,1) \n",
    "df_categories['TP']=[str(value[1])+' ('+str(percentage_fn[index])+'%)' for index,value in enumerate(df_categories['TP'].items())]\n",
    "\n",
    "df_categories['All findings']=[str(val)+' ('+str(np.round(100*val/total_num_discrepancies_with_tp,1))+'%)' for val in df_categories['All findings'].values]\n",
    "\n",
    "#Rename columns\n",
    "df_categories.rename(columns={'FP': 'AI found, reader missed', 'FN': 'AI missed, reader found', 'TP':'Both found'}, inplace=True)\n",
    "\n",
    "df_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1611a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_categories.to_excel('lymph_types_high.xlsx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2bdf1d7",
   "metadata": {},
   "source": [
    "### Similar analysis as the one performed above (low vs high BMI, volume subgroups) but including lymph nodes this time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21d11b71",
   "metadata": {},
   "source": [
    "Low BMI (nodules and lymph nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cf075a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low BMI numbers\n",
      "TP_AI:  109\n",
      "FP_AI:  96\n",
      "FN_AI:  27\n",
      "TP_read:  114\n",
      "FP_read:  28\n",
      "FN_read:  22\n",
      "TP_both:  87\n"
     ]
    }
   ],
   "source": [
    "TP_AI= TP_AI_low=TP_low+FP_nods_low\n",
    "FP_AI=FP_AI_low=FP_nonods_low\n",
    "FN_AI=FN_AI_low=TP_read_only=FN_nods_low #nodules and lymph nodes detected only by the reader\n",
    "\n",
    "TP_read=TP_read_low=TP_low+FN_nods_low\n",
    "FP_read=FP_read_low=FN_nonods_low\n",
    "FN_read=FN_read_low=TP_AI_only=FP_nods_low #nodules and lymph nodes detected only by AI\n",
    "\n",
    "TP_both=TP_low #Common nodules and lymph nodules detected by both AI and reader\n",
    "\n",
    "#Print the above\n",
    "print(\"Low BMI numbers\")\n",
    "print('TP_AI: ',TP_AI)\n",
    "print('FP_AI: ',FP_AI)\n",
    "print('FN_AI: ',FN_AI)\n",
    "print('TP_read: ',TP_read)\n",
    "print('FP_read: ',FP_read)\n",
    "print('FN_read: ',FN_read)\n",
    "print('TP_both: ',TP_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83580731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensitivity (95% CI)</th>\n",
       "      <th>PPV (95% CI)</th>\n",
       "      <th>F1 score (95% CI)</th>\n",
       "      <th>nodules detected</th>\n",
       "      <th>non-nodules detected</th>\n",
       "      <th>nodules missed</th>\n",
       "      <th>All findings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AI, low</th>\n",
       "      <td>0.8 (0.72, 0.86)</td>\n",
       "      <td>0.53 (0.46, 0.6)</td>\n",
       "      <td>0.64 (0.59, 0.69)</td>\n",
       "      <td>109 (47.0%)</td>\n",
       "      <td>96 (41.4%)</td>\n",
       "      <td>27 (11.6%)</td>\n",
       "      <td>232 (100%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reader, low</th>\n",
       "      <td>0.84 (0.76, 0.89)</td>\n",
       "      <td>0.8 (0.73, 0.86)</td>\n",
       "      <td>0.82 (0.77, 0.86)</td>\n",
       "      <td>114 (69.5%)</td>\n",
       "      <td>28 (17.1%)</td>\n",
       "      <td>22 (13.4%)</td>\n",
       "      <td>164 (100%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>223</td>\n",
       "      <td>124</td>\n",
       "      <td>49</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sensitivity (95% CI)      PPV (95% CI)  F1 score (95% CI)  \\\n",
       "AI, low         0.8 (0.72, 0.86)  0.53 (0.46, 0.6)  0.64 (0.59, 0.69)   \n",
       "reader, low    0.84 (0.76, 0.89)  0.8 (0.73, 0.86)  0.82 (0.77, 0.86)   \n",
       "Total                                                                   \n",
       "\n",
       "            nodules detected non-nodules detected nodules missed All findings  \n",
       "AI, low          109 (47.0%)           96 (41.4%)     27 (11.6%)   232 (100%)  \n",
       "reader, low      114 (69.5%)           28 (17.1%)     22 (13.4%)   164 (100%)  \n",
       "Total                    223                  124             49          396  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Table split in two tables: one for low and one for high BMI having also percentages.\n",
    "\n",
    "#Assessing detection performance - For nodules + lymph nodes \n",
    "df_all_new=pd.DataFrame(columns=['sensitivity (95% CI)','PPV (95% CI)','F1 score (95% CI)','nodules detected','non-nodules detected','nodules missed'],\n",
    "                        index=['AI, low', 'reader, low'])\n",
    "\n",
    "#AI nodules only emph\n",
    "df_all_new.iloc[0,0]=np.round(sensitivity(TP_AI,FN_AI),2)\n",
    "df_all_new.iloc[0,1]=np.round(PPV(TP_AI,FP_AI),2)\n",
    "df_all_new.iloc[0,2]=np.round(F1score(TP_AI,FP_AI,FN_AI),2)\n",
    "df_all_new.iloc[0,3]=TP_AI\n",
    "df_all_new.iloc[0,4]=FP_AI\n",
    "df_all_new.iloc[0,5]=FN_AI\n",
    "\n",
    "sensitivity_confidence_interval_AI, PPV_confidence_interval_AI, F1_confidence_interval_AI \\\n",
    "= sensitivity_and_specificity_with_confidence_intervals(TP_AI, FP_AI, FN_AI, 0, alpha=0.95)\n",
    "\n",
    "ci_sens_ai=[np.round(x,2) for x in sensitivity_confidence_interval_AI]\n",
    "ci_ppv_ai=[np.round(x,2) for x in PPV_confidence_interval_AI]\n",
    "ci_f1_ai=[np.round(x,2) for x in F1_confidence_interval_AI]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[0]=str(df_all_new['sensitivity (95% CI)'].iloc[0])+' '+str(tuple(ci_sens_ai))\n",
    "df_all_new['PPV (95% CI)'].iloc[0]=str(df_all_new['PPV (95% CI)'].iloc[0])+' '+str(tuple(ci_ppv_ai))\n",
    "df_all_new['F1 score (95% CI)'].iloc[0]=str(df_all_new['F1 score (95% CI)'].iloc[0])+' '+str(tuple(ci_f1_ai))\n",
    "\n",
    "#reader nodules\n",
    "df_all_new.iloc[1,0]=np.round(sensitivity(TP_read,FN_read),2)\n",
    "df_all_new.iloc[1,1]=np.round(PPV(TP_read,FP_read),2)\n",
    "df_all_new.iloc[1,2]=np.round(F1score(TP_read,FP_read,FN_read),2)\n",
    "df_all_new.iloc[1,3]=TP_read\n",
    "df_all_new.iloc[1,4]=FP_read\n",
    "df_all_new.iloc[1,5]=FN_read\n",
    "\n",
    "sensitivity_confidence_interval_read, PPV_confidence_interval_read, F1_confidence_interval_read\\\n",
    "    = sensitivity_and_specificity_with_confidence_intervals(TP_read, FP_read, FN_read, 0, alpha=0.95)\n",
    "\n",
    "ci_sens_read=[np.round(x,2) for x in sensitivity_confidence_interval_read]\n",
    "ci_ppv_read=[np.round(x,2) for x in PPV_confidence_interval_read]\n",
    "ci_f1_read=[np.round(x,2) for x in F1_confidence_interval_read]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[1]=str(df_all_new['sensitivity (95% CI)'].iloc[1])+' '+str(tuple(ci_sens_read))\n",
    "df_all_new['PPV (95% CI)'].iloc[1]=str(df_all_new['PPV (95% CI)'].iloc[1])+' '+str(tuple(ci_ppv_read))\n",
    "df_all_new['F1 score (95% CI)'].iloc[1]=str(df_all_new['F1 score (95% CI)'].iloc[1])+' '+str(tuple(ci_f1_read))\n",
    "\n",
    "\n",
    "df_all_new['All findings']=df_all_new['nodules detected']+df_all_new['non-nodules detected']+df_all_new['nodules missed']\n",
    "df_all_new.loc['Total']= df_all_new.sum()\n",
    "df_all_new.loc['Total'].iloc[0:3]=''\n",
    "\n",
    "\n",
    "all_findings=df_all_new.iloc[:-1,3:-1].sum().sum()\n",
    "\n",
    "for i in range(2):\n",
    "    row_all=np.sum(df_all_new.iloc[i][3:6].values)\n",
    "\n",
    "    percentage_tp=np.round((df_all_new.iloc[i][3]/row_all)*100,1) \n",
    "    df_all_new['nodules detected'].iloc[i]=str(df_all_new.iloc[i][3])+' ('+str(percentage_tp)+'%)'\n",
    "    percentage_fp=np.round((df_all_new.iloc[i][4]/row_all)*100,1) \n",
    "    df_all_new['non-nodules detected'].iloc[i]=str(df_all_new.iloc[i][4])+' ('+str(percentage_fp)+'%)'\n",
    "    percentage_fn=np.round((df_all_new.iloc[i][5]/row_all)*100,1) \n",
    "    df_all_new['nodules missed'].iloc[i]=str(df_all_new.iloc[i][5])+' ('+str(percentage_fn)+'%)'\n",
    "\n",
    "    df_all_new['All findings'].iloc[i]=str(df_all_new.iloc[i][6])+' (100%)'\n",
    "\n",
    "df_all_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2695f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_new.to_excel('nodules_lymph_low.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873d5fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "McNemar's test (nodules and lymphs), AI_vs_Reader with continuity correction (not exact) p value is 0.5677091661973526\n"
     ]
    }
   ],
   "source": [
    "#McNemar's test to compare Reader vs AI (using consensus panel)\n",
    "#Below format is: [[Both AI found and reader found, reader missed and AI found], [Reader found and AI missed, 0]]\n",
    "\n",
    "#For nodules\n",
    "data=[[TP_both, FN_read],\n",
    "        [FN_AI,0]]\n",
    "# print(data)\n",
    "\n",
    "# McNemar's Test without continuity correction\n",
    "print(\"McNemar's test (nodules and lymphs), AI_vs_Reader with continuity correction (not exact) p value is\",mcnemar(data, exact=False,correction=True).pvalue) \n",
    "\n",
    "\n",
    "# #For FPs\n",
    "# data=[[0, FP_AI], \n",
    "#         [FP_read, 0]]\n",
    "# # print(data)\n",
    "\n",
    "# # McNemar's Test without continuity correction\n",
    "# print(\"For FP findings, with continuity correction (not exact) p value is\",mcnemar(data, exact=False,correction=True).pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c12a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #AI vs GT, TP_both included\n",
    "# # Table looks like below:\n",
    "# #              GT\n",
    "# #             Yes                       No\n",
    "# # AI   Yes    TP_both+TP_AI_only      FP_AI\n",
    "# #      No     FN_AI                   0\n",
    "\n",
    "# list_of_lists=[[1 for x in range(TP_both)],[1 for x in range(TP_AI_only)],[0 for x in range(FP_AI) ],[1 for x in range(FN_AI) ] ] \n",
    "# rater_GT=[item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# list_of_lists=[[1 for x in range(TP_both)], [1 for x in range(TP_AI_only)],[1 for x in range(FP_AI) ],[0 for x in range(FN_AI) ]]\n",
    "# rater_AI=[item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# print(\"AI vs consensus (for nodules and lymphs), kappa is \",cohen_kappa_score(rater_GT, rater_AI))\n",
    "\n",
    "# #Reader vs GT, TP_both included\n",
    "# # Table looks like below:\n",
    "# #                    GT\n",
    "# #                   Yes                       No\n",
    "# # Reader   Yes    TP_both+TP_read_only      FP_read\n",
    "# #           No     FN_read                   0\n",
    "\n",
    "# list_of_lists=[[1 for x in range(TP_both)],[1 for x in range(TP_read_only)],[0 for x in range(FP_read) ],[1 for x in range(FN_read) ] ] \n",
    "# rater_GT=[item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# list_of_lists=[[1 for x in range(TP_both)],[1 for x in range(TP_read_only)],[1 for x in range(FP_read) ],[0 for x in range(FN_read) ]]\n",
    "# rater_read=[item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# print(\"Reader vs consensus (for non-nodules only), kappa is \",cohen_kappa_score(rater_GT, rater_read))\n",
    "\n",
    "\n",
    "\n",
    "# #Due to small numbers of TP, FP and FN, we cannot calculate the kappa for consensus vs AI (or reader). Better for AI vs reader.\n",
    "\n",
    "# #Reader vs AI, TP_both included\n",
    "\n",
    "# # Table looks like below (for nodule and lymph nodes (not FP)):\n",
    "# #                    AI\n",
    "# #                   Yes                       No\n",
    "# # Reader   Yes    TP_both                  FN_AI\n",
    "# #           No     FN_read                   0\n",
    "\n",
    "# list_of_lists=[[1 for x in range(TP_both)],[1 for x in range(FN_read) ],[0 for x in range(FN_AI)]] \n",
    "# rater_AI=[item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# list_of_lists=[[1 for x in range(TP_both)],[0 for x in range(FN_read) ],[1 for x in range(FN_AI)]]\n",
    "# rater_read=[item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# print(\"Reader vs AI (for nodules/lymph nodes only), kappa is \",cohen_kappa_score(rater_AI, rater_read))\n",
    "\n",
    "\n",
    "# # Table looks like below (for FP):\n",
    "# #                    AI\n",
    "# #                   Yes                       No\n",
    "# # Reader   Yes        0                     FP_read\n",
    "# #           No     FP_AI                       0\n",
    "\n",
    "# list_of_lists=[[0 for x in range(FP_read) ],[1 for x in range(FP_AI)]] \n",
    "# rater_AI=[item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# list_of_lists=[[1 for x in range(FP_read) ],[0 for x in range(FP_AI)]]\n",
    "# rater_read=[item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# print(\"Reader vs AI (for non-nodules only), kappa is \",cohen_kappa_score(rater_AI, rater_read))\n",
    "\n",
    "# #For both nodules/lymphs and non-nodules:\n",
    "# list_of_lists=[[0 for x in range(FP_read) ],[1 for x in range(FP_AI)],[1 for x in range(TP_both)],[1 for x in range(FN_read) ],[0 for x in range(FN_AI)]] \n",
    "# rater_AI=[item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# list_of_lists=[[1 for x in range(FP_read) ],[0 for x in range(FP_AI)],[1 for x in range(TP_both)],[0 for x in range(FN_read) ],[1 for x in range(FN_AI)]]\n",
    "# rater_read=[item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# print(\"Reader vs AI (for both nods/lymphs and non-nodules), kappa is \",cohen_kappa_score(rater_AI, rater_read))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "040e1a9d",
   "metadata": {},
   "source": [
    "High BMI (nodules and lymph nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6286202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High BMI numbers\n",
      "TP_AI:  98\n",
      "FP_AI:  53\n",
      "FN_AI:  33\n",
      "TP_read:  100\n",
      "FP_read:  9\n",
      "FN_read:  31\n",
      "TP_both:  67\n"
     ]
    }
   ],
   "source": [
    "TP_AI=TP_AI_high=TP_high+FP_nods_high\n",
    "FP_AI=FP_AI_high=FP_nonods_high\n",
    "FN_AI=FN_AI_high=TP_read_only=FN_nods_high #nodules and lymph nodes detected only by the reader\n",
    "\n",
    "TP_read=TP_read_high=TP_high+FN_nods_high\n",
    "FP_read=FP_read_high=FN_nonods_high\n",
    "FN_read=FN_read_high=TP_AI_only=FP_nods_high #nodules and lymph nodes detected only by AI\n",
    "\n",
    "TP_both=TP_high #Common nodules and lymph nodules detected by both AI and reader\n",
    "\n",
    "#Print the above\n",
    "print(\"High BMI numbers\")\n",
    "print('TP_AI: ',TP_AI)\n",
    "print('FP_AI: ',FP_AI)\n",
    "print('FN_AI: ',FN_AI)\n",
    "print('TP_read: ',TP_read)\n",
    "print('FP_read: ',FP_read)\n",
    "print('FN_read: ',FN_read)\n",
    "print('TP_both: ',TP_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79345ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensitivity (95% CI)</th>\n",
       "      <th>PPV (95% CI)</th>\n",
       "      <th>F1 score (95% CI)</th>\n",
       "      <th>nodules detected</th>\n",
       "      <th>non-nodules detected</th>\n",
       "      <th>nodules missed</th>\n",
       "      <th>All findings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AI, high</th>\n",
       "      <td>0.75 (0.66, 0.82)</td>\n",
       "      <td>0.65 (0.57, 0.72)</td>\n",
       "      <td>0.7 (0.64, 0.75)</td>\n",
       "      <td>98 (53.3%)</td>\n",
       "      <td>53 (28.8%)</td>\n",
       "      <td>33 (17.9%)</td>\n",
       "      <td>184 (100%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reader, high</th>\n",
       "      <td>0.76 (0.68, 0.83)</td>\n",
       "      <td>0.92 (0.84, 0.96)</td>\n",
       "      <td>0.83 (0.78, 0.88)</td>\n",
       "      <td>100 (71.4%)</td>\n",
       "      <td>9 (6.4%)</td>\n",
       "      <td>31 (22.1%)</td>\n",
       "      <td>140 (100%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>198</td>\n",
       "      <td>62</td>\n",
       "      <td>64</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             sensitivity (95% CI)       PPV (95% CI)  F1 score (95% CI)  \\\n",
       "AI, high        0.75 (0.66, 0.82)  0.65 (0.57, 0.72)   0.7 (0.64, 0.75)   \n",
       "reader, high    0.76 (0.68, 0.83)  0.92 (0.84, 0.96)  0.83 (0.78, 0.88)   \n",
       "Total                                                                     \n",
       "\n",
       "             nodules detected non-nodules detected nodules missed All findings  \n",
       "AI, high           98 (53.3%)           53 (28.8%)     33 (17.9%)   184 (100%)  \n",
       "reader, high      100 (71.4%)             9 (6.4%)     31 (22.1%)   140 (100%)  \n",
       "Total                     198                   62             64          324  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Assessing detection performance - For nodules and lymph nodes \n",
    "df_all_new=pd.DataFrame(columns=['sensitivity (95% CI)','PPV (95% CI)','F1 score (95% CI)','nodules detected','non-nodules detected','nodules missed'],\n",
    "                        index=['AI, high', 'reader, high' ])\n",
    "\n",
    "\n",
    "#AI nodules + lymph nodes\n",
    "df_all_new.iloc[0,0]=np.round(sensitivity(TP_AI,FN_AI),2)\n",
    "df_all_new.iloc[0,1]=np.round(PPV(TP_AI,FP_AI),2)\n",
    "df_all_new.iloc[0,2]=np.round(F1score(TP_AI,FP_AI,FN_AI),2)\n",
    "df_all_new.iloc[0,3]=TP_AI\n",
    "df_all_new.iloc[0,4]=FP_AI\n",
    "df_all_new.iloc[0,5]=FN_AI\n",
    "\n",
    "sensitivity_confidence_interval_AI, PPV_confidence_interval_AI, F1_confidence_interval_AI \\\n",
    "= sensitivity_and_specificity_with_confidence_intervals(TP_AI, FP_AI, FN_AI, 0, alpha=0.95)\n",
    "\n",
    "ci_sens_ai=[np.round(x,2) for x in sensitivity_confidence_interval_AI]\n",
    "ci_ppv_ai=[np.round(x,2) for x in PPV_confidence_interval_AI]\n",
    "ci_f1_ai=[np.round(x,2) for x in F1_confidence_interval_AI]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[0]=str(df_all_new['sensitivity (95% CI)'].iloc[0])+' '+str(tuple(ci_sens_ai))\n",
    "df_all_new['PPV (95% CI)'].iloc[0]=str(df_all_new['PPV (95% CI)'].iloc[0])+' '+str(tuple(ci_ppv_ai))\n",
    "df_all_new['F1 score (95% CI)'].iloc[0]=str(df_all_new['F1 score (95% CI)'].iloc[0])+' '+str(tuple(ci_f1_ai))\n",
    "\n",
    "#reader nodules + lymph nodes\n",
    "df_all_new.iloc[1,0]=np.round(sensitivity(TP_read,FN_read),2)\n",
    "df_all_new.iloc[1,1]=np.round(PPV(TP_read,FP_read),2)\n",
    "df_all_new.iloc[1,2]=np.round(F1score(TP_read,FP_read,FN_read),2)\n",
    "df_all_new.iloc[1,3]=TP_read\n",
    "df_all_new.iloc[1,4]=FP_read\n",
    "df_all_new.iloc[1,5]=FN_read\n",
    "\n",
    "sensitivity_confidence_interval_read, PPV_confidence_interval_read, F1_confidence_interval_read\\\n",
    "    = sensitivity_and_specificity_with_confidence_intervals(TP_read, FP_read, FN_read, 0, alpha=0.95)\n",
    "\n",
    "ci_sens_read=[np.round(x,2) for x in sensitivity_confidence_interval_read]\n",
    "ci_ppv_read=[np.round(x,2) for x in PPV_confidence_interval_read]\n",
    "ci_f1_read=[np.round(x,2) for x in F1_confidence_interval_read]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[1]=str(df_all_new['sensitivity (95% CI)'].iloc[1])+' '+str(tuple(ci_sens_read))\n",
    "df_all_new['PPV (95% CI)'].iloc[1]=str(df_all_new['PPV (95% CI)'].iloc[1])+' '+str(tuple(ci_ppv_read))\n",
    "df_all_new['F1 score (95% CI)'].iloc[1]=str(df_all_new['F1 score (95% CI)'].iloc[1])+' '+str(tuple(ci_f1_read))\n",
    "\n",
    "df_all_new['All findings']=df_all_new['nodules detected']+df_all_new['non-nodules detected']+df_all_new['nodules missed']\n",
    "df_all_new.loc['Total']= df_all_new.sum()\n",
    "df_all_new.loc['Total'].iloc[0:3]=''\n",
    "\n",
    "all_findings=df_all_new.iloc[:-1,3:-1].sum().sum()\n",
    "\n",
    "for i in range(2):\n",
    "    row_all=np.sum(df_all_new.iloc[i][3:6].values)\n",
    "    percentage_fp=np.round((df_all_new.iloc[i][4]/row_all)*100,1) \n",
    "    df_all_new['non-nodules detected'].iloc[i]=str(df_all_new.iloc[i][4])+' ('+str(percentage_fp)+'%)'\n",
    "    percentage_tp=np.round((df_all_new.iloc[i][3]/row_all)*100,1) \n",
    "    df_all_new['nodules detected'].iloc[i]=str(df_all_new.iloc[i][3])+' ('+str(percentage_tp)+'%)'\n",
    "    percentage_fn=np.round((df_all_new.iloc[i][5]/row_all)*100,1) \n",
    "    df_all_new['nodules missed'].iloc[i]=str(df_all_new.iloc[i][5])+' ('+str(percentage_fn)+'%)'\n",
    "\n",
    "    df_all_new['All findings'].iloc[i]=str(df_all_new.iloc[i][6])+' (100%)'\n",
    "\n",
    "df_all_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7ed651",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_new.to_excel('nodules_lymph_high.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8df419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "McNemar's test (nodules and lymphs), AI_vs_Reader with continuity correction (not exact) p value is 0.9005235503397742\n"
     ]
    }
   ],
   "source": [
    "#McNemar's test to compare Reader vs AI (using consensus panel)\n",
    "#Below format is: [[Both AI found and reader found, reader missed and AI found], [Reader found and AI missed, 0]]\n",
    "\n",
    "#For nodules\n",
    "data=[[TP_both, FN_read],\n",
    "        [FN_AI,0]]\n",
    "# print(data)\n",
    "\n",
    "# McNemar's Test without continuity correction\n",
    "print(\"McNemar's test (nodules and lymphs), AI_vs_Reader with continuity correction (not exact) p value is\",mcnemar(data, exact=False,correction=True).pvalue) \n",
    "\n",
    "\n",
    "# #For FPs\n",
    "# data=[[0, FP_AI], \n",
    "#         [FP_read, 0]]\n",
    "# # print(data)\n",
    "\n",
    "# # McNemar's Test without continuity correction\n",
    "# print(\"For FP findings, with continuity correction (not exact) p value is\",mcnemar(data, exact=False,correction=True).pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28742b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #AI vs GT, TP_both included\n",
    "# # Table looks like below:\n",
    "# #              GT\n",
    "# #             Yes                       No\n",
    "# # AI   Yes    TP_both+TP_AI_only      FP_AI\n",
    "# #      No     FN_AI                   0\n",
    "\n",
    "# list_of_lists=[[1 for x in range(TP_both)],[1 for x in range(TP_AI_only)],[0 for x in range(FP_AI) ],[1 for x in range(FN_AI) ] ] \n",
    "# rater_GT=[item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# list_of_lists=[[1 for x in range(TP_both)], [1 for x in range(TP_AI_only)],[1 for x in range(FP_AI) ],[0 for x in range(FN_AI) ]]\n",
    "# rater_AI=[item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# print(\"AI vs consensus (for nodules and lymphs), kappa is \",cohen_kappa_score(rater_GT, rater_AI))\n",
    "\n",
    "# #Reader vs GT, TP_both included\n",
    "# # Table looks like below:\n",
    "# #                    GT\n",
    "# #                   Yes                       No\n",
    "# # Reader   Yes    TP_both+TP_read_only      FP_read\n",
    "# #           No     FN_read                   0\n",
    "\n",
    "# list_of_lists=[[1 for x in range(TP_both)],[1 for x in range(TP_read_only)],[0 for x in range(FP_read) ],[1 for x in range(FN_read) ] ] \n",
    "# rater_GT=[item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# list_of_lists=[[1 for x in range(TP_both)],[1 for x in range(TP_read_only)],[1 for x in range(FP_read) ],[0 for x in range(FN_read) ]]\n",
    "# rater_read=[item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# print(\"Reader vs consensus (for non-nodules only), kappa is \",cohen_kappa_score(rater_GT, rater_read))\n",
    "\n",
    "\n",
    "# #Due to small numbers of TP, FP and FN, we cannot calculate the kappa for consensus vs AI (or reader). Better for AI vs reader.\n",
    "\n",
    "# #Reader vs AI, TP_both included\n",
    "\n",
    "# # Table looks like below (for nodule and lymph nodes (not FP)):\n",
    "# #                    AI\n",
    "# #                   Yes                       No\n",
    "# # Reader   Yes    TP_both                  FN_AI\n",
    "# #           No     FN_read                   0\n",
    "\n",
    "# list_of_lists=[[1 for x in range(TP_both)],[1 for x in range(FN_read) ],[0 for x in range(FN_AI)]] \n",
    "# rater_AI=[item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# list_of_lists=[[1 for x in range(TP_both)],[0 for x in range(FN_read) ],[1 for x in range(FN_AI)]]\n",
    "# rater_read=[item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# print(\"Reader vs AI (for nodules/lymph nodes only), kappa is \",cohen_kappa_score(rater_AI, rater_read))\n",
    "\n",
    "\n",
    "# # Table looks like below (for FP):\n",
    "# #                    AI\n",
    "# #                   Yes                       No\n",
    "# # Reader   Yes        0                     FP_read\n",
    "# #           No     FP_AI                       0\n",
    "\n",
    "# list_of_lists=[[0 for x in range(FP_read) ],[1 for x in range(FP_AI)]] \n",
    "# rater_AI=[item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# list_of_lists=[[1 for x in range(FP_read) ],[0 for x in range(FP_AI)]]\n",
    "# rater_read=[item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# print(\"Reader vs AI (for non-nodules only), kappa is \",cohen_kappa_score(rater_AI, rater_read))\n",
    "\n",
    "\n",
    "# #For both nodules/lymphs and non-nodules:\n",
    "# list_of_lists=[[0 for x in range(FP_read) ],[1 for x in range(FP_AI)],[1 for x in range(TP_both)],[1 for x in range(FN_read) ],[0 for x in range(FN_AI)]] \n",
    "# rater_AI=[item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# list_of_lists=[[1 for x in range(FP_read) ],[0 for x in range(FP_AI)],[1 for x in range(TP_both)],[0 for x in range(FN_read) ],[1 for x in range(FN_AI)]]\n",
    "# rater_read=[item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# print(\"Reader vs AI (for both nods/lymphs and non-nodules), kappa is \",cohen_kappa_score(rater_AI, rater_read))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b83897a",
   "metadata": {},
   "source": [
    "Note: We should not perform comparisons between emphysema/non-emphysema groups using McNemar's test - It should only be used in the same group of participants - better to use Mann-Whitney!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5da490f6",
   "metadata": {},
   "source": [
    "#### Same analysis for volume subgroups (nodules and lymph nodes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28dd1b06",
   "metadata": {},
   "source": [
    "Low BMI volume subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38a146f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low BMI numbers\n",
      "TP_AI_100:  81\n",
      "FP_AI_100:  26\n",
      "FN_AI_100:  26\n",
      "TP_read_100:  92\n",
      "FP_read_100:  22\n",
      "FN_read_100:  15\n",
      "TP_AI_100_300:  21\n",
      "FP_AI_100_300:  43\n",
      "FN_AI_100_300:  1\n",
      "TP_read_100_300:  17\n",
      "FP_read_100_300:  5\n",
      "FN_read_100_300:  5\n",
      "TP_AI_300:  7\n",
      "FP_AI_300:  27\n",
      "FN_AI_300:  0\n",
      "TP_read_300:  5\n",
      "FP_read_300:  1\n",
      "FN_read_300:  2\n",
      "TP_both_100:  66\n",
      "TP_both_100_300:  16\n",
      "TP_both_300:  5\n"
     ]
    }
   ],
   "source": [
    "TP_AI_100=TP_low_30_100+ai_only_nods_low_30_100+ai_lymph_low_30_100 \n",
    "FP_AI_100=ai_nonods_low_30_100\n",
    "FN_AI_100=reader_nods_low_30_100 #nodules of reader including lymph nodes\n",
    "\n",
    "TP_read_100=TP_low_30_100+reader_nods_low_30_100\n",
    "FP_read_100=reader_nonods_low_30_100\n",
    "FN_read_100=ai_only_nods_low_30_100+ai_lymph_low_30_100 #nodules of AI including lymph nodes\n",
    "\n",
    "TP_AI_100_300=TP_low_100_300+ai_only_nods_low_100_300+ai_lymph_low_100_300\n",
    "FP_AI_100_300=ai_nonods_low_100_300\n",
    "FN_AI_100_300=reader_nods_low_100_300 #nodules of reader excluding lymph nodes\n",
    "\n",
    "TP_read_100_300=TP_low_100_300+reader_nods_low_100_300\n",
    "FP_read_100_300=reader_nonods_low_100_300\n",
    "FN_read_100_300=ai_only_nods_low_100_300+ai_lymph_low_100_300 #nodules of AI excluding lymph nodes\n",
    "\n",
    "TP_AI_300=TP_low_300+ai_only_nods_low_300+ai_lymph_low_300\n",
    "FP_AI_300=ai_nonods_low_300\n",
    "FN_AI_300=reader_nods_low_300 #nodules of reader excluding lymph nodes\n",
    "\n",
    "TP_read_300=TP_low_300+reader_nods_low_300\n",
    "FP_read_300=reader_nonods_low_300\n",
    "FN_read_300=ai_only_nods_low_300+ai_lymph_low_300 #nodules of AI excluding lymph nodes\n",
    "\n",
    "TP_both_100=TP_low_30_100\n",
    "TP_both_100_300=TP_low_100_300\n",
    "TP_both_300=TP_low_300\n",
    "\n",
    "#Print the above\n",
    "print(\"Low BMI numbers\")\n",
    "print('TP_AI_100: ',TP_AI_100)\n",
    "print('FP_AI_100: ',FP_AI_100)\n",
    "print('FN_AI_100: ',FN_AI_100)\n",
    "print('TP_read_100: ',TP_read_100)\n",
    "print('FP_read_100: ',FP_read_100)\n",
    "print('FN_read_100: ',FN_read_100)\n",
    "print('TP_AI_100_300: ',TP_AI_100_300)\n",
    "print('FP_AI_100_300: ',FP_AI_100_300)\n",
    "print('FN_AI_100_300: ',FN_AI_100_300)\n",
    "print('TP_read_100_300: ',TP_read_100_300)\n",
    "print('FP_read_100_300: ',FP_read_100_300)\n",
    "print('FN_read_100_300: ',FN_read_100_300)\n",
    "print('TP_AI_300: ',TP_AI_300)\n",
    "print('FP_AI_300: ',FP_AI_300)\n",
    "print('FN_AI_300: ',FN_AI_300)\n",
    "print('TP_read_300: ',TP_read_300)\n",
    "print('FP_read_300: ',FP_read_300)\n",
    "print('FN_read_300: ',FN_read_300)\n",
    "print('TP_both_100: ',TP_both_100)\n",
    "print('TP_both_100_300: ',TP_both_100_300)\n",
    "print('TP_both_300: ',TP_both_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "108879e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Table with Reader, AI and consensus findings for nodules and lymph nodes for low BMI\n",
    "df_all_new=pd.DataFrame(columns=['Reader','AI','Consensus'],\n",
    "                        index=['Non-nodules','TP 30-100mm3','TP 100-300mm3', 'TP >300mm3'])\n",
    "\n",
    "df_all_new.index.name = 'Low BMI cases'\n",
    "\n",
    "df_all_new['Reader']=[FP_read_100+FP_read_100_300+FP_read_300,TP_both_100+FN_AI_100,TP_both_100_300+FN_AI_100_300, TP_both_300+FN_AI_300]\n",
    "df_all_new['AI']=[FP_AI_100+FP_AI_100_300+FP_AI_300,TP_both_100+FN_read_100,TP_both_100_300+FN_read_100_300,TP_both_300+FN_read_300]\n",
    "df_all_new['Consensus']=[FP_AI_100+FP_AI_100_300+FP_AI_300+FP_read_100+FP_read_100_300+FP_read_300,FN_AI_100+FN_read_100, FN_AI_100_300+FN_read_100_300, FN_AI_300+FN_read_300]\n",
    "\n",
    "df_all_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5584ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_new.to_excel('non_nodules_and_TP_low.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b58fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensitivity (95% CI)</th>\n",
       "      <th>PPV (95% CI)</th>\n",
       "      <th>F1 score (95% CI)</th>\n",
       "      <th>nodules correctly detected</th>\n",
       "      <th>non-nodules incorrectly detected</th>\n",
       "      <th>nodules missed</th>\n",
       "      <th>All findings</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GT by radiologists for discrepancies</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AI, low 30-100mm3</th>\n",
       "      <td>0.76 (0.66, 0.83)</td>\n",
       "      <td>0.76 (0.66, 0.83)</td>\n",
       "      <td>0.76 (0.69, 0.81)</td>\n",
       "      <td>81 (34.9%)</td>\n",
       "      <td>26 (11.2%)</td>\n",
       "      <td>26 (11.2%)</td>\n",
       "      <td>133 (57.3%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AI, low 100-300mm3</th>\n",
       "      <td>0.95 (0.75, 1.0)</td>\n",
       "      <td>0.33 (0.22, 0.46)</td>\n",
       "      <td>0.49 (0.38, 0.6)</td>\n",
       "      <td>21 (9.1%)</td>\n",
       "      <td>43 (18.5%)</td>\n",
       "      <td>1 (0.4%)</td>\n",
       "      <td>65 (28.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AI, low 300+mm3</th>\n",
       "      <td>1.0 (0.56, 0.99)</td>\n",
       "      <td>0.21 (0.09, 0.38)</td>\n",
       "      <td>0.34 (0.21, 0.51)</td>\n",
       "      <td>7 (3.0%)</td>\n",
       "      <td>27 (11.6%)</td>\n",
       "      <td>0 (0.0%)</td>\n",
       "      <td>34 (14.7%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>232 (100%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reader, low 30-100mm3</th>\n",
       "      <td>0.86 (0.78, 0.92)</td>\n",
       "      <td>0.81 (0.72, 0.87)</td>\n",
       "      <td>0.83 (0.78, 0.88)</td>\n",
       "      <td>92 (56.1%)</td>\n",
       "      <td>22 (13.4%)</td>\n",
       "      <td>15 (9.1%)</td>\n",
       "      <td>129 (78.7%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reader, low 100-300mm3</th>\n",
       "      <td>0.77 (0.54, 0.91)</td>\n",
       "      <td>0.77 (0.54, 0.91)</td>\n",
       "      <td>0.77 (0.62, 0.88)</td>\n",
       "      <td>17 (10.4%)</td>\n",
       "      <td>5 (3.0%)</td>\n",
       "      <td>5 (3.0%)</td>\n",
       "      <td>27 (16.5%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reader, low 300+mm3</th>\n",
       "      <td>0.71 (0.3, 0.95)</td>\n",
       "      <td>0.83 (0.36, 0.99)</td>\n",
       "      <td>0.77 (0.46, 0.94)</td>\n",
       "      <td>5 (3.0%)</td>\n",
       "      <td>1 (0.6%)</td>\n",
       "      <td>2 (1.2%)</td>\n",
       "      <td>8 (4.9%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>164 (100%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     sensitivity (95% CI)       PPV (95% CI)  \\\n",
       "GT by radiologists for discrepancies                                           \n",
       "AI, low 30-100mm3                       0.76 (0.66, 0.83)  0.76 (0.66, 0.83)   \n",
       "AI, low 100-300mm3                       0.95 (0.75, 1.0)  0.33 (0.22, 0.46)   \n",
       "AI, low 300+mm3                          1.0 (0.56, 0.99)  0.21 (0.09, 0.38)   \n",
       "                                                                               \n",
       "reader, low 30-100mm3                   0.86 (0.78, 0.92)  0.81 (0.72, 0.87)   \n",
       "reader, low 100-300mm3                  0.77 (0.54, 0.91)  0.77 (0.54, 0.91)   \n",
       "reader, low 300+mm3                      0.71 (0.3, 0.95)  0.83 (0.36, 0.99)   \n",
       "                                                                               \n",
       "\n",
       "                                      F1 score (95% CI)  \\\n",
       "GT by radiologists for discrepancies                      \n",
       "AI, low 30-100mm3                     0.76 (0.69, 0.81)   \n",
       "AI, low 100-300mm3                     0.49 (0.38, 0.6)   \n",
       "AI, low 300+mm3                       0.34 (0.21, 0.51)   \n",
       "                                                          \n",
       "reader, low 30-100mm3                 0.83 (0.78, 0.88)   \n",
       "reader, low 100-300mm3                0.77 (0.62, 0.88)   \n",
       "reader, low 300+mm3                   0.77 (0.46, 0.94)   \n",
       "                                                          \n",
       "\n",
       "                                     nodules correctly detected  \\\n",
       "GT by radiologists for discrepancies                              \n",
       "AI, low 30-100mm3                                    81 (34.9%)   \n",
       "AI, low 100-300mm3                                    21 (9.1%)   \n",
       "AI, low 300+mm3                                        7 (3.0%)   \n",
       "                                                                  \n",
       "reader, low 30-100mm3                                92 (56.1%)   \n",
       "reader, low 100-300mm3                               17 (10.4%)   \n",
       "reader, low 300+mm3                                    5 (3.0%)   \n",
       "                                                                  \n",
       "\n",
       "                                     non-nodules incorrectly detected  \\\n",
       "GT by radiologists for discrepancies                                    \n",
       "AI, low 30-100mm3                                          26 (11.2%)   \n",
       "AI, low 100-300mm3                                         43 (18.5%)   \n",
       "AI, low 300+mm3                                            27 (11.6%)   \n",
       "                                                                        \n",
       "reader, low 30-100mm3                                      22 (13.4%)   \n",
       "reader, low 100-300mm3                                       5 (3.0%)   \n",
       "reader, low 300+mm3                                          1 (0.6%)   \n",
       "                                                                        \n",
       "\n",
       "                                     nodules missed All findings  \n",
       "GT by radiologists for discrepancies                              \n",
       "AI, low 30-100mm3                        26 (11.2%)  133 (57.3%)  \n",
       "AI, low 100-300mm3                         1 (0.4%)   65 (28.0%)  \n",
       "AI, low 300+mm3                            0 (0.0%)   34 (14.7%)  \n",
       "                                                      232 (100%)  \n",
       "reader, low 30-100mm3                     15 (9.1%)  129 (78.7%)  \n",
       "reader, low 100-300mm3                     5 (3.0%)   27 (16.5%)  \n",
       "reader, low 300+mm3                        2 (1.2%)     8 (4.9%)  \n",
       "                                                      164 (100%)  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Comparison between reader and AI for volume subgroups\n",
    "df_all_new=pd.DataFrame(columns=['sensitivity (95% CI)','PPV (95% CI)','F1 score (95% CI)','nodules correctly detected','non-nodules incorrectly detected','nodules missed'], \n",
    "                        index=['AI, low 30-100mm3', 'AI, low 100-300mm3','AI, low 300+mm3','',\n",
    "                               'reader, low 30-100mm3','reader, low 100-300mm3', 'reader, low 300+mm3',''\n",
    "                              ])\n",
    "\n",
    "df_all_new.index.name = 'GT by radiologists for discrepancies' \n",
    "\n",
    "df_all_new.iloc[0,0]=np.round(sensitivity(TP_AI_100,FN_AI_100),2)\n",
    "df_all_new.iloc[0,1]=np.round(PPV(TP_AI_100,FP_AI_100),2)\n",
    "df_all_new.iloc[0,2]=np.round(F1score(TP_AI_100,FP_AI_100,FN_AI_100),2)\n",
    "df_all_new.iloc[0,3]=TP_AI_100\n",
    "df_all_new.iloc[0,4]=FP_AI_100\n",
    "df_all_new.iloc[0,5]=FN_AI_100\n",
    "\n",
    "sensitivity_confidence_interval_AI, PPV_confidence_interval_AI, F1_confidence_interval_AI \\\n",
    "= sensitivity_and_specificity_with_confidence_intervals(TP_AI_100, FP_AI_100, FN_AI_100, 0, alpha=0.95)\n",
    "\n",
    "ci_sens_ai=[np.round(x,2) for x in sensitivity_confidence_interval_AI]\n",
    "ci_ppv_ai=[np.round(x,2) for x in PPV_confidence_interval_AI]\n",
    "ci_f1_ai=[np.round(x,2) for x in F1_confidence_interval_AI]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[0]=str(df_all_new['sensitivity (95% CI)'].iloc[0])+' '+str(tuple(ci_sens_ai))\n",
    "df_all_new['PPV (95% CI)'].iloc[0]=str(df_all_new['PPV (95% CI)'].iloc[0])+' '+str(tuple(ci_ppv_ai))\n",
    "df_all_new['F1 score (95% CI)'].iloc[0]=str(df_all_new['F1 score (95% CI)'].iloc[0])+' '+str(tuple(ci_f1_ai))\n",
    "\n",
    "\n",
    "df_all_new.iloc[4,0]=np.round(sensitivity(TP_read_100,FN_read_100),2)\n",
    "df_all_new.iloc[4,1]=np.round(PPV(TP_read_100,FP_read_100),2)\n",
    "df_all_new.iloc[4,2]=np.round(F1score(TP_read_100,FP_read_100,FN_read_100),2)\n",
    "df_all_new.iloc[4,3]=TP_read_100\n",
    "df_all_new.iloc[4,4]=FP_read_100\n",
    "df_all_new.iloc[4,5]=FN_read_100\n",
    "\n",
    "sensitivity_confidence_interval_AI, PPV_confidence_interval_AI, F1_confidence_interval_AI \\\n",
    "= sensitivity_and_specificity_with_confidence_intervals(TP_read_100, FP_read_100, FN_read_100, 0, alpha=0.95)\n",
    "\n",
    "ci_sens_ai=[np.round(x,2) for x in sensitivity_confidence_interval_AI]\n",
    "ci_ppv_ai=[np.round(x,2) for x in PPV_confidence_interval_AI]\n",
    "ci_f1_ai=[np.round(x,2) for x in F1_confidence_interval_AI]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[4]=str(df_all_new['sensitivity (95% CI)'].iloc[4])+' '+str(tuple(ci_sens_ai))\n",
    "df_all_new['PPV (95% CI)'].iloc[4]=str(df_all_new['PPV (95% CI)'].iloc[4])+' '+str(tuple(ci_ppv_ai))\n",
    "df_all_new['F1 score (95% CI)'].iloc[4]=str(df_all_new['F1 score (95% CI)'].iloc[4])+' '+str(tuple(ci_f1_ai))\n",
    "\n",
    "\n",
    "df_all_new.iloc[1,0]=np.round(sensitivity(TP_AI_100_300,FN_AI_100_300),2)\n",
    "df_all_new.iloc[1,1]=np.round(PPV(TP_AI_100_300,FP_AI_100_300),2)\n",
    "df_all_new.iloc[1,2]=np.round(F1score(TP_AI_100_300,FP_AI_100_300,FN_AI_100_300),2)\n",
    "df_all_new.iloc[1,3]=TP_AI_100_300\n",
    "df_all_new.iloc[1,4]=FP_AI_100_300\n",
    "df_all_new.iloc[1,5]=FN_AI_100_300\n",
    "\n",
    "sensitivity_confidence_interval_AI, PPV_confidence_interval_AI, F1_confidence_interval_AI \\\n",
    "= sensitivity_and_specificity_with_confidence_intervals(TP_AI_100_300, FP_AI_100_300, FN_AI_100_300, 0, alpha=0.95)\n",
    "\n",
    "ci_sens_ai=[np.round(x,2) for x in sensitivity_confidence_interval_AI]\n",
    "ci_ppv_ai=[np.round(x,2) for x in PPV_confidence_interval_AI]\n",
    "ci_f1_ai=[np.round(x,2) for x in F1_confidence_interval_AI]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[1]=str(df_all_new['sensitivity (95% CI)'].iloc[1])+' '+str(tuple(ci_sens_ai))\n",
    "df_all_new['PPV (95% CI)'].iloc[1]=str(df_all_new['PPV (95% CI)'].iloc[1])+' '+str(tuple(ci_ppv_ai))\n",
    "df_all_new['F1 score (95% CI)'].iloc[1]=str(df_all_new['F1 score (95% CI)'].iloc[1])+' '+str(tuple(ci_f1_ai))\n",
    "\n",
    "\n",
    "df_all_new.iloc[5,0]=np.round(sensitivity(TP_read_100_300,FN_read_100_300),2)\n",
    "df_all_new.iloc[5,1]=np.round(PPV(TP_read_100_300,FP_read_100_300),2)\n",
    "df_all_new.iloc[5,2]=np.round(F1score(TP_read_100_300,FP_read_100_300,FN_read_100_300),2)\n",
    "df_all_new.iloc[5,3]=TP_read_100_300\n",
    "df_all_new.iloc[5,4]=FP_read_100_300\n",
    "df_all_new.iloc[5,5]=FN_read_100_300\n",
    "\n",
    "sensitivity_confidence_interval_AI, PPV_confidence_interval_AI, F1_confidence_interval_AI \\\n",
    "= sensitivity_and_specificity_with_confidence_intervals(TP_read_100_300, FP_read_100_300, FN_read_100_300, 0, alpha=0.95)\n",
    "\n",
    "ci_sens_ai=[np.round(x,2) for x in sensitivity_confidence_interval_AI]\n",
    "ci_ppv_ai=[np.round(x,2) for x in PPV_confidence_interval_AI]\n",
    "ci_f1_ai=[np.round(x,2) for x in F1_confidence_interval_AI]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[5]=str(df_all_new['sensitivity (95% CI)'].iloc[5])+' '+str(tuple(ci_sens_ai))\n",
    "df_all_new['PPV (95% CI)'].iloc[5]=str(df_all_new['PPV (95% CI)'].iloc[5])+' '+str(tuple(ci_ppv_ai))\n",
    "df_all_new['F1 score (95% CI)'].iloc[5]=str(df_all_new['F1 score (95% CI)'].iloc[5])+' '+str(tuple(ci_f1_ai))\n",
    "\n",
    "\n",
    "df_all_new.iloc[2,0]=np.round(sensitivity(TP_AI_300,FN_AI_300),2)\n",
    "df_all_new.iloc[2,1]=np.round(PPV(TP_AI_300,FP_AI_300),2)\n",
    "df_all_new.iloc[2,2]=np.round(F1score(TP_AI_300,FP_AI_300,FN_AI_300),2)\n",
    "df_all_new.iloc[2,3]=TP_AI_300\n",
    "df_all_new.iloc[2,4]=FP_AI_300\n",
    "df_all_new.iloc[2,5]=FN_AI_300\n",
    "\n",
    "sensitivity_confidence_interval_AI, PPV_confidence_interval_AI, F1_confidence_interval_AI \\\n",
    "= sensitivity_and_specificity_with_confidence_intervals(TP_AI_300, FP_AI_300, FN_AI_300, 0, alpha=0.95)\n",
    "\n",
    "ci_sens_ai=[np.round(x,2) for x in sensitivity_confidence_interval_AI]\n",
    "ci_ppv_ai=[np.round(x,2) for x in PPV_confidence_interval_AI]\n",
    "ci_f1_ai=[np.round(x,2) for x in F1_confidence_interval_AI]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[2]=str(df_all_new['sensitivity (95% CI)'].iloc[2])+' '+str(tuple(ci_sens_ai))\n",
    "df_all_new['PPV (95% CI)'].iloc[2]=str(df_all_new['PPV (95% CI)'].iloc[2])+' '+str(tuple(ci_ppv_ai))\n",
    "df_all_new['F1 score (95% CI)'].iloc[2]=str(df_all_new['F1 score (95% CI)'].iloc[2])+' '+str(tuple(ci_f1_ai))\n",
    "\n",
    "\n",
    "df_all_new.iloc[6,0]=np.round(sensitivity(TP_read_300,FN_read_300),2)\n",
    "df_all_new.iloc[6,1]=np.round(PPV(TP_read_300,FP_read_300),2)\n",
    "df_all_new.iloc[6,2]=np.round(F1score(TP_read_300,FP_read_300,FN_read_300),2)\n",
    "df_all_new.iloc[6,3]=TP_read_300\n",
    "df_all_new.iloc[6,4]=FP_read_300\n",
    "df_all_new.iloc[6,5]=FN_read_300\n",
    "\n",
    "sensitivity_confidence_interval_AI, PPV_confidence_interval_AI, F1_confidence_interval_AI \\\n",
    "= sensitivity_and_specificity_with_confidence_intervals(TP_read_300, FP_read_300, FN_read_300, 0, alpha=0.95)\n",
    "\n",
    "ci_sens_ai=[np.round(x,2) for x in sensitivity_confidence_interval_AI]\n",
    "ci_ppv_ai=[np.round(x,2) for x in PPV_confidence_interval_AI]\n",
    "ci_f1_ai=[np.round(x,2) for x in F1_confidence_interval_AI]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[6]=str(df_all_new['sensitivity (95% CI)'].iloc[6])+' '+str(tuple(ci_sens_ai))\n",
    "df_all_new['PPV (95% CI)'].iloc[6]=str(df_all_new['PPV (95% CI)'].iloc[6])+' '+str(tuple(ci_ppv_ai))\n",
    "df_all_new['F1 score (95% CI)'].iloc[6]=str(df_all_new['F1 score (95% CI)'].iloc[6])+' '+str(tuple(ci_f1_ai))\n",
    "\n",
    "\n",
    "df_all_new.iloc[3,0]=0\n",
    "df_all_new.iloc[3,1]=0\n",
    "df_all_new.iloc[3,2]=0\n",
    "df_all_new.iloc[3,3]=0\n",
    "df_all_new.iloc[3,4]=0\n",
    "df_all_new.iloc[3,5]=0\n",
    "\n",
    "\n",
    "AI_all=np.sum(df_all_new.iloc[0:3,3:].values)\n",
    "reader_all=np.sum(df_all_new.iloc[4:7,3:].values)\n",
    "\n",
    "df_all_new['All findings']=df_all_new['nodules correctly detected']+df_all_new['non-nodules incorrectly detected']+df_all_new['nodules missed']\n",
    "\n",
    "df_all_new.iloc[7,0]=''\n",
    "df_all_new.iloc[7,1]=''\n",
    "df_all_new.iloc[7,2]=''\n",
    "df_all_new.iloc[7,3]=''\n",
    "df_all_new.iloc[7,4]=''\n",
    "df_all_new.iloc[7,5]=''\n",
    "df_all_new.iloc[7,6]=np.sum(df_all_new['All findings'].iloc[4:7])\n",
    "\n",
    "df_all_new.iloc[3,0]=''\n",
    "df_all_new.iloc[3,1]=''\n",
    "df_all_new.iloc[3,2]=''\n",
    "df_all_new.iloc[3,3]=''\n",
    "df_all_new.iloc[3,4]=''\n",
    "df_all_new.iloc[3,5]=''\n",
    "df_all_new.iloc[3,6]=np.sum(df_all_new['All findings'].iloc[0:3])\n",
    "\n",
    "for i in range(7):\n",
    "    if i!=3:\n",
    "        if i<3:\n",
    "            sum_all=AI_all\n",
    "        elif i>3:\n",
    "            sum_all=reader_all\n",
    "            \n",
    "        percentage_tp=np.round((df_all_new.iloc[i][3]/sum_all)*100,1) \n",
    "        df_all_new['nodules correctly detected'].iloc[i]=str(df_all_new.iloc[i][3])+' ('+str(percentage_tp)+'%)'\n",
    "\n",
    "        percentage_fp=np.round((df_all_new.iloc[i][4]/sum_all)*100,1) \n",
    "        df_all_new['non-nodules incorrectly detected'].iloc[i]=str(df_all_new.iloc[i][4])+' ('+str(percentage_fp)+'%)'\n",
    "\n",
    "        percentage_fn=np.round((df_all_new.iloc[i][5]/sum_all)*100,1) \n",
    "        df_all_new['nodules missed'].iloc[i]=str(df_all_new.iloc[i][5])+' ('+str(percentage_fn)+'%)'\n",
    "\n",
    "        df_all_new['All findings'].iloc[i]=str(df_all_new.iloc[i][6])+' ('+str(np.round(100*df_all_new.iloc[i][6]/sum_all,1))+'%)'\n",
    "\n",
    "    \n",
    "df_all_new['All findings'].iloc[3]=str(df_all_new.iloc[3][6])+' (100%)'\n",
    "df_all_new['All findings'].iloc[7]=str(df_all_new.iloc[7][6])+' (100%)'\n",
    "\n",
    "df_all_new #Detection performance comparison for nodules and lymph nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc3ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_new.to_excel('nodules_lymph_volumes_low.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b369b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For nodules and lymph nodes (AI vs reader) of 30-100mm3 with continuity correction (not exact) p value is 0.11834981273562842\n",
      "For nodules and lymph nodes (AI vs reader) of 100-300mm3 with continuity correction (not exact) p value is 0.22067136191984324\n"
     ]
    }
   ],
   "source": [
    "data=[[TP_both_100, FN_read_100],\n",
    "        [FN_AI_100,0]]\n",
    "# print(data)\n",
    "\n",
    "# McNemar's Test without continuity correction\n",
    "print(\"For nodules and lymph nodes (AI vs reader) of 30-100mm3 with continuity correction (not exact) p value is\",mcnemar(data, exact=False,correction=True).pvalue)\n",
    "# print(\"\\n\")\n",
    "\n",
    "# #For FPs\n",
    "# data=[[0, FP_AI_100], \n",
    "#         [FP_read_100, 0]]\n",
    "# # print(data)\n",
    "\n",
    "# # McNemar's Test without continuity correction\n",
    "# print(\"For FP findings of 30-100mm3, with continuity correction (not exact) p value is\",mcnemar(data, exact=False,correction=True).pvalue)\n",
    "# print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "data=[[TP_both_100_300,FN_read_100_300 ], \n",
    "        [FN_AI_100_300, 0]]\n",
    "# print(data)\n",
    "\n",
    "# McNemar's Test without continuity correction\n",
    "print(\"For nodules and lymph nodes (AI vs reader) of 100-300mm3 with continuity correction (not exact) p value is\",mcnemar(data, exact=False,correction=True).pvalue)\n",
    "# print(\"\\n\")\n",
    "\n",
    "# #For FPs\n",
    "# data=[[0, FP_AI_100_300], \n",
    "#         [FP_read_100_300, 0]]\n",
    "# # print(data)\n",
    "\n",
    "# # McNemar's Test without continuity correction\n",
    "# print(\"For FP findings of 100-300mm3, with continuity correction (not exact) p value is\",mcnemar(data, exact=False,correction=True).pvalue)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89d30c20",
   "metadata": {},
   "source": [
    "High BMI volume subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baf96f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High BMI numbers\n",
      "TP_AI_100:  76\n",
      "FP_AI_100:  11\n",
      "FN_AI_100:  29\n",
      "TP_read_100:  79\n",
      "FP_read_100:  8\n",
      "FN_read_100:  26\n",
      "TP_AI_100_300:  18\n",
      "FP_AI_100_300:  27\n",
      "FN_AI_100_300:  4\n",
      "TP_read_100_300:  17\n",
      "FP_read_100_300:  1\n",
      "FN_read_100_300:  5\n",
      "TP_AI_300:  4\n",
      "FP_AI_300:  15\n",
      "FN_AI_300:  0\n",
      "TP_read_300:  4\n",
      "FP_read_300:  0\n",
      "FN_read_300:  0\n",
      "TP_both_100:  50\n",
      "TP_both_100_300:  13\n",
      "TP_both_300:  4\n"
     ]
    }
   ],
   "source": [
    "TP_AI_100=TP_high_30_100+ai_only_nods_high_30_100+ai_lymph_high_30_100\n",
    "FP_AI_100=ai_nonods_high_30_100\n",
    "FN_AI_100=reader_nods_high_30_100 #nodules of reader excluding lymph nodes\n",
    "\n",
    "TP_read_100=TP_high_30_100+reader_nods_high_30_100\n",
    "FP_read_100=reader_nonods_high_30_100\n",
    "FN_read_100=ai_only_nods_high_30_100+ai_lymph_high_30_100 #nodules of AI excluding lymph nodes\n",
    "\n",
    "TP_AI_100_300=TP_high_100_300+ai_only_nods_high_100_300+ai_lymph_high_100_300\n",
    "FP_AI_100_300=ai_nonods_high_100_300\n",
    "FN_AI_100_300=reader_nods_high_100_300 #nodules of reader excluding lymph nodes\n",
    "\n",
    "TP_read_100_300=TP_high_100_300+reader_nods_high_100_300\n",
    "FP_read_100_300=reader_nonods_high_100_300\n",
    "FN_read_100_300=ai_only_nods_high_100_300+ai_lymph_high_100_300 #nodules of AI excluding lymph nodes\n",
    "\n",
    "TP_AI_300=TP_high_300+ai_only_nods_high_300+ai_lymph_high_300\n",
    "FP_AI_300=ai_nonods_high_300\n",
    "FN_AI_300=reader_nods_high_300 #nodules of reader excluding lymph nodes\n",
    "\n",
    "TP_read_300=TP_high_300+reader_nods_high_300\n",
    "FP_read_300=reader_nonods_high_300\n",
    "FN_read_300=ai_only_nods_high_300+ai_lymph_high_300 #nodules of AI excluding lymph nodes\n",
    "\n",
    "TP_both_100=TP_high_30_100\n",
    "TP_both_100_300=TP_high_100_300\n",
    "TP_both_300=TP_high_300\n",
    "\n",
    "#Print the above\n",
    "print(\"High BMI numbers\")\n",
    "print('TP_AI_100: ',TP_AI_100)\n",
    "print('FP_AI_100: ',FP_AI_100)\n",
    "print('FN_AI_100: ',FN_AI_100)\n",
    "print('TP_read_100: ',TP_read_100)\n",
    "print('FP_read_100: ',FP_read_100)\n",
    "print('FN_read_100: ',FN_read_100)\n",
    "print('TP_AI_100_300: ',TP_AI_100_300)\n",
    "print('FP_AI_100_300: ',FP_AI_100_300)\n",
    "print('FN_AI_100_300: ',FN_AI_100_300)\n",
    "print('TP_read_100_300: ',TP_read_100_300)\n",
    "print('FP_read_100_300: ',FP_read_100_300)\n",
    "print('FN_read_100_300: ',FN_read_100_300)\n",
    "print('TP_AI_300: ',TP_AI_300)\n",
    "print('FP_AI_300: ',FP_AI_300)\n",
    "print('FN_AI_300: ',FN_AI_300)\n",
    "print('TP_read_300: ',TP_read_300)\n",
    "print('FP_read_300: ',FP_read_300)\n",
    "print('FN_read_300: ',FN_read_300)\n",
    "print('TP_both_100: ',TP_both_100)\n",
    "print('TP_both_100_300: ',TP_both_100_300)\n",
    "print('TP_both_300: ',TP_both_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6acb278",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Table with Reader, AI and consensus findings for nodules and lymph nodes for high BMI\n",
    "df_all_new=pd.DataFrame(columns=['Reader','AI','Consensus'],\n",
    "                        index=['Non-nodules','TP 30-100mm3','TP 100-300mm3', 'TP >300mm3'])\n",
    "\n",
    "df_all_new.index.name = 'High BMI cases'\n",
    "\n",
    "df_all_new['Reader']=[FP_read_100+FP_read_100_300+FP_read_300,TP_both_100+FN_AI_100,TP_both_100_300+FN_AI_100_300, TP_both_300+FN_AI_300]\n",
    "df_all_new['AI']=[FP_AI_100+FP_AI_100_300+FP_AI_300,TP_both_100+FN_read_100,TP_both_100_300+FN_read_100_300,TP_both_300+FN_read_300]\n",
    "df_all_new['Consensus']=[FP_AI_100+FP_AI_100_300+FP_AI_300+FP_read_100+FP_read_100_300+FP_read_300,FN_AI_100+FN_read_100, FN_AI_100_300+FN_read_100_300, FN_AI_300+FN_read_300]\n",
    "\n",
    "df_all_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7bd9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_new.to_excel('non_nodules_and_TP_high.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45814181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensitivity (95% CI)</th>\n",
       "      <th>PPV (95% CI)</th>\n",
       "      <th>F1 score (95% CI)</th>\n",
       "      <th>nodules correctly detected</th>\n",
       "      <th>non-nodules incorrectly detected</th>\n",
       "      <th>nodules missed</th>\n",
       "      <th>All findings</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GT by radiologists for discrepancies</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AI, high 30-100mm3</th>\n",
       "      <td>0.72 (0.63, 0.8)</td>\n",
       "      <td>0.87 (0.78, 0.93)</td>\n",
       "      <td>0.79 (0.73, 0.85)</td>\n",
       "      <td>76 (41.3%)</td>\n",
       "      <td>11 (6.0%)</td>\n",
       "      <td>29 (15.8%)</td>\n",
       "      <td>116 (63.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AI, high 100-300mm3</th>\n",
       "      <td>0.82 (0.59, 0.94)</td>\n",
       "      <td>0.4 (0.26, 0.56)</td>\n",
       "      <td>0.54 (0.41, 0.66)</td>\n",
       "      <td>18 (9.8%)</td>\n",
       "      <td>27 (14.7%)</td>\n",
       "      <td>4 (2.2%)</td>\n",
       "      <td>49 (26.6%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AI, high 300+mm3</th>\n",
       "      <td>1.0 (0.4, 0.98)</td>\n",
       "      <td>0.21 (0.07, 0.46)</td>\n",
       "      <td>0.35 (0.17, 0.57)</td>\n",
       "      <td>4 (2.2%)</td>\n",
       "      <td>15 (8.2%)</td>\n",
       "      <td>0 (0.0%)</td>\n",
       "      <td>19 (10.3%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>184 (100%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reader, high 30-100mm3</th>\n",
       "      <td>0.75 (0.66, 0.83)</td>\n",
       "      <td>0.91 (0.82, 0.96)</td>\n",
       "      <td>0.82 (0.76, 0.87)</td>\n",
       "      <td>79 (56.4%)</td>\n",
       "      <td>8 (5.7%)</td>\n",
       "      <td>26 (18.6%)</td>\n",
       "      <td>113 (80.7%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reader, high 100-300mm3</th>\n",
       "      <td>0.77 (0.54, 0.91)</td>\n",
       "      <td>0.94 (0.71, 1.0)</td>\n",
       "      <td>0.85 (0.69, 0.94)</td>\n",
       "      <td>17 (12.1%)</td>\n",
       "      <td>1 (0.7%)</td>\n",
       "      <td>5 (3.6%)</td>\n",
       "      <td>23 (16.4%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reader, high 300+mm3</th>\n",
       "      <td>1.0 (0.4, 0.98)</td>\n",
       "      <td>1.0 (0.4, 0.98)</td>\n",
       "      <td>1.0 (0.6, 0.99)</td>\n",
       "      <td>4 (2.9%)</td>\n",
       "      <td>0 (0.0%)</td>\n",
       "      <td>0 (0.0%)</td>\n",
       "      <td>4 (2.9%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>140 (100%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     sensitivity (95% CI)       PPV (95% CI)  \\\n",
       "GT by radiologists for discrepancies                                           \n",
       "AI, high 30-100mm3                       0.72 (0.63, 0.8)  0.87 (0.78, 0.93)   \n",
       "AI, high 100-300mm3                     0.82 (0.59, 0.94)   0.4 (0.26, 0.56)   \n",
       "AI, high 300+mm3                          1.0 (0.4, 0.98)  0.21 (0.07, 0.46)   \n",
       "                                                                               \n",
       "reader, high 30-100mm3                  0.75 (0.66, 0.83)  0.91 (0.82, 0.96)   \n",
       "reader, high 100-300mm3                 0.77 (0.54, 0.91)   0.94 (0.71, 1.0)   \n",
       "reader, high 300+mm3                      1.0 (0.4, 0.98)    1.0 (0.4, 0.98)   \n",
       "                                                                               \n",
       "\n",
       "                                      F1 score (95% CI)  \\\n",
       "GT by radiologists for discrepancies                      \n",
       "AI, high 30-100mm3                    0.79 (0.73, 0.85)   \n",
       "AI, high 100-300mm3                   0.54 (0.41, 0.66)   \n",
       "AI, high 300+mm3                      0.35 (0.17, 0.57)   \n",
       "                                                          \n",
       "reader, high 30-100mm3                0.82 (0.76, 0.87)   \n",
       "reader, high 100-300mm3               0.85 (0.69, 0.94)   \n",
       "reader, high 300+mm3                    1.0 (0.6, 0.99)   \n",
       "                                                          \n",
       "\n",
       "                                     nodules correctly detected  \\\n",
       "GT by radiologists for discrepancies                              \n",
       "AI, high 30-100mm3                                   76 (41.3%)   \n",
       "AI, high 100-300mm3                                   18 (9.8%)   \n",
       "AI, high 300+mm3                                       4 (2.2%)   \n",
       "                                                                  \n",
       "reader, high 30-100mm3                               79 (56.4%)   \n",
       "reader, high 100-300mm3                              17 (12.1%)   \n",
       "reader, high 300+mm3                                   4 (2.9%)   \n",
       "                                                                  \n",
       "\n",
       "                                     non-nodules incorrectly detected  \\\n",
       "GT by radiologists for discrepancies                                    \n",
       "AI, high 30-100mm3                                          11 (6.0%)   \n",
       "AI, high 100-300mm3                                        27 (14.7%)   \n",
       "AI, high 300+mm3                                            15 (8.2%)   \n",
       "                                                                        \n",
       "reader, high 30-100mm3                                       8 (5.7%)   \n",
       "reader, high 100-300mm3                                      1 (0.7%)   \n",
       "reader, high 300+mm3                                         0 (0.0%)   \n",
       "                                                                        \n",
       "\n",
       "                                     nodules missed All findings  \n",
       "GT by radiologists for discrepancies                              \n",
       "AI, high 30-100mm3                       29 (15.8%)  116 (63.0%)  \n",
       "AI, high 100-300mm3                        4 (2.2%)   49 (26.6%)  \n",
       "AI, high 300+mm3                           0 (0.0%)   19 (10.3%)  \n",
       "                                                      184 (100%)  \n",
       "reader, high 30-100mm3                   26 (18.6%)  113 (80.7%)  \n",
       "reader, high 100-300mm3                    5 (3.6%)   23 (16.4%)  \n",
       "reader, high 300+mm3                       0 (0.0%)     4 (2.9%)  \n",
       "                                                      140 (100%)  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Comparison between reader and AI for volume subgroups\n",
    "df_all_new=pd.DataFrame(columns=['sensitivity (95% CI)','PPV (95% CI)','F1 score (95% CI)','nodules correctly detected','non-nodules incorrectly detected','nodules missed'], \n",
    "                        index=['AI, high 30-100mm3', 'AI, high 100-300mm3','AI, high 300+mm3','',\n",
    "                               'reader, high 30-100mm3','reader, high 100-300mm3', 'reader, high 300+mm3',''\n",
    "                              ])\n",
    "\n",
    "df_all_new.index.name = 'GT by radiologists for discrepancies' \n",
    "\n",
    "\n",
    "df_all_new.iloc[0,0]=np.round(sensitivity(TP_AI_100,FN_AI_100),2)\n",
    "df_all_new.iloc[0,1]=np.round(PPV(TP_AI_100,FP_AI_100),2)\n",
    "df_all_new.iloc[0,2]=np.round(F1score(TP_AI_100,FP_AI_100,FN_AI_100),2)\n",
    "df_all_new.iloc[0,3]=TP_AI_100\n",
    "df_all_new.iloc[0,4]=FP_AI_100\n",
    "df_all_new.iloc[0,5]=FN_AI_100\n",
    "\n",
    "sensitivity_confidence_interval_AI, PPV_confidence_interval_AI, F1_confidence_interval_AI \\\n",
    "= sensitivity_and_specificity_with_confidence_intervals(TP_AI_100, FP_AI_100, FN_AI_100, 0, alpha=0.95)\n",
    "\n",
    "ci_sens_ai=[np.round(x,2) for x in sensitivity_confidence_interval_AI]\n",
    "ci_ppv_ai=[np.round(x,2) for x in PPV_confidence_interval_AI]\n",
    "ci_f1_ai=[np.round(x,2) for x in F1_confidence_interval_AI]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[0]=str(df_all_new['sensitivity (95% CI)'].iloc[0])+' '+str(tuple(ci_sens_ai))\n",
    "df_all_new['PPV (95% CI)'].iloc[0]=str(df_all_new['PPV (95% CI)'].iloc[0])+' '+str(tuple(ci_ppv_ai))\n",
    "df_all_new['F1 score (95% CI)'].iloc[0]=str(df_all_new['F1 score (95% CI)'].iloc[0])+' '+str(tuple(ci_f1_ai))\n",
    "\n",
    "\n",
    "df_all_new.iloc[4,0]=np.round(sensitivity(TP_read_100,FN_read_100),2)\n",
    "df_all_new.iloc[4,1]=np.round(PPV(TP_read_100,FP_read_100),2)\n",
    "df_all_new.iloc[4,2]=np.round(F1score(TP_read_100,FP_read_100,FN_read_100),2)\n",
    "df_all_new.iloc[4,3]=TP_read_100\n",
    "df_all_new.iloc[4,4]=FP_read_100\n",
    "df_all_new.iloc[4,5]=FN_read_100\n",
    "\n",
    "sensitivity_confidence_interval_AI, PPV_confidence_interval_AI, F1_confidence_interval_AI \\\n",
    "= sensitivity_and_specificity_with_confidence_intervals(TP_read_100, FP_read_100, FN_read_100, 0, alpha=0.95)\n",
    "\n",
    "ci_sens_ai=[np.round(x,2) for x in sensitivity_confidence_interval_AI]\n",
    "ci_ppv_ai=[np.round(x,2) for x in PPV_confidence_interval_AI]\n",
    "ci_f1_ai=[np.round(x,2) for x in F1_confidence_interval_AI]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[4]=str(df_all_new['sensitivity (95% CI)'].iloc[4])+' '+str(tuple(ci_sens_ai))\n",
    "df_all_new['PPV (95% CI)'].iloc[4]=str(df_all_new['PPV (95% CI)'].iloc[4])+' '+str(tuple(ci_ppv_ai))\n",
    "df_all_new['F1 score (95% CI)'].iloc[4]=str(df_all_new['F1 score (95% CI)'].iloc[4])+' '+str(tuple(ci_f1_ai))\n",
    "\n",
    "\n",
    "df_all_new.iloc[1,0]=np.round(sensitivity(TP_AI_100_300,FN_AI_100_300),2)\n",
    "df_all_new.iloc[1,1]=np.round(PPV(TP_AI_100_300,FP_AI_100_300),2)\n",
    "df_all_new.iloc[1,2]=np.round(F1score(TP_AI_100_300,FP_AI_100_300,FN_AI_100_300),2)\n",
    "df_all_new.iloc[1,3]=TP_AI_100_300\n",
    "df_all_new.iloc[1,4]=FP_AI_100_300\n",
    "df_all_new.iloc[1,5]=FN_AI_100_300\n",
    "\n",
    "sensitivity_confidence_interval_AI, PPV_confidence_interval_AI, F1_confidence_interval_AI \\\n",
    "= sensitivity_and_specificity_with_confidence_intervals(TP_AI_100_300, FP_AI_100_300, FN_AI_100_300, 0, alpha=0.95)\n",
    "\n",
    "ci_sens_ai=[np.round(x,2) for x in sensitivity_confidence_interval_AI]\n",
    "ci_ppv_ai=[np.round(x,2) for x in PPV_confidence_interval_AI]\n",
    "ci_f1_ai=[np.round(x,2) for x in F1_confidence_interval_AI]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[1]=str(df_all_new['sensitivity (95% CI)'].iloc[1])+' '+str(tuple(ci_sens_ai))\n",
    "df_all_new['PPV (95% CI)'].iloc[1]=str(df_all_new['PPV (95% CI)'].iloc[1])+' '+str(tuple(ci_ppv_ai))\n",
    "df_all_new['F1 score (95% CI)'].iloc[1]=str(df_all_new['F1 score (95% CI)'].iloc[1])+' '+str(tuple(ci_f1_ai))\n",
    "\n",
    "\n",
    "df_all_new.iloc[5,0]=np.round(sensitivity(TP_read_100_300,FN_read_100_300),2)\n",
    "df_all_new.iloc[5,1]=np.round(PPV(TP_read_100_300,FP_read_100_300),2)\n",
    "df_all_new.iloc[5,2]=np.round(F1score(TP_read_100_300,FP_read_100_300,FN_read_100_300),2)\n",
    "df_all_new.iloc[5,3]=TP_read_100_300\n",
    "df_all_new.iloc[5,4]=FP_read_100_300\n",
    "df_all_new.iloc[5,5]=FN_read_100_300\n",
    "\n",
    "sensitivity_confidence_interval_AI, PPV_confidence_interval_AI, F1_confidence_interval_AI \\\n",
    "= sensitivity_and_specificity_with_confidence_intervals(TP_read_100_300, FP_read_100_300, FN_read_100_300, 0, alpha=0.95)\n",
    "\n",
    "ci_sens_ai=[np.round(x,2) for x in sensitivity_confidence_interval_AI]\n",
    "ci_ppv_ai=[np.round(x,2) for x in PPV_confidence_interval_AI]\n",
    "ci_f1_ai=[np.round(x,2) for x in F1_confidence_interval_AI]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[5]=str(df_all_new['sensitivity (95% CI)'].iloc[5])+' '+str(tuple(ci_sens_ai))\n",
    "df_all_new['PPV (95% CI)'].iloc[5]=str(df_all_new['PPV (95% CI)'].iloc[5])+' '+str(tuple(ci_ppv_ai))\n",
    "df_all_new['F1 score (95% CI)'].iloc[5]=str(df_all_new['F1 score (95% CI)'].iloc[5])+' '+str(tuple(ci_f1_ai))\n",
    "\n",
    "\n",
    "df_all_new.iloc[2,0]=np.round(sensitivity(TP_AI_300,FN_AI_300),2)\n",
    "df_all_new.iloc[2,1]=np.round(PPV(TP_AI_300,FP_AI_300),2)\n",
    "df_all_new.iloc[2,2]=np.round(F1score(TP_AI_300,FP_AI_300,FN_AI_300),2)\n",
    "df_all_new.iloc[2,3]=TP_AI_300\n",
    "df_all_new.iloc[2,4]=FP_AI_300\n",
    "df_all_new.iloc[2,5]=FN_AI_300\n",
    "\n",
    "sensitivity_confidence_interval_AI, PPV_confidence_interval_AI, F1_confidence_interval_AI \\\n",
    "= sensitivity_and_specificity_with_confidence_intervals(TP_AI_300, FP_AI_300, FN_AI_300, 0, alpha=0.95)\n",
    "\n",
    "ci_sens_ai=[np.round(x,2) for x in sensitivity_confidence_interval_AI]\n",
    "ci_ppv_ai=[np.round(x,2) for x in PPV_confidence_interval_AI]\n",
    "ci_f1_ai=[np.round(x,2) for x in F1_confidence_interval_AI]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[2]=str(df_all_new['sensitivity (95% CI)'].iloc[2])+' '+str(tuple(ci_sens_ai))\n",
    "df_all_new['PPV (95% CI)'].iloc[2]=str(df_all_new['PPV (95% CI)'].iloc[2])+' '+str(tuple(ci_ppv_ai))\n",
    "df_all_new['F1 score (95% CI)'].iloc[2]=str(df_all_new['F1 score (95% CI)'].iloc[2])+' '+str(tuple(ci_f1_ai))\n",
    "\n",
    "\n",
    "df_all_new.iloc[6,0]=np.round(sensitivity(TP_read_300,FN_read_300),2)\n",
    "df_all_new.iloc[6,1]=np.round(PPV(TP_read_300,FP_read_300),2)\n",
    "df_all_new.iloc[6,2]=np.round(F1score(TP_read_300,FP_read_300,FN_read_300),2)\n",
    "df_all_new.iloc[6,3]=TP_read_300\n",
    "df_all_new.iloc[6,4]=FP_read_300\n",
    "df_all_new.iloc[6,5]=FN_read_300\n",
    "\n",
    "sensitivity_confidence_interval_AI, PPV_confidence_interval_AI, F1_confidence_interval_AI \\\n",
    "= sensitivity_and_specificity_with_confidence_intervals(TP_read_300, FP_read_300, FN_read_300, 0, alpha=0.95)\n",
    "\n",
    "ci_sens_ai=[np.round(x,2) for x in sensitivity_confidence_interval_AI]\n",
    "ci_ppv_ai=[np.round(x,2) for x in PPV_confidence_interval_AI]\n",
    "ci_f1_ai=[np.round(x,2) for x in F1_confidence_interval_AI]\n",
    "\n",
    "df_all_new['sensitivity (95% CI)'].iloc[6]=str(df_all_new['sensitivity (95% CI)'].iloc[6])+' '+str(tuple(ci_sens_ai))\n",
    "df_all_new['PPV (95% CI)'].iloc[6]=str(df_all_new['PPV (95% CI)'].iloc[6])+' '+str(tuple(ci_ppv_ai))\n",
    "df_all_new['F1 score (95% CI)'].iloc[6]=str(df_all_new['F1 score (95% CI)'].iloc[6])+' '+str(tuple(ci_f1_ai))\n",
    "\n",
    "\n",
    "df_all_new.iloc[3,0]=0\n",
    "df_all_new.iloc[3,1]=0\n",
    "df_all_new.iloc[3,2]=0\n",
    "df_all_new.iloc[3,3]=0\n",
    "df_all_new.iloc[3,4]=0\n",
    "df_all_new.iloc[3,5]=0\n",
    "\n",
    "\n",
    "AI_all=np.sum(df_all_new.iloc[0:3,3:].values)\n",
    "reader_all=np.sum(df_all_new.iloc[4:7,3:].values)\n",
    "\n",
    "df_all_new['All findings']=df_all_new['nodules correctly detected']+df_all_new['non-nodules incorrectly detected']+df_all_new['nodules missed']\n",
    "\n",
    "df_all_new.iloc[7,0]=''\n",
    "df_all_new.iloc[7,1]=''\n",
    "df_all_new.iloc[7,2]=''\n",
    "df_all_new.iloc[7,3]=''\n",
    "df_all_new.iloc[7,4]=''\n",
    "df_all_new.iloc[7,5]=''\n",
    "df_all_new.iloc[7,6]=np.sum(df_all_new['All findings'].iloc[4:7])\n",
    "\n",
    "df_all_new.iloc[3,0]=''\n",
    "df_all_new.iloc[3,1]=''\n",
    "df_all_new.iloc[3,2]=''\n",
    "df_all_new.iloc[3,3]=''\n",
    "df_all_new.iloc[3,4]=''\n",
    "df_all_new.iloc[3,5]=''\n",
    "df_all_new.iloc[3,6]=np.sum(df_all_new['All findings'].iloc[0:3])\n",
    "\n",
    "for i in range(7):\n",
    "    if i!=3:\n",
    "        if i<3:\n",
    "            sum_all=AI_all\n",
    "        elif i>3:\n",
    "            sum_all=reader_all\n",
    "            \n",
    "        percentage_tp=np.round((df_all_new.iloc[i][3]/sum_all)*100,1) \n",
    "        df_all_new['nodules correctly detected'].iloc[i]=str(df_all_new.iloc[i][3])+' ('+str(percentage_tp)+'%)'\n",
    "\n",
    "        percentage_fp=np.round((df_all_new.iloc[i][4]/sum_all)*100,1) \n",
    "        df_all_new['non-nodules incorrectly detected'].iloc[i]=str(df_all_new.iloc[i][4])+' ('+str(percentage_fp)+'%)'\n",
    "\n",
    "        percentage_fn=np.round((df_all_new.iloc[i][5]/sum_all)*100,1) \n",
    "        df_all_new['nodules missed'].iloc[i]=str(df_all_new.iloc[i][5])+' ('+str(percentage_fn)+'%)'\n",
    "\n",
    "        df_all_new['All findings'].iloc[i]=str(df_all_new.iloc[i][6])+' ('+str(np.round(100*df_all_new.iloc[i][6]/sum_all,1))+'%)'\n",
    "\n",
    "    \n",
    "df_all_new['All findings'].iloc[3]=str(df_all_new.iloc[3][6])+' (100%)'\n",
    "df_all_new['All findings'].iloc[7]=str(df_all_new.iloc[7][6])+' (100%)'\n",
    "\n",
    "df_all_new #Detection performance comparison for nodules and lymph nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976583dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_new.to_excel('nodules_lymph_volumes_high.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55186a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For nodules and lymph nodes (AI vs reader) of 30-100mm3 with continuity correction (not exact) p value is 0.7874064906662693\n",
      "For nodules and lymph nodes (AI vs reader) of 100-300mm3 with continuity correction (not exact) p value is 1.0\n"
     ]
    }
   ],
   "source": [
    "data=[[TP_both_100, FN_read_100],\n",
    "        [FN_AI_100,0]]\n",
    "# print(data)\n",
    "\n",
    "# McNemar's Test without continuity correction\n",
    "print(\"For nodules and lymph nodes (AI vs reader) of 30-100mm3 with continuity correction (not exact) p value is\",mcnemar(data, exact=False,correction=True).pvalue)\n",
    "\n",
    "# #For FPs\n",
    "# data=[[0, FP_AI_100], \n",
    "#         [FP_read_100, 0]]\n",
    "# # print(data)\n",
    "\n",
    "# # McNemar's Test without continuity correction\n",
    "# print(\"For FP findings of 30-100mm3, with continuity correction (not exact) p value is\",mcnemar(data, exact=False,correction=True).pvalue)\n",
    "# print(\"\\n\")\n",
    "\n",
    "\n",
    "data=[[TP_both_100_300,FN_read_100_300 ], \n",
    "        [FN_AI_100_300, 0]]\n",
    "# print(data)\n",
    "\n",
    "# McNemar's Test without continuity correction\n",
    "print(\"For nodules and lymph nodes (AI vs reader) of 100-300mm3 with continuity correction (not exact) p value is\",mcnemar(data, exact=False,correction=True).pvalue)\n",
    "\n",
    "# #For FPs\n",
    "# data=[[0, FP_AI_100_300], \n",
    "#         [FP_read_100_300, 0]]\n",
    "# # print(data)\n",
    "\n",
    "# # McNemar's Test without continuity correction\n",
    "# print(\"For FP findings of 100-300mm3, with continuity correction (not exact) p value is\",mcnemar(data, exact=False,correction=True).pvalue)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25b09d78",
   "metadata": {},
   "source": [
    "#### Mann-Whitney U test to check for differences in volumes between low/high BMI within each volume subgroup \n",
    "\n",
    "This is an unpaired test meaning that we consider each nodule as separate from others. It can be used with unequal sample sizes as well.\n",
    "Bland-Altman is not a good choice since it can be performed on nodules detected by both AI and reader to assess for agreement in the volume measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746db191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Perform the Mann-Whitney U test for nodules only\n",
    "# # stats.mannwhitneyu(group1, group2, alternative='two-sided')\n",
    "\n",
    "# #Compare all high BMI reader vs AI volumes\n",
    "# print('High BMI reader vs AI volumes p value is', stats.mannwhitneyu(reader_only_nods_high_30_100_vols+reader_only_nods_high_100_300_vols+reader_only_nods_high_300_vols,\n",
    "#                    ai_only_nods_high_30_100_vols+ai_only_nods_high_100_300_vols+ai_only_nods_high_300_vols).pvalue)\n",
    "\n",
    "# #Compare all low BMI reader vs AI volumes\n",
    "# print('Low BMI reader vs AI volumes p value is',stats.mannwhitneyu(reader_only_nods_low_30_100_vols+reader_only_nods_low_100_300_vols+reader_only_nods_low_300_vols,\n",
    "#                    ai_only_nods_low_30_100_vols+ai_only_nods_low_100_300_vols+ai_only_nods_low_300_vols).pvalue)\n",
    "\n",
    "# #Compare all low vs high BMI volumes for nodules, for reader only\n",
    "# print('Low vs high BMI for reader p value is',stats.mannwhitneyu(reader_only_nods_low_30_100_vols+reader_only_nods_low_100_300_vols+reader_only_nods_low_300_vols,\n",
    "#                    reader_only_nods_high_30_100_vols+reader_only_nods_high_100_300_vols+reader_only_nods_high_300_vols).pvalue)\n",
    "\n",
    "# #Compare all low vs high BMI volumes for nodules, for AI only\n",
    "# print('Low vs high BMI for AI p value is',stats.mannwhitneyu(ai_only_nods_high_30_100_vols+ai_only_nods_high_100_300_vols+ai_only_nods_high_300_vols,\n",
    "#                    ai_only_nods_low_30_100_vols+ai_only_nods_low_100_300_vols+ai_only_nods_low_300_vols).pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dd308a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Similarly as above for non-nodule findings\n",
    "\n",
    "# #Compare all high BMI reader vs AI volumes for non-nodule findings\n",
    "# print('High BMI reader vs AI volumes for non-nodule findings p value is',stats.mannwhitneyu(reader_nonods_high_30_100_vols+reader_nonods_high_100_300_vols+reader_nonods_high_300_vols,\n",
    "#                    ai_nonods_high_30_100_vols+ai_nonods_high_100_300_vols+ai_nonods_high_300_vols).pvalue)\n",
    "\n",
    "# #Compare all low BMI reader vs AI volumes for non-nodule findings\n",
    "# print('Low BMI reader vs AI volumes for non-nodule findings p value is',stats.mannwhitneyu(reader_nonods_low_30_100_vols+reader_nonods_low_100_300_vols+reader_nonods_low_300_vols,\n",
    "#                    ai_nonods_low_30_100_vols+ai_nonods_low_100_300_vols+ai_nonods_low_300_vols).pvalue)\n",
    "\n",
    "# #Compare all low vs high BMI volumes for reader only, for non-nodule findings\n",
    "# print('Low vs high BMI volumes for reader only, for non-nodule findings p value is',stats.mannwhitneyu(reader_nonods_low_30_100_vols+reader_nonods_low_100_300_vols+reader_nonods_low_300_vols,\n",
    "#                    reader_nonods_high_30_100_vols+reader_nonods_high_100_300_vols+reader_nonods_high_300_vols).pvalue)\n",
    "\n",
    "# #Compare all low vs high BMI volumes for AI only, for non-nodule findings\n",
    "# print('Low vs high BMI volumes for AI only, for non-nodule findings p value is',stats.mannwhitneyu(ai_nonods_high_30_100_vols+ai_nonods_high_100_300_vols+ai_nonods_high_300_vols,\n",
    "#                    ai_nonods_low_30_100_vols+ai_nonods_low_100_300_vols+ai_nonods_low_300_vols).pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08c24ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Similarly as above for nodules and lymph nodes\n",
    "\n",
    "# #Compare all high BMI reader vs AI volumes for nodules and lymph nodes\n",
    "# print('High BMI reader vs AI volumes for nodules and lymph nodes p value is',stats.mannwhitneyu(reader_only_nods_high_30_100_vols+reader_only_nods_high_100_300_vols+reader_only_nods_high_300_vols+\n",
    "#                    reader_lymph_high_30_100_vols+ reader_lymph_high_100_300_vols+ reader_lymph_high_300_vols,\n",
    "#                    ai_only_nods_high_30_100_vols+ai_only_nods_high_100_300_vols+ai_only_nods_high_300_vols+\n",
    "#                   ai_lymph_high_30_100_vols+ ai_lymph_high_100_300_vols+ ai_lymph_high_300_vols).pvalue)\n",
    "\n",
    "# #Compare all low BMI reader vs AI volumes for nodules and lymph nodes\n",
    "# print('Low BMI reader vs AI volumes for nodules and lymph nodes p value is',stats.mannwhitneyu(reader_only_nods_low_30_100_vols+reader_only_nods_low_100_300_vols+reader_only_nods_low_300_vols+\n",
    "#                    reader_lymph_low_30_100_vols+ reader_lymph_low_100_300_vols+ reader_lymph_low_300_vols,\n",
    "#                    ai_only_nods_low_30_100_vols+ai_only_nods_low_100_300_vols+ai_only_nods_low_300_vols+\n",
    "#                   ai_lymph_low_30_100_vols+ ai_lymph_low_100_300_vols+ ai_lymph_low_300_vols).pvalue)\n",
    "\n",
    "# #Compare all low vs high BMI volumes for reader only for nodules and lymph nodes\n",
    "# print('Low vs high BMI volumes for reader only for nodules and lymph nodes p value is',stats.mannwhitneyu(reader_only_nods_low_30_100_vols+reader_only_nods_low_100_300_vols+reader_only_nods_low_300_vols+\n",
    "#                    reader_lymph_low_30_100_vols+ reader_lymph_low_100_300_vols+ reader_lymph_low_300_vols,\n",
    "#                    reader_only_nods_high_30_100_vols+reader_only_nods_high_100_300_vols+reader_only_nods_high_300_vols+\n",
    "#                   reader_lymph_high_30_100_vols+ reader_lymph_high_100_300_vols+ reader_lymph_high_300_vols).pvalue)\n",
    "\n",
    "# #Compare all low vs high BMI volumes for AI only for nodules and lymph nodes\n",
    "# print('Low vs high BMI volumes for AI only for nodules and lymph nodes p value is',stats.mannwhitneyu(ai_only_nods_high_30_100_vols+ai_only_nods_high_100_300_vols+ai_only_nods_high_300_vols+\n",
    "#                    ai_lymph_high_30_100_vols+ ai_lymph_high_100_300_vols+ ai_lymph_high_300_vols,\n",
    "#                    ai_only_nods_low_30_100_vols+ai_only_nods_low_100_300_vols+ai_only_nods_low_300_vols+\n",
    "#                   ai_lymph_low_30_100_vols+ ai_lymph_low_100_300_vols+ ai_lymph_low_300_vols).pvalue)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
