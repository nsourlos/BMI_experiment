{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c4c8fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as st\n",
    "import time\n",
    "import traceback\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ae35d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"redcap_levels/redcap_output/\" #Path with BMI files from REDCap \n",
    "#Only those of interest - Taken from BMI_6-6-2023.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8784a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read excel files with data\n",
    "high=pd.read_excel(os.getcwd()+'/BMI_exp_files'+\"\\\\high_scans.xlsx\") \n",
    "low=pd.read_excel(os.getcwd()+'/BMI_exp_files'+\"\\\\low_scans.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e22aa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Needed for FN extraction\n",
    "path_low=os.getcwd()+'/details_final/low'\n",
    "path_high=os.getcwd()+'/details_final/high'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a153231",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4527591",
   "metadata": {},
   "outputs": [],
   "source": [
    "BMI_data=pd.DataFrame() #Create empty dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e56067d1",
   "metadata": {},
   "source": [
    "Changes compared to emphysema experiment:\n",
    "- 'participant_id_only' to get only a 6-digit participant, without any errors from REDCap export\n",
    "- Removed section with 'indices_delete'\n",
    "- Changed 'split' conventions below, 'BMI' column added\n",
    "- Removed '!!!'\n",
    "- Keep volumes >300mm3\n",
    "- Change to 'astype(str)' for TP\n",
    "- Change to 'astype(float)' in convert function below\n",
    "- Added 'all_values[2]!=2' condition in function below - Also added in emphysema, seems no problem\n",
    "- 'Statistics - Not needed...', 'Resample' section, 'Keep only the largest nodule of each participant' paragraph etc. deleted\n",
    "- Set 'plotting=False' by default to the function 'get_nodule_names_and_sizes'\n",
    "- Sections 'only nodules <=100mm3' and below not copied since we will probably not need any pie plots\n",
    "- We will only keep solid component\n",
    "- Excluded cases with 10 findings by initial reading\n",
    "- Excluded Peribronchial tissue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3643802b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high_redcap.csv\n",
      "176\n",
      "low_redcap.csv\n",
      "176\n"
     ]
    }
   ],
   "source": [
    "#Combine all dataframes with the different degrees of BMI\n",
    "\n",
    "total_pat=0 #count number of participants\n",
    "\n",
    "for file in os.listdir(path): #loop over dataframes\n",
    "    print(file) #print BMI degree\n",
    "    dataframe=file.split('.')[0] #Use degree of BMI as a name for each corresponding dataframe\n",
    "\n",
    "    exec(dataframe+'=pd.read_csv(os.getcwd()+\"/\"+path+file)') #read each dataframe\n",
    "\n",
    "    exec(dataframe+\"['BMI']=str(dataframe.split('.')[0])\") #Add column with BMI degree\n",
    "    BMI_data=BMI_data.append(eval(dataframe)) #Combine each BMI dataframe to another dataframe\n",
    "    total_pat=total_pat+len(eval(dataframe)) #Add number of participants of that degree to the total number of participants\n",
    "    print(len(np.unique(eval(dataframe+\"['participant_id_only']\")))) #print num of unique participants of that degree of BMI\n",
    "\n",
    "    assert(total_pat==len(BMI_data)) #Confirm that everything worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BMI_data=BMI_data.reset_index(drop=True) #Reset indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d9725a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete nodule in slice 72 (id 1 in syngo.via) due to being small \n",
    "# Find the row where 'column_to_match' matches the specific value\n",
    "row_index = BMI_data[BMI_data['participant_id'] == 888119].index[0]\n",
    "\n",
    "# Iterate over the columns and replace those containing 'n3' with NaN for the selected row\n",
    "for col in BMI_data.columns:\n",
    "    if 'n1' in col and 'n10' not in col and 'ai' not in col:\n",
    "        try:\n",
    "            BMI_data.at[row_index, col] = np.nan\n",
    "        except:\n",
    "            print('Column:',col, 'Value:',BMI_data.at[row_index, col])\n",
    "            print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2488628b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique participants is 352\n",
      "We have in total 352 participants\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique participants is {}\".format(len(np.unique(BMI_data['participant_id_only']))))\n",
    "print(\"We have in total {} participants\".format(len(BMI_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7faf1662",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BMI types ordered by severity - Taken from np.unique(BMI_data['BMI'])\n",
    "severity=['high_redcap','low_redcap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21faa683",
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_nums=[1,0] #create integers from the most severe to the least one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acd23bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'high_redcap': 1, 'low_redcap': 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "severity_dict={} #create a dict with severity name and corresponding integer\n",
    "for index,val in enumerate(severity):\n",
    "    severity_dict[val]=severity_nums[index]\n",
    "severity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bcd7893",
   "metadata": {},
   "outputs": [],
   "source": [
    "BMI_data['Severity']=BMI_data['BMI'] #copy BMI column to severity column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b447ab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "BMI_data=BMI_data.replace({\"Severity\": severity_dict}) #replace severity with the numbered version of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "225b2bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BMI_data=BMI_data.sort_values(by=['Severity'], ascending=False) #sort by severity of BMI\n",
    "BMI_data=BMI_data.reset_index(drop=True) #Reset indices\n",
    "BMI_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a163fdb",
   "metadata": {},
   "source": [
    "## Inconsistencies\n",
    "There are some participants for which the number of volumes is less than the number of nodules found. According to a radiologist this is because the volumes of ground-class nodules were not included in the downloads. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a7f6b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,11): #Create empty columns to be filled with volumes of nodules below\n",
    "    BMI_data['volume_all_n'+str(i)]=float(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dec80c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BMI_data.iloc[:,20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8cf516",
   "metadata": {},
   "source": [
    "Kept only solid part of both solid and subsolid nodules, only if >=30mm3. These should also be changed in Excel file, as mentioned below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4ebf1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find cases in which we have both 'solid' and 'subsolid' volumes for same nodule keep only solid components if vol>30mm3\n",
    "#Subsolid in redcap will be solid + the subsolid part. So only subsolid will be subsolid in redcap- solid in redcap.\n",
    "\n",
    "vol_both_comps=[] #To be filled with participants who have both solid and subsolid component\n",
    "\n",
    "for j in range(len(BMI_data)):\n",
    "    \n",
    "    for nod_num in range(1,11):\n",
    "\n",
    "        # if BMI_data.loc[j,'participant_id'] in (low_pats+high_pats): #If participant in our experiment - Should define these above in the notebook\n",
    "\n",
    "            solid=BMI_data.loc[j,'volume_solid_n'+str(nod_num)]\n",
    "            subsolid=BMI_data.loc[j,'volume_subsolid_n'+str(nod_num)]\n",
    "            \n",
    "            if np.isnan(solid)==True and np.isnan(subsolid)==False: #If only subsolid component\n",
    "                print(\"Only subsolid component for nod id\",nod_num,'of participant',BMI_data.loc[j,'participant_id'],\"Volume set to 0.0\")\n",
    "                BMI_data.loc[j,'volume_all_n'+str(nod_num)]=0.0 #These are taken into account in other file\n",
    "\n",
    "            elif np.isnan(solid)==False and np.isnan(subsolid)==True: #If only solid component\n",
    "                BMI_data.loc[j,'volume_all_n'+str(nod_num)]=solid\n",
    "                if solid<30:\n",
    "                     print(\"We should not have in REDCap a solid component <30mm3! Check again nod id\",nod_num,'of participant',BMI_data.loc[j,'participant_id'])\n",
    "                \n",
    "            elif np.isnan(solid)==False and np.isnan(subsolid)==False: #If both solid and subsolid components\n",
    "\n",
    "                # vol_both_comps.append(BMI_data.loc[j,'participant_id_only'])\n",
    "        \n",
    "                if solid>=30: #If solid component >30mm3\n",
    "                    BMI_data.loc[j,'volume_all_n'+str(nod_num)]=solid\n",
    "                else:\n",
    "                    print(\"Both solid and subsolid components but solid <=30. Didn't keep any of them for nod id\",nod_num,'of participant',BMI_data.loc[j,'participant_id'])\n",
    "                \n",
    "            else: \n",
    "                BMI_data.loc[j,'volume_all_n'+str(nod_num)]=np.nan #We only get in here if there is no nodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e1f5245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Show these lists of participants with volumes for solid and subsolid components of the same nodule\n",
    "# list(np.unique(vol_both_comps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf27bc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Show volumes for cases with solid and subsolid components\n",
    "# check_cols=[col for col in BMI_data.columns if 'volume' in col]\n",
    "# BMI_data.loc[BMI_data['participant_id'].isin(vol_both_comps)][check_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e7fa64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BMI_data = BMI_data[~BMI_data.participant_id.isin(vol_both_comps[1:])] #Remove these participants\n",
    "BMI_data=BMI_data.reset_index(drop=True)\n",
    "BMI_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40b1cef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high_redcap\n",
      "176\n",
      "low_redcap\n",
      "176\n"
     ]
    }
   ],
   "source": [
    "for name in severity_dict.keys(): #loop over names of ΒΜΙ degrees and print num of participants of each degree now\n",
    "    print(name)\n",
    "    #Transform each dataframe to have each participant 1 time\n",
    "    exec(name+\"=BMI_data[BMI_data['BMI']==name]\") \n",
    "    \n",
    "    print(len(BMI_data[BMI_data['BMI']==name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "182a2fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high_redcap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c30f0340",
   "metadata": {},
   "source": [
    "#### Get information from REDCap for participants in the ΒΜΙ experiment - Use it below to get TP information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb160e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BMI_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31e7eb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of individuals with low BMI is 176\n",
      "Number of individuals with high BMI is 176\n"
     ]
    }
   ],
   "source": [
    "#Patient IDs of individuals with low and high BMI\n",
    "low_pats=[....]\n",
    "\n",
    "high_pats=[....]\n",
    "        \n",
    "print(\"Number of individuals with low BMI is {}\".format(len(low_pats)))\n",
    "print(\"Number of individuals with high BMI is {}\".format(len(high_pats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99348b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pats=high_pats+low_pats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "751774b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP_export=BMI_data[BMI_data['participant_id_only'].isin(all_pats)]\n",
    "TP_export"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2be53e4",
   "metadata": {},
   "source": [
    "### Get indices of TP for each participant - Use manually checked annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd99c563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1fae1b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace errors of automation algorithm extraction\n",
    "high=high.replace(\"!!!\",'',regex=True) #Replace double exclamation marks \n",
    "low=low.replace(\"!!!\",'',regex=True) #Replace double exclamation marks \n",
    "\n",
    "high=high.replace(\"xxx\",'',regex=True) #Replace double exclamation marks \n",
    "low=low.replace(\"xxx\",'',regex=True) #Replace double exclamation marks \n",
    "\n",
    "low=low.reset_index(drop=True) #Reset indices\n",
    "high=high.reset_index(drop=True) #Reset indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3412912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace 0 with NaN - Output of algorithm is 0 but need nan below\n",
    "high['0-100fn'].replace(0, np.nan, inplace=True)\n",
    "high['100-300fn'].replace(0, np.nan, inplace=True)\n",
    "high['300+ fn'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "low['0-100fn'].replace(0, np.nan, inplace=True)\n",
    "low['100-300fn'].replace(0, np.nan, inplace=True)\n",
    "low['300+ fn'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "\n",
    "high['0-100tp'].replace(0, np.nan, inplace=True)\n",
    "high['100-300tp'].replace(0, np.nan, inplace=True)\n",
    "high['300+ tp'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "low['0-100tp'].replace(0, np.nan, inplace=True)\n",
    "low['100-300tp'].replace(0, np.nan, inplace=True)\n",
    "low['300+ tp'].replace(0, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "286f8af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507fe5d0",
   "metadata": {},
   "source": [
    "Cell below should be commented. It changes the number of TPs so that they are not the same as in the excel files by not considering TP with nodules measured by AI being <30mm3 (example 528787, 922165). The issue of not considering as FPs AI findings with vol <30mm3 is addressed elsewhere.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1de51d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #This is to prevent considering vols <30 \n",
    "# vol_cols=[col for col in low.columns if 'V' in col] #Get name of columns containing volumes of AI nodules\n",
    "\n",
    "# BMI_deg=['low_fp','high_fp']\n",
    "\n",
    "# for deg in BMI_deg: #Loop over emphysema degrees\n",
    "#     for col in vol_cols: #Loop over columns with volumes\n",
    "#         #If the volume is less than 30mm3 we should ignore them - set it along with the corresponding AI nod to '-'\n",
    "#         for ind,val in eval(deg[:-3]+\"[(\"+deg[:-3]+\"['\"+col+\"']<=30) ]['\"+col+\"'].items()\"):\n",
    "#              #| (\"+deg[:-3]+\"['\"+col+\"']>300) - Not added since we might have a TP with vol>300 from AI which is <300 in REDCap\n",
    "#             exec(deg[:-3]+\"['\"+col+\"'].iloc[ind]=np.nan\") #was '-' instead of nan\n",
    "#             exec(deg[:-3]+\"['AI_nod\"+str(col[1:])+\"'].iloc[ind]=np.nan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a78a9a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select rows where we have at least one TP in any of the 0-100, 100-300, or 300mm3 volume subgroup\n",
    "high_tp=high[(high['100-300tp'].notnull() | high['0-100tp'].notnull() | high['300+ tp'].notnull()) & high['participant_id'].notnull()]\n",
    "low_tp=low[(low['100-300tp'].notnull() | low['0-100tp'].notnull() | low['300+ tp'].notnull()) & low['participant_id'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6247de39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# low_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d26e890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize empty dicts in the form {'pat_id1':[],'pat_id2':[],...}\n",
    "high_dict=dict.fromkeys([str(numeric_string) for numeric_string in high_tp['participant_id'].values], [])\n",
    "high_dict=[[key[:6],[]] for (key, value) in high_dict.items()] #Initialize list in the form [participant_id,[]]\n",
    "high_dict = {item[0]: item[1] for item in high_dict} #convert to dictionary in the form {pat_1:[nod_1,...],...}\n",
    "high_tp['participant_id']=list(high_dict.keys())\n",
    "\n",
    "low_dict=dict.fromkeys([str(numeric_string) for numeric_string in low_tp['participant_id'].values], [])\n",
    "low_dict=[[key[:6],[]] for (key, value) in low_dict.items()] #Initialize list in the form [participant_id,[]]\n",
    "low_dict = {item[0]: item[1] for item in low_dict} #convert to dictionary in the form {pat_1:[nod_1,...],...}\n",
    "low_tp['participant_id']=list(low_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5899174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_cols=[col for col in high_tp.columns if 'AI_nod' in col] #Get name of columns containing AI nodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87bd3d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75bf9ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high_tp\n",
      "low_tp\n"
     ]
    }
   ],
   "source": [
    "BMI_deg=['high_tp','low_tp'] #Initialize list with possible BMI degrees. \n",
    "#Could also be without the suffix '_tp' and then just use 'deg' instead of 'deg[:-3]' below - For now stays as is\n",
    "\n",
    "for deg in BMI_deg: #Loop over BMI degrees\n",
    "    print(deg)\n",
    "    for ind_col,col in enumerate(AI_cols): #loop over AI columns with nodules and their ids\n",
    "        \n",
    "        #Following line to change nan with '-' since otherwise cannot check for string with 'L' below\n",
    "        exec(deg[:-3]+\"_tp['\"+col+\"']=\"+deg[:-3]+\"_tp['\"+col+\"'].fillna('-')\")\n",
    "\n",
    "        exec(deg[:-3]+\"_tp['\"+str(col)+\"'] = \"+deg[:-3]+\"_tp['\"+str(col)+\"'].astype(str)\") #Type as string\n",
    "        \n",
    "        #Create a variable storing only those rows of df that a specific AI_nod col contains 'L' (denotes a TP)\n",
    "        exec('temp='+deg[:-3]+'_tp['+deg[:-3]+\"_tp['\"+str(col)+\"'].str.contains('L')]\")\n",
    "        \n",
    "        if not temp.empty: #If we have TP for that participant\n",
    "\n",
    "            for ind,pat in enumerate(temp['participant_id']): #Loop over all participants with TP in a specific AI col\n",
    "\n",
    "                try: #To ensure that there are no errors\n",
    "                    nod_id=temp.iloc[ind,ind_col+1][temp.iloc[ind,ind_col+1].find('L')+1:] #Get id\n",
    "                    nod_id=nod_id.split(' ')[0] #To get actual id\n",
    "                    exec(deg[:-3]+'_dict'+\"['\"+str(pat)+\"'].append('\"+nod_id+\"')\") #Add that to the dictionary\n",
    "                except:\n",
    "                    print(traceback.print_exc()) #print errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ae7426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39f58535",
   "metadata": {},
   "outputs": [],
   "source": [
    "for deg in BMI_deg: #Loop over BMI degrees\n",
    "    for key,val in eval(deg[:-3]+'_dict.items()'): #Loop over all participants and keys\n",
    "        #Replace keys like '04' with just the integer 4\n",
    "        exec(deg[:-3]+\"_dict['\"+key+\"']=[int(x) for x in \"+deg[:-3]+\"_dict['\"+key+\"']]\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2502fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcdc2839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# low_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "212cd74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of TPs findings in high BMI: 67 Those come from 46 participants\n",
      "Number of TPs findings in low BMI: 87 Those come from 59 participants\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of TPs findings in high BMI:\",len([y for x in high_dict.values() for y in x]),\"Those come from\",len(high_dict),\"participants\")\n",
    "print(\"Number of TPs findings in low BMI:\",len([y for x in low_dict.values() for y in x]),\"Those come from\",len(low_dict),\"participants\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0d0cded",
   "metadata": {},
   "source": [
    "#### Extract TP information for the ids of TP nodules for each participant found above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f1fbb6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_nod_ids=[col for col in TP_export.columns if 'nodule_id_n' in col] #Columns with nodule ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f64ac20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep only the participants of interest - Those in the low/high BMI lists\n",
    "high_TP_fin=TP_export[TP_export['participant_id_only'].isin(high_pats)]\n",
    "low_TP_fin=TP_export[TP_export['participant_id_only'].isin(low_pats)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "149c1333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high_TP_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "701c6c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert participant ids to integers\n",
    "high_dict= {int(k):v for k,v in high_dict.items()} \n",
    "low_dict= {int(k):v for k,v in low_dict.items()} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "686d5d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep in each df only those rows (participants) that have at least one TP nodule\n",
    "high_TP_fin=high_TP_fin.loc[high_TP_fin['participant_id_only'].isin(list(high_dict.keys()))]\n",
    "low_TP_fin=low_TP_fin.loc[low_TP_fin['participant_id_only'].isin(list(low_dict.keys()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98e800ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high_TP_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68e5c395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# low_TP_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "542c9a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Until now we have the ids in the Nodule ID attribute on REDCap. We have to convert them to numbers from 1-10 that\n",
    "#correspond to the REDCap attributes - initialize dict with {pat:empty} list pairs\n",
    "\n",
    "true_ids_high_dict={key:[] for key,val in high_dict.items()} \n",
    "true_ids_low_dict={key:[] for key,val in low_dict.items()} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "baac129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_true_ids(dictionary,df,cols_with_nod_ids,new_dictionary):\n",
    "    \n",
    "    #Get ids of where TP nodules stored in REDCap - from 1 to 10 - used to get information from respective columns below\n",
    "    for pat,nods in dictionary.items(): #Loop over dictionary key-values\n",
    "        true_nod_id=df.loc[df['participant_id_only'] == pat][cols_with_nod_ids].values[0].astype(float) #Get value of nodule \n",
    "        # 748658 nod id 1 is int and not float! This is why we added astype(float)\n",
    "\n",
    "        for nod in nods: #Loop over nodules\n",
    "            new_dictionary[pat].append(np.where(nod==true_nod_id)[0][0]+1) #Add true nod_id in dictionary\n",
    "            \n",
    "    return new_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "11b77d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply function to get nodule id in REDCap from nodule value stored in it\n",
    "true_ids_high_dict=convert_to_true_ids(high_dict,high_TP_fin,cols_with_nod_ids,true_ids_high_dict)\n",
    "true_ids_low_dict=convert_to_true_ids(low_dict,low_TP_fin,cols_with_nod_ids,true_ids_low_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0719bb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of TPs ids in high BMI: 67 Those come from 46 participants\n",
      "Number of TPs ids in low BMI: 87 Those come from 59 participants\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of TPs ids in high BMI:\",len([y for x in true_ids_high_dict.values() for y in x]),\"Those come from\",len(true_ids_high_dict),\"participants\")\n",
    "print(\"Number of TPs ids in low BMI:\",len([y for x in true_ids_low_dict.values() for y in x]),\"Those come from\",len(true_ids_low_dict),\"participants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37466e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# low_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ba4c590",
   "metadata": {},
   "source": [
    "## Add information from REDCap to lists"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c132232",
   "metadata": {},
   "source": [
    "#### Explanation of REDCap values\n",
    "- calcification - we keep 2-6 (calcified):\n",
    "\n",
    "1. No=>1 \n",
    "2. Fully calcified, centrally, popcorn, rum,other=>1-6\n",
    "\n",
    "- pfn (numbers represent value in REDCap) - we keep 3 (atypical PFN/triangular lymph node) - caution: 2,3 exist also for typical/fissural (if for attachment_n7 value is 1). In general, both typical and atypical PFNs (intrapulmonary lymph nodes) are always non-calcified (calcification=1), have sharp borders and oval, triangular, or polygonal (rectangle or dumdbell rarely) shape -any of shape_==2 or 3 or 4- (also round but could be a misclassification of cancer). Also, most of them have a vascular, or fissure or pleural attachment (any of attachments 2,3,7 is 1). \n",
    "Typical PFNs always have a fissure attached (attach_7=1) while atypical may or may not be fissure attached:\n",
    "\n",
    "1. No=>1\n",
    "2. Typical PFN=>2\n",
    "3. Atypical PFN/possible =>3\n",
    "4. Don't know =>4\n",
    "\n",
    "- attachment (if eg. attach_n1_2 is 0 means not pleural based) - we keep either n_2=1 (pleural) or n_4, n_5=1 (peribronchial/bronciovascular lymph node, if also central 'location', smooth 'edge', triangular or polygonal 'shape', and solid 'nodule_type'):\n",
    "\n",
    "1. 0mm, reaching to pleura\n",
    "2. pleural based \n",
    "3. juxtapleural\n",
    "4. peribronchial\n",
    "5. vascular attached\n",
    "6. juxtavascular\n",
    "7. fissural attached\n",
    "\n",
    "- nodule type - we keep 3 (subsolid/ground glass)\n",
    "1. solid\n",
    "2. partial solid\n",
    "3. pure ground glass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4501d3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create cols to extract nodule information from REDCap\n",
    "#For 'attachment_n' since only possible values are 0 or 1, we could also work with lists instead of dictionaries.\n",
    "#More convenient dictionaries since we already have the pipeline for the rest\n",
    "\n",
    "def extract_information(df):\n",
    "    calcified_nodules=[col for col in df.columns if 'calcification_n' in col]\n",
    "    pleural_nodules=[col for col in df.columns if 'attachment_n' in col and '___2' in col]\n",
    "    subclass_ground_glass_nodules=[col for col in df.columns if 'nodule_type_n' in col] #value should be 3\n",
    "    #for 'other' nodules no information - should check for each one of those what are their attributes - same for 'cancer'\n",
    "\n",
    "    atypical_periphysural_fissural_pfn_triangular_lymph_nodes=[col for col in df.columns if 'pfn_n' in col] \n",
    "    attachment_to_distinguish_atypical_typical=[col for col in df.columns if 'attachment_n' in col and ('___7' in col)]\n",
    "    \n",
    "    peribronchial_bronchiovasc_ln=[col for col in df.columns if 'attachment_n' in col and ('___4' in col)]\n",
    "    peribronchial_bronchiovasc_ln_2=[col for col in df.columns if 'attachment_n' in col and ('___5' in col)]\n",
    "    \n",
    "    location=[col for col in df.columns if 'loc_n' in col]\n",
    "    shape=[col for col in df.columns if 'shape_n' in col]\n",
    "    edge=[col for col in df.columns if 'edge_n' in col]\n",
    "        \n",
    "    #non-nodules do not exist (fibrosis/scar/pleural thickening and other (bone, tissue etc.))\n",
    "\n",
    "    return (calcified_nodules, pleural_nodules, subclass_ground_glass_nodules,\n",
    "            atypical_periphysural_fissural_pfn_triangular_lymph_nodes,attachment_to_distinguish_atypical_typical,\n",
    "            peribronchial_bronchiovasc_ln,peribronchial_bronchiovasc_ln_2,location,shape,edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b2d4cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract columns with nodules of interest for all dfs - same for low/high BMI categories\n",
    "(calcified_nodules, pleural_nodules, subclass_ground_glass_nodules,\n",
    " atypical_periphysural_fissural_pfn_triangular_lymph_nodes,attachment_to_distinguish_atypical_typical,\n",
    " peribronchial_bronchiovasc_ln,peribronchial_bronchiovasc_ln_2,location,shape,edge)=extract_information(TP_export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "978d1de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the above to a single list\n",
    "all_cols=(calcified_nodules+pleural_nodules+subclass_ground_glass_nodules+\n",
    "          atypical_periphysural_fissural_pfn_triangular_lymph_nodes+attachment_to_distinguish_atypical_typical+\n",
    "          peribronchial_bronchiovasc_ln+peribronchial_bronchiovasc_ln_2+location+shape+edge)\n",
    "#We get 60 columns in total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7976f448",
   "metadata": {},
   "source": [
    "Keep track of nodule id and slice where it can be found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9fd73d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary with dictionaries in format: {pat:{nod_num:value},...}\n",
    "\n",
    "def add_pat(dictionary,pat,nod_num,slices,vol_dict,volume): \n",
    "    \n",
    "    if pat in dictionary: #If participant in dictionary add/replace one nod_id value to it. Same for volume\n",
    "        dictionary[pat][nod_num]=slices\n",
    "        vol_dict[pat][nod_num]=volume\n",
    "        \n",
    "    else: #If not, then initialize the value of that participant to an empty dictionary and fill in its values\n",
    "        dictionary[pat]={}\n",
    "        dictionary[pat][nod_num]=slices\n",
    "        \n",
    "        vol_dict[pat]={} #Same for dictionary with participant_ids and nod volumes\n",
    "        vol_dict[pat][nod_num]=volume\n",
    "    \n",
    "    return dictionary,vol_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e356ae",
   "metadata": {},
   "source": [
    "Peribronchial lymph nodes are included since only 'peribronchial tissue' should be excluded which shouldn't exist in REDCap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2bff6708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TP_info_redcap(tp_or_fn='tp'):\n",
    "    \"\"\"Function to extract description of TP or FN nodules from REDCap.\"\"\"\n",
    "\n",
    "    # Make below variables accessible outside function without using 'return' and setting new variables:\n",
    "    global calcified, calcified_vol, pleural, pleural_vol, sub_ground, sub_ground_vol, atypical_triangular, atypical_triangular_vol, per_fisu, per_fisu_vol, \\\n",
    "            peri_bronch, peri_bronch_vol, other_all, other_all_low, other_all_high, other_all_vol, atypical_triangular_low, per_fisu_low, peri_bronch_low, \\\n",
    "            calcified_low, pleural_low, sub_ground_low, atypical_triangular_high, per_fisu_high, peri_bronch_high, calcified_high, pleural_high, sub_ground_high\n",
    "\n",
    "    #Initialize empty lists for nodule categories - also empty lists for their volumes to be filled in\n",
    "\n",
    "    #For this and for the below one maybe also 'shape'=>3,4 means more benign. Not used for now\n",
    "    atypical_triangular={} #'pfn_n'=>3 caution: 2,3 exist also for typical/fissural (if for attachement_n7 value is 1)\n",
    "    atypical_triangular_vol={}\n",
    "    per_fisu={} #'pfn_n'=>2,3 and attachment_n7 value is 1. Look above for more\n",
    "    per_fisu_vol={}\n",
    "    peri_bronch={} #'attachment'=> n4,n5=1\n",
    "    peri_bronch_vol={}\n",
    "\n",
    "    calcified={} #'calcification' =>2-6\n",
    "    calcified_vol={}\n",
    "    pleural={}#'attachment'=> n_2=1\n",
    "    pleural_vol={}\n",
    "    sub_ground={} #'nodule_type' =>3\n",
    "    sub_ground_vol={}\n",
    "\n",
    "    #For other type of nodules not in the above categories\n",
    "    other_all={}\n",
    "    other_all_low={}\n",
    "    other_all_high={}\n",
    "    other_all_vol={}\n",
    "\n",
    "    #Non-nodule categories not exist in REDCap\n",
    "\n",
    "    #For low/high BMI table - Not sure if needed given that we can obtain those from original dict combined with \n",
    "    #list of participants that belong to each degree, found above\n",
    "\n",
    "    #Low BMI\n",
    "    atypical_triangular_low={}\n",
    "    per_fisu_low={}\n",
    "    peri_bronch_low={}\n",
    "    calcified_low={}\n",
    "    pleural_low={}\n",
    "    sub_ground_low={}\n",
    "\n",
    "    #High BMI\n",
    "    atypical_triangular_high={}\n",
    "    per_fisu_high={}\n",
    "    peri_bronch_high={}\n",
    "    calcified_high={}\n",
    "    pleural_high={}\n",
    "    sub_ground_high={}\n",
    "\n",
    "\n",
    "    if tp_or_fn=='tp':\n",
    "        type_extract='true_ids_'\n",
    "        attr='_TP_fin'\n",
    "    else:\n",
    "        type_extract='false_ids_'\n",
    "        attr='_df_fn'\n",
    "\n",
    "\n",
    "    #Implement above definition of each type of finding and store each variable in the corresponding dictionary\n",
    "\n",
    "    for deg in BMI_deg: #Loop over low/high BMI degrees (in the form 'low_tp')\n",
    "        \n",
    "        #Loop over participant ids and actual nod_ids of TPs in REDCap - from 1 to 10\n",
    "        for pat,nods in eval(type_extract+deg[:-3]+'_dict.items()'): \n",
    "\n",
    "            for nod_num in nods: #Loop over TP nodule ids\n",
    "                \n",
    "                flag=0 #Initialize a flag to help us distinguish atypical from fissural PFN - when both have 'pfn' value=3 \n",
    "                flag_any=0 #Flag to check if we got in any of the below categories\n",
    "                \n",
    "                #Get columns with volume for a specific nodule id - need two statements to distinguish 'n1' from 'n10'\n",
    "                if nod_num!=1:\n",
    "                    nod_cols=[col for col in all_cols if '_n'+str(nod_num) in col]\n",
    "                    vol_cols=[col for col in TP_export.columns if 'volume_all_n'+str(nod_num) in col]\n",
    "                    slice_cols=[col for col in TP_export.columns if 'slice_number_n'+str(nod_num) in col]\n",
    "                else:\n",
    "                    nod_cols=[col for col in all_cols if '_n1' in col and '_n10' not in col]\n",
    "                    vol_cols=[col for col in TP_export.columns if 'volume_all_n'+str(nod_num) in col and '_n10' not in col]\n",
    "                    slice_cols=[col for col in TP_export.columns if 'slice_number_n'+str(nod_num) in col and '_n10' not in col]\n",
    "\n",
    "                #  #For debug\n",
    "    #             if pat==490144:\n",
    "    #                 print(pat,nod_num)\n",
    "    #                 print(eval(deg[:-3]+\"_TP_fin[\"+deg[:-3]+\"_TP_fin['participant_id']==\"+str(pat)+\"][nod_cols]\"))\n",
    "                    \n",
    "                #Get all values and volumes for a specific nodule id. Order is as defined above:\n",
    "                #calcified,pleural,subclass_ground_glass,atypical_periphysural_fissural,\n",
    "                #attachment_to_distinguish_atypical_typical,peri_bronch_ln,peri_bronch_ln_2,location,shape,edge\n",
    "                all_values=eval(deg[:-3]+attr+\"[\"+deg[:-3]+attr+\"['participant_id']==\"+str(pat)+\"][nod_cols].values[0]\")\n",
    "                all_volumes=eval(deg[:-3]+attr+\"[\"+deg[:-3]+attr+\"['participant_id']==\"+str(pat)+\"][vol_cols].values[0]\")\n",
    "                all_slices=eval(deg[:-3]+attr+\"[\"+deg[:-3]+attr+\"['participant_id']==\"+str(pat)+\"][slice_cols].values[0]\")\n",
    "\n",
    "                \n",
    "                if np.isnan(all_values[0]): #'calcification'\n",
    "                    print('For ', pat, 'we have nan calcified in nod number (in REDCap)',nod_num)\n",
    "                elif all_values[0]!=1: #Could be 2-6 which signifies calcified nodule\n",
    "                    flag_any=1\n",
    "                    add_pat(calcified,pat,nod_num,all_slices[0],calcified_vol,all_volumes[0]) #Add to dict with all patients\n",
    "                    if 'high' in deg: #Add it to dict with only nonemphysema participants\n",
    "                        add_pat(calcified_high,pat,nod_num,all_slices[0],calcified_vol,all_volumes[0])\n",
    "                    else: #Add it to dict with only emphysema participants\n",
    "                        add_pat(calcified_low,pat,nod_num,all_slices[0],calcified_vol,all_volumes[0])\n",
    "                \n",
    "\n",
    "                if np.isnan(all_values[2]): #For subsolid/ground glass nodules the 'nodule_type' must be 3 (ground glass) or 2 (partial solid)\n",
    "                    print('For ', pat, 'we have nan nodule type in nod number (in REDCap)',nod_num)\n",
    "                elif all_values[2]==3 or all_values[2]==2:\n",
    "                    flag_any=1\n",
    "                    add_pat(sub_ground,pat,nod_num,all_slices[0],sub_ground_vol,all_volumes[0])\n",
    "                    if 'high' in deg:\n",
    "                        add_pat(sub_ground_high,pat,nod_num,all_slices[0],sub_ground_vol,all_volumes[0])\n",
    "                    else:\n",
    "                        add_pat(sub_ground_low,pat,nod_num,all_slices[0],sub_ground_vol,all_volumes[0])\n",
    "                    \n",
    "                    \n",
    "                if np.isnan(all_values[3]): #For atypical we want 'pfn' to be 3 or 4 and 'attachment_7' to be 1\n",
    "                    print('for ', pat, 'we have nan for pfn_n in nod number (in REDCap)',nod_num)\n",
    "\n",
    "                elif all_values[3]==3: #If 'pfn' is 3 then 'attachment_7' has to be checked\n",
    "                #- For 'pfn'=4 (don't know) we agreed to consider them as 'No'\n",
    "                    \n",
    "                    flag=1 #That means that we check if it's atypical\n",
    "                    flag_any=1\n",
    "                    \n",
    "                    if ((all_values[8]==3 or all_values[8]==4 or all_values[8]==2 or all_values[8]==1) or all_values[9]==1 #shape triangular, polygonal or oval (or round)\n",
    "                        and all_values[0]==1): #non-calcified\n",
    "                    \n",
    "                        if all_values[4]==0 and np.isnan(all_values[4])==False: #If 'attachment_7' is 0 then atypical\n",
    "                            add_pat(atypical_triangular,pat,nod_num,all_slices[0],atypical_triangular_vol,all_volumes[0])\n",
    "                            if 'high' in deg:\n",
    "                                add_pat(atypical_triangular_high,pat,nod_num,all_slices[0],atypical_triangular_vol,all_volumes[0])\n",
    "                            else:\n",
    "                                add_pat(atypical_triangular_low,pat,nod_num,all_slices[0],atypical_triangular_vol,all_volumes[0])\n",
    "\n",
    "                    \n",
    "                    else:\n",
    "                        print('atypical PFN error - either shape or calcification not match even if attachment correct for pat',pat)\n",
    "                        if all_values[4]==0:# and np.isnan(all_values[4])==False: #If 'attachment_7' is 0 then atypical\n",
    "                            add_pat(atypical_triangular,pat,nod_num,all_slices[0],atypical_triangular_vol,all_volumes[0])\n",
    "                            if 'high' in deg:\n",
    "                                add_pat(atypical_triangular_high,pat,nod_num,all_slices[0],atypical_triangular_vol,all_volumes[0])\n",
    "                            else:\n",
    "                                add_pat(atypical_triangular_low,pat,nod_num,all_slices[0],atypical_triangular_vol,all_volumes[0])\n",
    "                        else:\n",
    "                            print(\"Added as atypical although attributes do not match (attach_7=1) for pat\",pat)\n",
    "                            add_pat(atypical_triangular,pat,nod_num,all_slices[0],atypical_triangular_vol,all_volumes[0])\n",
    "                            if 'high' in deg:\n",
    "                                add_pat(atypical_triangular_high,pat,nod_num,all_slices[0],atypical_triangular_vol,all_volumes[0])\n",
    "                            else:\n",
    "                                add_pat(atypical_triangular_low,pat,nod_num,all_slices[0],atypical_triangular_vol,all_volumes[0])\n",
    "                            \n",
    "                #if 'pfn' is 2 or 3 (typical PFN- possible/atypical pfn) and attachment_7=1 (fissural) then peri/fis\n",
    "                if all_values[3]==2 or all_values[3]==3: \n",
    "                    flag_any=1\n",
    "                    if ((all_values[8]==3 or all_values[8]==4 or all_values[8]==2 or all_values[8]==1) #shape triangular, polygonal or oval (or round)\n",
    "                        and all_values[0]==1): #non-calcified\n",
    "                        \n",
    "                        if all_values[4]==1: #If attachment_7 is 1 we have typical PFNs\n",
    "                            add_pat(per_fisu,pat,nod_num,all_slices[0],per_fisu_vol,all_volumes[0])   \n",
    "                            if 'high' in deg:\n",
    "                                add_pat(per_fisu_high,pat,nod_num,all_slices[0],per_fisu_vol,all_volumes[0])\n",
    "                            else:\n",
    "                                add_pat(per_fisu_low,pat,nod_num,all_slices[0],per_fisu_vol,all_volumes[0])\n",
    "                                \n",
    "                        elif all_values[4]==0 and all_values[3]==2: #If 'attachment_7'=0 and 'pfn'=2\n",
    "                            print('pat:',pat,'nod_id:',nod_num,'. Error! We expected attachment_7 to be 1 given that pfn=2 (typical PFN) but it is 0! Need to check manually')\n",
    "                            print(\"For now added in PFN group (periphysural)\")\n",
    "\n",
    "                            add_pat(per_fisu,pat,nod_num,all_slices[0],per_fisu_vol,all_volumes[0])   \n",
    "                            if 'high' in deg:\n",
    "                                add_pat(per_fisu_high,pat,nod_num,all_slices[0],per_fisu_vol,all_volumes[0])\n",
    "                            else:\n",
    "                                add_pat(per_fisu_low,pat,nod_num,all_slices[0],per_fisu_vol,all_volumes[0])\n",
    "                            \n",
    "                        elif flag==1: #If we also check for 'atypical' then don't do anything since all scenarios already considered\n",
    "                            pass\n",
    "                        else:\n",
    "                            print('We should never be in here for pat',pat,'with nod id',nod_num,'and values',all_values,'need manual check')\n",
    "                            \n",
    "                    else:\n",
    "                        if all_values[4]==1 and all_values[0]==1: #If attachment_7 is 1 we have typical PFNs and calcified is no\n",
    "                            add_pat(per_fisu,pat,nod_num,all_slices[0],per_fisu_vol,all_volumes[0])   \n",
    "                            if 'high' in deg:\n",
    "                                add_pat(per_fisu_high,pat,nod_num,all_slices[0],per_fisu_vol,all_volumes[0])\n",
    "                            else:\n",
    "                                add_pat(per_fisu_low,pat,nod_num,all_slices[0],per_fisu_vol,all_volumes[0])\n",
    "                                \n",
    "                        elif all_values[4]==0 and all_values[3]==2 and all_values[0]==1: #If 'attachment_7'=0 and 'pfn'=2 and calcified is no\n",
    "                            print('pat:',pat,'nod_id:',nod_num,'. Error! We expected attachment_7 to be 1 given that pfn=2 (typical PFN) but it is 0! Need to check manually')\n",
    "                            print(\"For now added in PFN group (periphysural)\")\n",
    "\n",
    "                            add_pat(per_fisu,pat,nod_num,all_slices[0],per_fisu_vol,all_volumes[0])   \n",
    "                            if 'high' in deg:\n",
    "                                add_pat(per_fisu_high,pat,nod_num,all_slices[0],per_fisu_vol,all_volumes[0])\n",
    "                            else:\n",
    "                                add_pat(per_fisu_low,pat,nod_num,all_slices[0],per_fisu_vol,all_volumes[0])\n",
    "                            \n",
    "                        elif flag==1: #If we also check for 'atypical' then don't do anything since all scenarios already considered\n",
    "                            print(pat,nod_num,\"Classified as typical or atypical PFN but should be checked manually - There are attribute errors!\")\n",
    "                            if all_values[3]==2:\n",
    "                                print(\"Classified as typical PFN\")\n",
    "                                add_pat(per_fisu,pat,nod_num,all_slices[0],per_fisu_vol,all_volumes[0])   \n",
    "                                if 'high' in deg:\n",
    "                                    add_pat(per_fisu_high,pat,nod_num,all_slices[0],per_fisu_vol,all_volumes[0])\n",
    "                                else:\n",
    "                                    add_pat(per_fisu_low,pat,nod_num,all_slices[0],per_fisu_vol,all_volumes[0])\n",
    "                            else: #only possible value =3 here\n",
    "                                    print(\"Classified as atypical PFN\")\n",
    "                                    add_pat(atypical_triangular,pat,nod_num,all_slices[0],atypical_triangular_vol,all_volumes[0])\n",
    "                                    if 'high' in deg:\n",
    "                                        add_pat(atypical_triangular_high,pat,nod_num,all_slices[0],atypical_triangular_vol,all_volumes[0])\n",
    "                                    else:\n",
    "                                        add_pat(atypical_triangular_low,pat,nod_num,all_slices[0],atypical_triangular_vol,all_volumes[0])\n",
    "                            print(\"\\n\")\n",
    "                        else:\n",
    "                            print('2.We should never be in here for pat',pat,'with nod id',nod_num,'and values',all_values,'need manual check. Added in PFN group (periphysural)')\n",
    "                            add_pat(per_fisu,pat,nod_num,all_slices[0],per_fisu_vol,all_volumes[0])   \n",
    "                            if 'high' in deg:\n",
    "                                add_pat(per_fisu_high,pat,nod_num,all_slices[0],per_fisu_vol,all_volumes[0])\n",
    "                            else:\n",
    "                                add_pat(per_fisu_low,pat,nod_num,all_slices[0],per_fisu_vol,all_volumes[0])\n",
    "                \n",
    "                if np.isnan(all_values[1]): #For pleural nodules 'attachment_2' has to be 1\n",
    "                    print('for ', pat, 'we have nan in attachment__2 in nod number (in REDCap)',nod_num)\n",
    "                elif all_values[1]==1:\n",
    "                    flag_any=1\n",
    "                    add_pat(pleural,pat,nod_num,all_slices[0],pleural_vol,all_volumes[0])\n",
    "                    if 'high' in deg:\n",
    "                        add_pat(pleural_high,pat,nod_num,all_slices[0],pleural_vol,all_volumes[0])\n",
    "                    else:\n",
    "                        add_pat(pleural_low,pat,nod_num,all_slices[0],pleural_vol,all_volumes[0])\n",
    "                    \n",
    "                #For perinbronchial/bronchiovascular lymph nodes we want 'nodule_type' to be 1 and 'location' central,\n",
    "                #and 'edge' smooth, and 'shape' triangular or polygonal\n",
    "                if (all_values[7]==1 and (all_values[8]==3 or all_values[8]==4) and all_values[9]==1 and \n",
    "                    all_values[2]==1):\n",
    "                                \n",
    "                    #Also 'attachment_4' or 'attachment_5' has to be 1\n",
    "                    if np.isnan(all_values[5]): \n",
    "                        print('for ', pat, 'we have nan in attachment!__4 in nod number (in REDCap)',nod_num)\n",
    "                    elif all_values[6]==1 or all_values[5]==1:\n",
    "                        add_pat(peri_bronch,pat,nod_num,all_slices[0],peri_bronch_vol,all_volumes[0])\n",
    "                        if 'high' in deg:\n",
    "                            add_pat(peri_bronch_high,pat,nod_num,all_slices[0],peri_bronch_vol,all_volumes[0])\n",
    "                        else:\n",
    "                            add_pat(peri_bronch_low,pat,nod_num,all_slices[0],peri_bronch_vol,all_volumes[0])\n",
    "\n",
    "                    # elif all_values[5]==1:\n",
    "                    #     print(\"Peribronchial finding in participant\",pat,\"with nod_num is\",nod_num, '(degree is',deg,'). Excluded in our analysis')\n",
    "\n",
    "                    else: #for cases in which all attachments are 0 - probably errors\n",
    "                        if flag_any==0 :\n",
    "                            print(\"Attachments empty for pat\",pat,\"with nod id\",nod_num)\n",
    "                            #if 'pfn' is 'No' or 'I don't know and not in any other nodule category, then add it to 'other nodule'\n",
    "                            if all_values[3]==1 or all_values[3]==4: \n",
    "                                print(\"We consider as nodule the id\",nod_num,\"of participant\",pat)\n",
    "                                add_pat(other_all,pat,nod_num,all_slices[0],other_all_vol,all_volumes[0])\n",
    "                                if 'high' in deg:\n",
    "                                    add_pat(other_all_high,pat,nod_num,all_slices[0],other_all_vol,all_volumes[0])\n",
    "                                else:\n",
    "                                    add_pat(other_all_low,pat,nod_num,all_slices[0],other_all_vol,all_volumes[0])\n",
    "                            else:#If is pfn and attachments empty then add to typical/atypical\n",
    "                                if all_values[3]==2:\n",
    "                                    add_pat(per_fisu,pat,nod_num,all_slices[0],per_fisu_vol,all_volumes[0])   \n",
    "                                    if 'high' in deg:\n",
    "                                        add_pat(per_fisu_high,pat,nod_num,all_slices[0],per_fisu_vol,all_volumes[0])\n",
    "                                    else:\n",
    "                                        add_pat(per_fisu_low,pat,nod_num,all_slices[0],per_fisu_vol,all_volumes[0])\n",
    "                                else: #only possible value =3 here\n",
    "                                        add_pat(atypical_triangular,pat,nod_num,all_slices[0],atypical_triangular_vol,all_volumes[0])\n",
    "                                        if 'high' in deg:\n",
    "                                            add_pat(atypical_triangular_high,pat,nod_num,all_slices[0],atypical_triangular_vol,all_volumes[0])\n",
    "                                        else:\n",
    "                                            add_pat(atypical_triangular_low,pat,nod_num,all_slices[0],atypical_triangular_vol,all_volumes[0])\n",
    "                            print(\"\\n\")\n",
    "                    flag_any=1\n",
    "                \n",
    "                #Check for errors - Cases where we didn't get in any of the conditions above - new categories\n",
    "                if (all_values[0]==1 and all_values[2]!=3 and all_values[3]!=2 and all_values[3]!=3  and #and all_values[3]!=4\n",
    "                    all_values[1]==0 and all_values[2]!=2 and\n",
    "                    all_values[7]!=1 and (all_values[8]!=3 and all_values[8]!=4) and all_values[9]!=1 and all_values[2]!=1):\n",
    "                    \n",
    "                        print(\"None of the above categories for participant\",pat,\"with nod id\",nod_num,\"and with values\",all_values)\n",
    "                        print('flag_any is',flag_any)\n",
    "                        print('flag is',flag)\n",
    "                        \n",
    "                flag=0 #Set value to 0 before looping to next nodule     \n",
    "                \n",
    "                if flag_any==0:\n",
    "                \n",
    "                    #if 'pfn' is 'No' or 'I don't know and not in any other nodule category, then add it to 'other nodule'\n",
    "                    if all_values[3]==1 or all_values[3]==4:              \n",
    "                        add_pat(other_all,pat,nod_num,all_slices[0],other_all_vol,all_volumes[0])\n",
    "                        if 'high' in deg:\n",
    "                            add_pat(other_all_high,pat,nod_num,all_slices[0],other_all_vol,all_volumes[0])\n",
    "                        else:\n",
    "                            add_pat(other_all_low,pat,nod_num,all_slices[0],other_all_vol,all_volumes[0])\n",
    "                    \n",
    "                flag_any=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a24965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP_info_redcap(tp_or_fn='tp')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "557508bc",
   "metadata": {},
   "source": [
    "Those with errors (like '2.We should never be in here for pat ....') should be checked manually!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aef3a7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high_dict #True nodule ids added in REDCap can be found in these dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c90cc268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define nodule group names\n",
    "nod_groups=['sub_ground','atypical_triangular','per_fisu','pleural', 'peri_bronch','calcified','other_all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2cff705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #For debugging\n",
    "# #Print which groups have nodules from particular patient\n",
    "# for group_nam in nod_groups:\n",
    "#     try:\n",
    "#         print('group',group_nam,'is',eval(group_nam+'[845334]'))\n",
    "#     except:         \n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a6fb022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors_check(tp_or_fn='tp'):\n",
    "    \"Function to correct errors in REDCap data entry\"\n",
    "    \n",
    "    if tp_or_fn=='tp':\n",
    "        type_extract='true_ids_'\n",
    "    else:\n",
    "        type_extract='false_ids_'\n",
    "\n",
    "    #Check for participants that were not added in any of the categories above, or for particular nodules that were not added\n",
    "\n",
    "    degrees=['high','low'] #List of degree names\n",
    "\n",
    "    for deg in degrees: #Loop over each degree\n",
    "        print(\"Checking nodules of\",deg,'degree....')\n",
    "        \n",
    "        for pat_id in eval(type_extract+deg+'_dict'): #Loop over participant with TP nodules of participants of that degree \n",
    "            \n",
    "            nod_ids=eval(type_extract+deg+'_dict'+'[pat_id]') #Get all possible nodules ids for that participant (from 1-10)\n",
    "            BMI_found=[] #Initialize list to empty to be filled with nodule ids that were not added in any of the lists above\n",
    "            \n",
    "            for group in nod_groups: #Loop over each of the nodule groups\n",
    "                \n",
    "                BMI_group=eval(group) #Get a variable with that group's dictionary\n",
    "\n",
    "                if pat_id in BMI_group: #If participant in that dictionary\n",
    "                    nod_BMI=BMI_group[pat_id] #Get nodule dictionary of that participant\n",
    "                    for nod_found in nod_BMI: #Loop over nodule ids of that participant\n",
    "                        BMI_found.append(nod_found) #Add that nodule id in the above list\n",
    "\n",
    "            missed=list(set(nod_ids) - set(BMI_found)) #Get which nodules not added in the above list (not in any category)\n",
    "\n",
    "\n",
    "            if len(BMI_found)==len(nod_ids): #All nodule included -those are the correct ones, no need for further check\n",
    "                pass\n",
    "\n",
    "            #For those below more manual checks are needed\n",
    "            else:\n",
    "                flag=0\n",
    "                \n",
    "                if len(BMI_found)==0: #All nodules for that participant belong to a different category\n",
    "                    print(pat_id,'all nodules',missed,\"not added in any category\")\n",
    "                    flag=1\n",
    "\n",
    "                if len(BMI_found)!=len(np.unique(BMI_found)): #Nodules missed and some that belong to multiple categories\n",
    "                    print(pat_id,\"has\",missed,\"nodules not added in any category, and nodules with multiple attributes\",list(set([i for i in BMI_found if BMI_found.count(i)>1])))\n",
    "                    flag=1\n",
    "                    check_elements=list(set([i for i in BMI_found if BMI_found.count(i)>1]))\n",
    "                    \n",
    "                if flag==0: #Some nodules of a participant that don't belong to any of the above categories\n",
    "                    print(pat_id,'nodules',missed,\"weren't added in any category\")\n",
    "                flag=0\n",
    "\n",
    "            if ((len(missed)==0 or len(BMI_found)!=len(np.unique(BMI_found))) and len(BMI_found)!=len(nod_ids) and len(BMI_found)!=0):\n",
    "                nod_ids=eval(type_extract+deg+'_dict'+'[pat_id]') #Get all possible nodules ids for that participant (from 1-10)\n",
    "                BMI_found=[] #Initialize list to empty to be filled with nodule ids that were not added in any of the lists above\n",
    "                print(\"Deletion starts.......\")\n",
    "\n",
    "                for group in nod_groups[:-1]: #Loop over each of the nodule groups\n",
    "\n",
    "                    BMI_group=eval(group) #Get a variable with that group's dictionary\n",
    "\n",
    "                    if pat_id in BMI_group: #Loop over participants in that dictionary\n",
    "                        nod_BMI=BMI_group[pat_id] #Get nodule dictionary of that participant\n",
    "                        should_del=[] #list of keys that should be deleted\n",
    "                        \n",
    "                        for key,nod_found in nod_BMI.items(): #Loop over nodules of that participant\n",
    "                            \n",
    "                                if key in check_elements:\n",
    "                                    BMI_found.append(key) #Add that nodule in the above list\n",
    "                                    BMI_found.append(group)\n",
    "\n",
    "                                    try: #Get volume and add to corresponding dictionary\n",
    "                                        vol=eval(group+'_vol'+\"[\"+str(pat_id)+\"][\"+str(key)+\"]\")\n",
    "                                        all_slices=eval(group+\"[\"+str(pat_id)+\"][\"+str(key)+\"]\")\n",
    "\n",
    "                                        add_pat(other_all,pat_id,key,all_slices,other_all_vol,vol)\n",
    "                                        if 'high' in deg:\n",
    "                                            add_pat(other_all_high,pat_id,key,all_slices,other_all_vol,vol)\n",
    "                                        else:\n",
    "                                            add_pat(other_all_low,pat_id,key,all_slices,other_all_vol,vol)\n",
    "                                            \n",
    "                                        should_del.append(key)\n",
    "                                        \n",
    "                                    except:\n",
    "                                        print('Error - sth went wrong')\n",
    "                                        pass\n",
    "                        \n",
    "                        for ind_del in should_del: #Delete keys from corresponding dictionaries along with their volumes\n",
    "                            \n",
    "                            try:\n",
    "                                exec(\"del \"+group+\"[pat_id][ind_del]\")\n",
    "                                exec(\"del \"+group+\"_vol[pat_id][ind_del]\")\n",
    "                                print('Deleted nodule of participant',pat_id,' with id',ind_del,'from group',group)\n",
    "                                \n",
    "                                try:\n",
    "                                    exec(\"del \"+group+\"_high[pat_id][ind_del]\")\n",
    "                                    print('Deleted nodule from high group with id',ind_del,'from group',group)\n",
    "                                except:\n",
    "                                    exec(\"del \"+group+\"_low[pat_id][ind_del]\")\n",
    "                                    print('Deleted nodule of pat',pat_id,' from low group with id',ind_del,'from group',group)\n",
    "                                \n",
    "                            except:\n",
    "                                print('Deletion error')\n",
    "                                print(traceback.print_exc())\n",
    "                                pass\n",
    "            \n",
    "                print(\"Here are multiple attributes for\",pat_id,\":\", BMI_found)\n",
    "                print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e21e1ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_check(tp_or_fn='tp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4788f46",
   "metadata": {},
   "source": [
    "Atypical lymph nodes will be considered in the nodule group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "96fdf960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define nodule group names\n",
    "nod_groups_only=['sub_ground','pleural', 'calcified','other_all','atypical_triangular'] \n",
    "lymph_groups=['per_fisu','peri_bronch'] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb7a68e4",
   "metadata": {},
   "source": [
    "#### Get actual number of TP for nodules and for lymph nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "630c8583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking findings of _low degree....\n",
      "sub_ground_low 1\n",
      "sub_ground_low30_100 1\n",
      "sub_ground_low100-300 0\n",
      "sub_ground_low300+ 0\n",
      "\n",
      "\n",
      "pleural_low 12\n",
      "pleural_low30_100 8\n",
      "pleural_low100-300 2\n",
      "pleural_low300+ 2\n",
      "\n",
      "\n",
      "calcified_low 9\n",
      "calcified_low30_100 9\n",
      "calcified_low100-300 0\n",
      "calcified_low300+ 0\n",
      "\n",
      "\n",
      "other_all_low 37\n",
      "other_all_low30_100 26\n",
      "other_all_low100-300 9\n",
      "other_all_low300+ 2\n",
      "\n",
      "\n",
      "atypical_triangular_low 5\n",
      "atypical_triangular_low30_100 5\n",
      "atypical_triangular_low100-300 0\n",
      "atypical_triangular_low300+ 0\n",
      "\n",
      "\n",
      "per_fisu_low 22\n",
      "per_fisu_low30_100 17\n",
      "per_fisu_low100-300 5\n",
      "per_fisu_low300+ 0\n",
      "\n",
      "\n",
      "peri_bronch_low 1\n",
      "peri_bronch_low30_100 0\n",
      "peri_bronch_low100-300 0\n",
      "peri_bronch_low300+ 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Checking findings of _high degree....\n",
      "sub_ground_high 1\n",
      "sub_ground_high30_100 0\n",
      "sub_ground_high100-300 0\n",
      "sub_ground_high300+ 1\n",
      "\n",
      "\n",
      "pleural_high 8\n",
      "pleural_high30_100 6\n",
      "pleural_high100-300 2\n",
      "pleural_high300+ 0\n",
      "\n",
      "\n",
      "calcified_high 6\n",
      "calcified_high30_100 6\n",
      "calcified_high100-300 0\n",
      "calcified_high300+ 0\n",
      "\n",
      "\n",
      "other_all_high 36\n",
      "other_all_high30_100 26\n",
      "other_all_high100-300 7\n",
      "other_all_high300+ 3\n",
      "\n",
      "\n",
      "atypical_triangular_high 4\n",
      "atypical_triangular_high30_100 2\n",
      "atypical_triangular_high100-300 2\n",
      "atypical_triangular_high300+ 0\n",
      "\n",
      "\n",
      "per_fisu_high 11\n",
      "per_fisu_high30_100 9\n",
      "per_fisu_high100-300 2\n",
      "per_fisu_high300+ 0\n",
      "\n",
      "\n",
      "peri_bronch_high 1\n",
      "peri_bronch_high30_100 1\n",
      "peri_bronch_high100-300 0\n",
      "peri_bronch_high300+ 0\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for BMI_level in ['_low','_high']: #Loop over low/high BMI degrees\n",
    "    print(\"Checking findings of\",BMI_level,'degree....')\n",
    "    \n",
    "    for nod_group in nod_groups_only: #Loop over nodule groups\n",
    "        \n",
    "        #Get total num of values in that group\n",
    "        exec(nod_group+BMI_level+'_nod_only='+'sum([len(x) for x in '+nod_group+BMI_level+'.values()])') \n",
    "        print(nod_group+BMI_level,eval(nod_group+BMI_level+'_nod_only'))\n",
    "        \n",
    "        #Get volumes for each group\n",
    "        exec(nod_group+BMI_level+'_nod_only_30_100=0')\n",
    "        exec(nod_group+BMI_level+'_nod_only_100_300=0')\n",
    "        exec(nod_group+BMI_level+'_nod_only_300=0')\n",
    "        \n",
    "        for pat in eval(nod_group+BMI_level+'.keys()'): #Loop over participants of that group\n",
    "            exec(nod_group+BMI_level+'_nod_only_30_100+='+'len([1 for val in '+nod_group+'_vol[pat].values() if val>=30 and val<100])')\n",
    "            exec(nod_group+BMI_level+'_nod_only_100_300+='+'len([1 for val in '+nod_group+'_vol[pat].values() if val>=100 and val<=300])')\n",
    "            exec(nod_group+BMI_level+'_nod_only_300+='+'len([1 for val in '+nod_group+'_vol[pat].values() if val>300])')\n",
    "            \n",
    "        print(nod_group+BMI_level+'30_100',eval(nod_group+BMI_level+'_nod_only_30_100'))\n",
    "        print(nod_group+BMI_level+'100-300',eval(nod_group+BMI_level+'_nod_only_100_300'))\n",
    "        print(nod_group+BMI_level+'300+',eval(nod_group+BMI_level+'_nod_only_300'))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        #Save them to be used in the other file to create tables with these numbers           \n",
    "        with open(nod_group+BMI_level+'_nod_only'+'.pickle','wb') as f:\n",
    "            pickle.dump(eval(nod_group+BMI_level+'_nod_only'),f)          \n",
    "            \n",
    "        with open(nod_group+BMI_level+'_nod_only_30_100'+'.pickle','wb') as f:\n",
    "            pickle.dump(eval(nod_group+BMI_level+'_nod_only_30_100'),f)\n",
    "        with open(nod_group+BMI_level+'_nod_only_100_300'+'.pickle','wb') as f:\n",
    "            pickle.dump(eval(nod_group+BMI_level+'_nod_only_100_300'),f)\n",
    "        with open(nod_group+BMI_level+'_nod_only_300'+'.pickle','wb') as f:\n",
    "            pickle.dump(eval(nod_group+BMI_level+'_nod_only_300'),f)\n",
    "     \n",
    "    #Same as above for lymph nodes        \n",
    "    for lymph_group in lymph_groups:\n",
    "        \n",
    "        exec(lymph_group+BMI_level+'_lymph='+'sum([len(x) for x in '+lymph_group+BMI_level+'.values()])')\n",
    "        print(lymph_group+BMI_level,eval(lymph_group+BMI_level+'_lymph'))\n",
    "        \n",
    "        #Get volumes for each group\n",
    "        exec(lymph_group+BMI_level+'_lymph_30_100=0')\n",
    "        exec(lymph_group+BMI_level+'_lymph_100_300=0')\n",
    "        exec(lymph_group+BMI_level+'_lymph_300=0')\n",
    "        \n",
    "        for pat in eval(lymph_group+BMI_level+'.keys()'):\n",
    "            exec(lymph_group+BMI_level+'_lymph_30_100+='+'len([1 for val in '+lymph_group+'_vol[pat].values() if val>=30 and val<100])')\n",
    "            exec(lymph_group+BMI_level+'_lymph_100_300+='+'len([1 for val in '+lymph_group+'_vol[pat].values() if val>=100 and val<=300])')\n",
    "            exec(lymph_group+BMI_level+'_lymph_300+='+'len([1 for val in '+lymph_group+'_vol[pat].values() if val>300])')\n",
    "     \n",
    "        print(lymph_group+BMI_level+'30_100',eval(lymph_group+BMI_level+'_lymph_30_100'))\n",
    "        print(lymph_group+BMI_level+'100-300',eval(lymph_group+BMI_level+'_lymph_100_300'))\n",
    "        print(lymph_group+BMI_level+'300+',eval(lymph_group+BMI_level+'_lymph_300'))\n",
    "        print(\"\\n\")\n",
    "\n",
    "        with open(lymph_group+BMI_level+'_lymph'+'.pickle','wb') as f:\n",
    "            pickle.dump(eval(lymph_group+BMI_level+'_lymph'),f)\n",
    "    \n",
    "        with open(lymph_group+BMI_level+'_lymph_30_100'+'.pickle','wb') as f:\n",
    "            pickle.dump(eval(lymph_group+BMI_level+'_lymph_30_100'),f)\n",
    "        with open(lymph_group+BMI_level+'_lymph_100_300'+'.pickle','wb') as f:\n",
    "            pickle.dump(eval(lymph_group+BMI_level+'_lymph_100_300'),f)\n",
    "        with open(lymph_group+BMI_level+'_lymph_300'+'.pickle','wb') as f:\n",
    "            pickle.dump(eval(lymph_group+BMI_level+'_lymph_300'),f)\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5c240a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High BMI total findings: 67\n",
      "Low BMI total findings: 87\n",
      "High BMI number of participants: 46\n",
      "Low BMI number of participants: 59\n"
     ]
    }
   ],
   "source": [
    "#Print total num of values of TPs in each group\n",
    "print(\"High BMI total findings:\",sum([len(x) for x in high_dict.values()]))\n",
    "print(\"Low BMI total findings:\",sum([len(x) for x in low_dict.values()]))\n",
    "\n",
    "print(\"High BMI number of participants:\",(len(high_dict.keys())))\n",
    "print(\"Low BMI number of participants:\",(len(low_dict.keys())))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "799c35bc",
   "metadata": {},
   "source": [
    "#### Get ids of FN to automatically perform analysis - Similar as above for TPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2aff0f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select rows where we have at least one FN in any of the 0-100, 100-300 or 300+mm3 volume subgroup\n",
    "high_fn=high[(high['100-300fn'].notnull() | high['0-100fn'].notnull() | high['300+ fn'].notnull()) & high['participant_id'].notnull()] \n",
    "low_fn=low[(low['100-300fn'].notnull() | low['0-100fn'].notnull() | low['300+ fn'].notnull()) & low['participant_id'].notnull()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bf01bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c102764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get lists of participants with at least one FN\n",
    "high_pats_fn=[int(str(x)[:6]) for x in high_fn['participant_id'].values]\n",
    "low_pats_fn=[int(str(x)[:6]) for x in low_fn['participant_id'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e72a7f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get only rows from REDCap dataframe with participants that have at least on FN\n",
    "high_df_fn=TP_export[TP_export['participant_id'].isin(high_pats_fn)]\n",
    "low_df_fn=TP_export[TP_export['participant_id'].isin(low_pats_fn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3020d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high_df_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8738f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# low_df_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b2b42ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize empty dicts in the form {'pat_id1':[],'pat_id2':[],...} mod_df_fn['participant_id']\n",
    "\n",
    "high_dict_fn=dict.fromkeys([int(str(numeric_string)[:6]) for numeric_string in high_df_fn['participant_id'].values], [])\n",
    "low_dict_fn=dict.fromkeys([int(str(numeric_string)[:6]) for numeric_string in low_df_fn['participant_id'].values], [])\n",
    "\n",
    "#Initialize same dict for storing volumes of each FN nodule\n",
    "high_dict_fn_vol=high_dict_fn.copy()\n",
    "low_dict_fn_vol=low_dict_fn.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f8e900dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for deg in BMI_deg: #Loop over low/high BMI degrees (in the form 'low_tp')\n",
    "\n",
    "    for ind,pat in enumerate(eval(deg[:-3]+\"_df_fn['participant_id']\")): #Loop over all indices and participants of a given df\n",
    "        if pat in eval(deg[:-3]+\"_dict.keys()\"): #If the participant is also in dict with some nodule ids being TP\n",
    "            \n",
    "            for nod_id,val in enumerate(eval(deg[:-3]+\"_df_fn[cols_with_nod_ids].iloc[ind].values\")): #Loop over all ids\n",
    "\n",
    "                if nod_id!=0: #Avoid first column in which we might have both 'n1' and 'n10'\n",
    "                    vol_cols=[col for col in TP_export.columns if 'volume_all_n'+str(nod_id+1) in col]\n",
    "                else:\n",
    "                    vol_cols=[col for col in TP_export.columns if 'volume_all_n'+str(nod_id+1) in col and '_n10' not in col]        \n",
    "\n",
    "                #If a specific nodule id is not in the dict with TP add it to dict with FN. Same for its volume\n",
    "                #First condition will result in error in pat not in that dict - That's why we need the 'else' condition below\n",
    "                if val not in eval(deg[:-3]+\"_dict[pat]\") and np.isnan(val)==False: \n",
    "                    exec(deg[:-3]+\"_dict_fn[pat]=\"+deg[:-3]+\"_dict_fn[pat]+[val]\")\n",
    "                    exec(deg[:-3]+\"_dict_fn_vol[pat]=\"+deg[:-3]+\"_dict_fn_vol[pat]+[\"+deg[:-3]+\"_df_fn[vol_cols].iloc[ind].values[0]]\")\n",
    "                        \n",
    "        else: #If this is not in dictionary with at least one TP then all are FNs\n",
    "\n",
    "            for nod_id,val in enumerate(eval(deg[:-3]+\"_df_fn[cols_with_nod_ids].iloc[ind].values\")):\n",
    "\n",
    "                if nod_id!=0: #Avoid first column in which we might have both 'n1' and 'n10'\n",
    "                    vol_cols=[col for col in TP_export.columns if 'volume_all_n'+str(nod_id+1) in col]\n",
    "                else:\n",
    "                    vol_cols=[col for col in TP_export.columns if 'volume_all_n'+str(nod_id+1) in col and '_n10' not in col]        \n",
    "\n",
    "                if np.isnan(val)==False:\n",
    "                    exec(deg[:-3]+\"_dict_fn[pat]=\"+deg[:-3]+\"_dict_fn[pat]+[val]\")\n",
    "                    exec(deg[:-3]+\"_dict_fn_vol[pat]=\"+deg[:-3]+\"_dict_fn_vol[pat]+[\"+deg[:-3]+\"_df_fn[vol_cols].iloc[ind].values[0]]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c12b0b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_ids_high_dict={key:[] for key,val in high_dict_fn.items()} \n",
    "false_ids_low_dict={key:[] for key,val in low_dict_fn.items()} \n",
    "\n",
    "#Apply function to get nodule id in REDCap from nodule value stored in it\n",
    "false_ids_high_dict=convert_to_true_ids(high_dict_fn,high_df_fn,cols_with_nod_ids,false_ids_high_dict)\n",
    "false_ids_low_dict=convert_to_true_ids(low_dict_fn,low_df_fn,cols_with_nod_ids,false_ids_low_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "188b96e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# low_dict_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42fa6074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# false_ids_low_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high_dict_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false_ids_high_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6a82754b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants with High BMI FN is 28\n",
      "Number of participants with Low BMI FN is 38\n",
      "Number of findings in High BMI FN is 42\n",
      "Number of findings in Low BMI FN is 55\n"
     ]
    }
   ],
   "source": [
    "#Print total num of values of FNs in each group\n",
    "print('Number of participants with High BMI FN is',len(high_dict_fn))\n",
    "print('Number of participants with Low BMI FN is',len(low_dict_fn))\n",
    "\n",
    "print(\"Number of findings in High BMI FN is\",len([y for x in high_dict_fn.values() for y in x])) #Same as sum([len(x) for x in high_dict_fn.values()])\n",
    "print(\"Number of findings in Low BMI FN is\",len([y for x in low_dict_fn.values() for y in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In High BMI there are in total 60 participants with nodules (either TP or FN)\n",
      "In Low BMI there are in total 78 participants with nodules (either TP or FN)\n"
     ]
    }
   ],
   "source": [
    "print(\"In High BMI there are in total\", len(np.unique(list(high_dict_fn.keys())+list(high_dict.keys()))), 'participants with nodules (either TP or FN)')\n",
    "print(\"In Low BMI there are in total\", len(np.unique(list(low_dict_fn.keys())+list(low_dict.keys()))), 'participants with nodules (either TP or FN)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ff5b6f7",
   "metadata": {},
   "source": [
    "#### Get IDs and slices of FN per type - Extract type of findings for those from REDCap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03697b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# false_ids_low_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f393533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# atypical_triangular_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74b75510",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP_info_redcap(tp_or_fn='fn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b354e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# atypical_triangular_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78ad0f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7ac3b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_check(tp_or_fn='fn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ce14fd",
   "metadata": {},
   "source": [
    "Findings 'weren't added in any category' could be due to being peribronchial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a1edf902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking findings of _low degree....\n",
      "sub_ground_low 0.0\n",
      "pleural_low 3\n",
      "calcified_low 1\n",
      "other_all_low 29\n",
      "atypical_triangular_low 1\n",
      "per_fisu_low 13\n",
      "peri_bronch_low 8\n",
      "Total _low 55.0\n",
      "\n",
      "\n",
      "Checking findings of _high degree....\n",
      "sub_ground_high 0.0\n",
      "pleural_high 2\n",
      "calcified_high 1\n",
      "other_all_high 19\n",
      "atypical_triangular_high 2\n",
      "per_fisu_high 11\n",
      "peri_bronch_high 7\n",
      "Total _high 42.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Count total num of FN in those lists\n",
    "for bmi in ['_low','_high']: #Loop over low/high BMI degrees\n",
    "\n",
    "    print(\"Checking findings of\",bmi,'degree....')\n",
    "    \n",
    "    exec('total'+bmi+'=0') #Count\n",
    "\n",
    "    for nod_group in nod_groups_only: #Loop over nodule groups\n",
    "        total_vals=np.sum([len(x) for x in eval(nod_group+bmi+'.values()')])\n",
    "        exec('total'+bmi+'+=total_vals') #Add to total\n",
    "        print(nod_group+bmi,total_vals)\n",
    "    for lymph_group in lymph_groups:\n",
    "        total_vals=np.sum([len(x) for x in eval(lymph_group+bmi+'.values()')])\n",
    "        print(lymph_group+bmi,total_vals) #Add to total\n",
    "        exec('total'+bmi+'+=total_vals')\n",
    "\n",
    "    print(\"Total\",bmi,eval('total'+bmi))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8946ef71",
   "metadata": {},
   "source": [
    "Above are those detected only by reader - could be nodules, lymphs or non-nodules based on consensus - should be manually checked below"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88354512",
   "metadata": {},
   "source": [
    "#### Compare initial reading vs consensus for FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2e8f3899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking findings of low degree....\n",
      "Checking findings of high degree....\n"
     ]
    }
   ],
   "source": [
    "#Get all FNs in a list\n",
    "for bmi in ['low','high']: #Loop over low/high BMI degrees\n",
    "\n",
    "    print(\"Checking findings of\",bmi,'degree....')\n",
    "    \n",
    "    exec(bmi+\"_FN_all={}\") \n",
    "    exec(bmi+\"_FN_all_vols={}\") \n",
    "\n",
    "    for pat in eval(\"false_ids_\"+bmi+\"_dict\"+'.keys()'):\n",
    "\n",
    "        exec(bmi+\"_FN_all\"+\"[\"+str(pat)+\"]={}\") #Initialize empty dict for each participant\n",
    "        exec(bmi+\"_FN_all_vols\"+\"[\"+str(pat)+\"]={}\") #Initialize empty dict for each participant\n",
    "\n",
    "        for fn_id in eval(\"false_ids_\"+bmi+\"_dict\"+'['+str(pat)+']'):\n",
    "\n",
    "            slice_val=TP_export[TP_export['participant_id']==pat]['slice_number_n'+str(fn_id)].values[0]\n",
    "            exec(bmi+\"_FN_all[\"+str(pat)+\"][fn_id]=slice_val\") #Add slice number of each FN nodule\n",
    "\n",
    "            vol_val=TP_export[TP_export['participant_id']==pat]['volume_solid_n'+str(fn_id)].values[0]\n",
    "            if np.isnan(vol_val):\n",
    "                vol_val=TP_export[TP_export['participant_id']==pat]['volume_subsolid_n'+str(fn_id)].values[0]\n",
    "                # if np.isnan(vol_val):\n",
    "                #     print(\"NaN volume for\",pat,fn_id)\n",
    "\n",
    "            exec(bmi+\"_FN_all_vols[\"+str(pat)+\"][fn_id]=vol_val\") #Add slice number of each FN nodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8e55be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# low_FN_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4acfdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# low_FN_all_vols"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1271ee57",
   "metadata": {},
   "source": [
    "Extract information from consensus review - For now should compare manually the two dictionaries below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c73e156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low/High BMI\n",
    "\n",
    "for deg in ['low','high']:\n",
    "    exec(deg+\"_consensus=copy.deepcopy(\"+deg+'_FN_all)') #Create a copy of the above dictionary\n",
    "\n",
    "    for key,val in eval(deg+\"_consensus.items()\"): \n",
    "        for id,slice_num in val.items():\n",
    "            exec(deg+\"_consensus[key][id]={slice_num:{}}\") #Add slice number as key for each id\n",
    "\n",
    "\n",
    "    for folder in os.listdir(eval(\"path_\"+deg)): #Loop over folders in which we have txt files with consensus review\n",
    "        if os.path.isdir(eval('path_'+deg)+'/'+folder):\n",
    "\n",
    "            folder_name=folder[:6]\n",
    "\n",
    "            for file in os.listdir(eval('path_'+deg)+'/'+folder):\n",
    "                if 'fn' in file and 'txt' in file:\n",
    "                    with open (eval('path_'+deg)+'/'+folder+'/'+file,'r') as f:\n",
    "                        lines=f.readlines() #Read lines of txt file\n",
    "\n",
    "\n",
    "                    pat_fn=eval(deg+'_FN_all[int(folder_name)]') #Get dictionary with slice numbers of FNs for that participant\n",
    "\n",
    "                    for id,slice_num in pat_fn.items(): #Loop over slice numbers of FNs for that participant\n",
    "                        if str(int(slice_num)) in file: #If slice number of FN is in file name\n",
    "\n",
    "                            for pat,vals in eval(deg+'_FN_all.items()'): #Loop over all participants with FNs\n",
    "                                if pat==int(folder_name): #If participant is the same as the one in file name\n",
    "                                    for id_num in vals.keys(): #Loop over all ids of that participant\n",
    "                                        \n",
    "                                        if int(eval(deg+'_FN_all[int(folder_name)][id_num]==int(slice_num)')): \n",
    "                                            exec(deg+'_consensus[int(folder_name)][id_num][slice_num]=lines')\n",
    "                                            #Add lines of txt file as value for that slice number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a6d1fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high_FN_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "639e6d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# low_consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "27dc000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# low_FN_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7dd7da67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# low_FN_all_vols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "15a7c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high_consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "16443bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high_FN_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2ee7c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high_FN_all_vols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4e78dc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to print in color\n",
    "def prRed(skk): print(\"\\033[91m {}\\033[00m\".format(skk))\n",
    "#https://www.geeksforgeeks.org/print-colors-python-terminal/\n",
    "\n",
    "#Possible colors are:\n",
    "# red = '\\033[91m'\n",
    "# green = '\\033[92m'\n",
    "# yellow = '\\033[93m'\n",
    "# blue = '\\033[94m'\n",
    "# pink = '\\033[95m'\n",
    "# teal = '\\033[96m'\n",
    "# grey = '\\033[97m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "39b0d72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#low/high BMI final comparison of consensus vs REDCap\n",
    "for bmi in ['low','high']: #Loop over emphysema degrees\n",
    "\n",
    "    show_deg=\"Degree is \"+bmi+'.....'\n",
    "    prRed(show_deg) #Print in red\n",
    "    all_count=0\n",
    "\n",
    "    for pat in eval(bmi+'_consensus'): #Loop over participants with FNs in consensus review\n",
    "        for id in eval(bmi+'_consensus[pat]'): #Loop over nodule ids of that participant\n",
    "            \n",
    "            for slice_num in eval(bmi+'_consensus['+str(pat)+']['+str(id)+']'): #Loop over slice numbers of that nodule\n",
    "\n",
    "                #Count total num of FN in those lists\n",
    "                for type_bmi in ['_low','_high']: #Loop over emphysema/non-emphysema degrees\n",
    "                    for nod_group in nod_groups_only: #this is based on redcap attibutes and not on consensus review\n",
    "                        try: #If slice number is in that dictionary\n",
    "                            if slice_num in eval(nod_group+type_bmi+'[pat].values()'): \n",
    "                                for check_id,check_val in eval(nod_group+type_bmi+'[pat].items()'): #Loop over all ids of that participant\n",
    "                                    if slice_num==check_val: #If slice number is the same as the one in dictionary\n",
    "                                        \n",
    "                                        vol_val=eval(bmi+'_FN_all_vols[pat][id]')\n",
    "\n",
    "                                        print(\"Participant\",pat,'with bmi:',bmi,\"nodule id:\",id,\"slice number:\",slice_num,\"volume:\",vol_val)\n",
    "                                        print(\"REDCap:\",nod_group)\n",
    "                                        print(\"Consensus:\",(eval(bmi+'_consensus['+str(pat)+']['+str(id)+']['+str(slice_num)+']')))\n",
    "                                        all_count+=1\n",
    "                                        print(\"\\n\")\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    for lymph_group in lymph_groups: #this is based on redcap attibutes and not on consensus review\n",
    "                        try:\n",
    "                            if slice_num in eval(lymph_group+type_bmi+'[pat].values()'):\n",
    "                                for check_id,check_val in eval(lymph_group+type_bmi+'[pat].items()'):\n",
    "                                    if slice_num==check_val:\n",
    "\n",
    "                                        vol_val=eval(bmi+'_FN_all_vols[pat][id]')\n",
    "\n",
    "                                        print(\"Participant\",pat,'with bmi:',bmi,\"nodule id:\",id,\"slice number:\",slice_num,\"volume:\",vol_val)\n",
    "                                        print(\"REDCap:\",lymph_group)\n",
    "                                        print(\"Consensus:\",(eval(bmi+'_consensus['+str(pat)+']['+str(id)+']['+str(slice_num)+']')))\n",
    "                                        all_count+=1\n",
    "                                        print(\"\\n\")\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "    print(\"Total findings:\",all_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea1a5213",
   "metadata": {},
   "source": [
    "#### Get actual slice with those ids - To be used to match nodule slices from review sessions\n",
    "\n",
    "#### Cells below only load dictionaries created from first part of file 'reviewing_info_extract.ipynb'. This is why it is suggested to run that file instead of this, since there, this file is called and it works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "423b5d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_cols=[col for col in TP_export.columns if 'slice_number_n' in col] #Get columns with slice numbers\n",
    "vol_cols=[col for col in TP_export.columns if 'volume_all_n' in col] #Get columns with volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a271ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dictionaries that contain all nodules and lymph nodes\n",
    "with open('dict_FN_wrong_low.pickle', 'rb') as f:\n",
    "    dict_FN_wrong_low = pickle.load(f)\n",
    "\n",
    "with open('dict_FN_correct_low.pickle', 'rb') as f:\n",
    "    dict_FN_correct_low = pickle.load(f)\n",
    "\n",
    "with open('dict_FN_wrong_high.pickle', 'rb') as f:\n",
    "    dict_FN_wrong_high = pickle.load(f)\n",
    "    \n",
    "with open('dict_FN_correct_high.pickle', 'rb') as f:\n",
    "    dict_FN_correct_high = pickle.load(f)\n",
    "    \n",
    "#Similarly for dictionaries having only nodules or only lymph nodes\n",
    "with open('lymph_FN_correct_low.pickle','rb') as f:\n",
    "    lymph_FN_correct_low=pickle.load(f)    \n",
    "\n",
    "with open('lymph_FN_correct_high.pickle','rb') as f:\n",
    "    lymph_FN_correct_high=pickle.load(f) \n",
    "    \n",
    "with open('nod_FN_correct_low.pickle','rb') as f:\n",
    "    nod_FN_correct_low=pickle.load(f)    \n",
    "\n",
    "with open('nod_FN_correct_high.pickle','rb') as f:\n",
    "    nod_FN_correct_high=pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7954e8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_FN_wrong_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f87c4174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_FN_correct_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d30568c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get copies of the dictionaries above to match slices with their indices\n",
    "\n",
    "#Get list of strings with the names of the above loaded dictionaries\n",
    "all_FNs=['dict_FN_wrong_low','dict_FN_correct_low','dict_FN_wrong_high','dict_FN_correct_high',\n",
    "         'lymph_FN_correct_low','lymph_FN_correct_high','nod_FN_correct_low','nod_FN_correct_high']\n",
    "\n",
    "count_del=0 #count the number of elements to be deleted\n",
    "\n",
    "for FN_dict in all_FNs: #Loop over above dictionaries with FNs\n",
    "\n",
    "    #Get copies of dictionaries with ids, and volumes for those\n",
    "    exec(FN_dict+'_ids=copy.deepcopy('+FN_dict+')')\n",
    "    exec(FN_dict+'_ids={item: [] for item in '+FN_dict+'_ids}')\n",
    "    exec(FN_dict+'_vols=copy.deepcopy('+FN_dict+'_ids)') #Get empty lists to fill in volumes\n",
    "\n",
    "    exec(FN_dict+'_errors=[]') #Empty list to keep track of errors\n",
    "    \n",
    "    for key,slices in eval(FN_dict+'.items()'): #Loop over elements of above dictionaries\n",
    "\n",
    "        df_values=list(TP_export[slice_cols][TP_export['participant_id']==int(key)].values[0]) #Get values from id 1-10\n",
    "        df_volumes=list(TP_export[vol_cols][TP_export['participant_id']==int(key)].values[0]) #Get their volumes\n",
    "\n",
    "        for slice_num in slices: #Loop over the slices from the reviewing session\n",
    "            if df_values.count(slice_num)==1: #If this slice exists exactly one time\n",
    "                 for ind,val in enumerate(df_values): #Loop over the slice ids\n",
    "                        if val==slice_num: #When we fing this slice keep its index\n",
    "                            exec(FN_dict+'_ids[key].append(ind+1)') #Add it to dictionary\n",
    "                            exec(FN_dict+'_vols[key].append(df_volumes[ind])') #Same for volume\n",
    "\n",
    "            else: #These should be checked manually\n",
    "                print(\"Dictionary is\",FN_dict)\n",
    "                print(\"Slices are\",slices)\n",
    "                print(\"Num of occurences is\",df_values.count(slice_num))\n",
    "                print(\"In participant\",key,\"the slice\",slice_num,\"exists more than once - Should be checked manually\")\n",
    "                print(\"Values of the slices for that participant are:\",df_values)\n",
    "                \n",
    "                exec(FN_dict+'_errors.append(key)')\n",
    "\n",
    "    #Delete participants for which we have the same value more than once - probably wrong information added there\n",
    "    for par in eval(FN_dict+'_errors'):\n",
    "        \n",
    "        if 'dict' in FN_dict: \n",
    "            try: #Since it might not exist\n",
    "                count_del=count_del+len(eval(FN_dict+'_ids[par]'))\n",
    "            except:\n",
    "                print('empty participant:',par)\n",
    "            \n",
    "\n",
    "        try: #In case it does not exist\n",
    "            print(\"Going to delete dictionary\", FN_dict,\"with ids\",eval(FN_dict+'_ids[par]'),'of participant',par)\n",
    "\n",
    "            exec(FN_dict+'_ids.pop(par)')\n",
    "            exec(FN_dict+'_vols.pop(par)')\n",
    "        except:\n",
    "            print(\"Deletion Failed for\",par)\n",
    "        print('\\n')\n",
    "\n",
    "print(\"Total number of elements deleted from main dictionary is\",count_del)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee028943",
   "metadata": {},
   "source": [
    "The above should be added manually, if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1758d3",
   "metadata": {},
   "source": [
    "We changed atypical PFNs to nodule group - IDs should be checked to confirm that - Should be fine, added to corresponding group based on consensus panel review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a9a8989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add manually slices, ids, and volumes to corresponding dictionary for cases found above \n",
    "dict_FN_correct_low_vols['123097']=[54.0] \n",
    "lymph_FN_correct_low_vols['123097']=[54.0]\n",
    "\n",
    "dict_FN_correct_low_ids['123097']=[6]\n",
    "lymph_FN_correct_low_ids['123097']=[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are not used anymore - Manually modified excels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3845a7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save all dictionaries to be used in the other file to match slices with ids\n",
    "with open('dict_FN_wrong_low_ids.pickle','wb') as f:\n",
    "    pickle.dump(dict_FN_wrong_low_ids,f)\n",
    "\n",
    "with open('dict_FN_correct_low_ids.pickle','wb') as f:\n",
    "    pickle.dump(dict_FN_correct_low_ids,f)    \n",
    "\n",
    "with open('dict_FN_wrong_high_ids.pickle','wb') as f:\n",
    "    pickle.dump(dict_FN_wrong_high_ids,f)\n",
    "\n",
    "with open('dict_FN_correct_high_ids.pickle','wb') as f:\n",
    "    pickle.dump(dict_FN_correct_high_ids,f) \n",
    "    \n",
    "#Same for volumes of the above files\n",
    "with open('dict_FN_wrong_low_vols.pickle','wb') as f:\n",
    "    pickle.dump(dict_FN_wrong_low_vols,f)\n",
    "\n",
    "with open('dict_FN_correct_low_vols.pickle','wb') as f:\n",
    "    pickle.dump(dict_FN_correct_low_vols,f)    \n",
    "\n",
    "with open('dict_FN_wrong_high_vols.pickle','wb') as f:\n",
    "    pickle.dump(dict_FN_wrong_high_vols,f)\n",
    "\n",
    "with open('dict_FN_correct_high_vols.pickle','wb') as f:\n",
    "    pickle.dump(dict_FN_correct_high_vols,f) \n",
    "    \n",
    "#Same as above for lymph node ids and volumes\n",
    "with open('lymph_FN_correct_low_ids.pickle','wb') as f:\n",
    "    pickle.dump(lymph_FN_correct_low_ids,f)  \n",
    "\n",
    "with open('lymph_FN_correct_high_ids.pickle','wb') as f:\n",
    "    pickle.dump(lymph_FN_correct_high_ids,f)\n",
    "    \n",
    "with open('lymph_FN_correct_low_vols.pickle','wb') as f:\n",
    "    pickle.dump(lymph_FN_correct_low_vols,f) \n",
    "\n",
    "with open('lymph_FN_correct_high_vols.pickle','wb') as f:\n",
    "    pickle.dump(lymph_FN_correct_high_vols,f)\n",
    "    \n",
    "#Same for nodule dictionaries\n",
    "with open('nod_FN_correct_low_ids.pickle','wb') as f:\n",
    "    pickle.dump(nod_FN_correct_low_ids,f)  \n",
    "\n",
    "with open('nod_FN_correct_high_ids.pickle','wb') as f:\n",
    "    pickle.dump(nod_FN_correct_high_ids,f)\n",
    "\n",
    "with open('nod_FN_correct_low_vols.pickle','wb') as f:\n",
    "    pickle.dump(nod_FN_correct_low_vols,f) \n",
    "\n",
    "with open('nod_FN_correct_high_vols.pickle','wb') as f:\n",
    "    pickle.dump(nod_FN_correct_high_vols,f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6905d11",
   "metadata": {},
   "source": [
    "#### Analysis per number of nodules/sizes and Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 60 participants with at least one nodule in high BMI group\n",
      "There are 116 participants with no nodules in high BMI group\n",
      "There are 78 participants with at least one nodule in low BMI group\n",
      "There are 98 participants with no nodules in low BMI group\n"
     ]
    }
   ],
   "source": [
    "conditions = (high[['0-100tp', '100-300tp', '300+ tp', '0-100fn', '100-300fn', '300+ fn']] == 0) | high[['0-100tp', '100-300tp', '300+ tp', '0-100fn', '100-300fn', '300+ fn']].isna()\n",
    "high_nonnods= conditions.all(axis=1).astype(int)\n",
    "print(\"There are\",(high_nonnods == 0).sum()-1,'participants with at least one nodule in high BMI group')\n",
    "print(\"There are\",(high_nonnods == 1).sum(),\"participants with no nodules in high BMI group\") #-1 to remove last row with total nodules\n",
    "\n",
    "conditions = (low[['0-100tp', '100-300tp', '300+ tp', '0-100fn', '100-300fn', '300+ fn']] == 0) | low[['0-100tp', '100-300tp', '300+ tp', '0-100fn', '100-300fn', '300+ fn']].isna()\n",
    "low_nonnods= conditions.all(axis=1).astype(int)\n",
    "print(\"There are\",(low_nonnods == 0).sum()-1,'participants with at least one nodule in low BMI group')\n",
    "print(\"There are\",(low_nonnods == 1).sum(),\"participants with no nodules in low BMI group\") #-1 to remove last row with total nodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 109 nodules in high BMI group\n",
      "There are 87 nodules of size 30-100mm3 in high BMI group\n",
      "There are 18 nodules of size 100-300mm3 in high BMI group\n",
      "There are 4 nodules of size 300+mm3 in high BMI group\n",
      "There are 142 nodules in low BMI group\n",
      "There are 114 nodules of size 30-100mm3 in low BMI group\n",
      "There are 22 nodules of size 100-300mm3 in low BMI group\n",
      "There are 6 nodules of size 300+mm3 in low BMI group\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\",int(high.iloc[-1,:]['0-100tp']+high.iloc[-1,:]['100-300tp']+high.iloc[-1,:]['300+ tp']+high.iloc[-1,:]['0-100fn']+high.iloc[-1,:]['100-300fn']), 'nodules in high BMI group')\n",
    "#+high.iloc[-1,:]['300+ fn'] is nan\n",
    "print(\"There are\", int(np.sum(high[['0-100tp']].iloc[:-1])[0])+int(np.sum(high[['0-100fn']].iloc[:-1])[0]),'nodules of size 30-100mm3 in high BMI group')\n",
    "print(\"There are\", int(np.sum(high[['100-300tp']].iloc[:-1])[0])+int(np.sum(high[['100-300fn']].iloc[:-1])[0]),'nodules of size 100-300mm3 in high BMI group')\n",
    "print(\"There are\", int(np.sum(high[['300+ tp']].iloc[:-1])[0])+int(np.sum(high[['300+ fn']].iloc[:-1])[0]),'nodules of size 300+mm3 in high BMI group')\n",
    "\n",
    "print(\"There are\",int(low.iloc[-1,:]['0-100tp']+low.iloc[-1,:]['100-300tp']+low.iloc[-1,:]['300+ tp']+low.iloc[-1,:]['0-100fn']+low.iloc[-1,:]['100-300fn']+low.iloc[-1,:]['300+ fn']), 'nodules in low BMI group')\n",
    "print(\"There are\", int(np.sum(low[['0-100tp']].iloc[:-1])[0])+int(np.sum(low[['0-100fn']].iloc[:-1])[0]),'nodules of size 30-100mm3 in low BMI group')\n",
    "print(\"There are\", int(np.sum(low[['100-300tp']].iloc[:-1])[0])+int(np.sum(low[['100-300fn']].iloc[:-1])[0]),'nodules of size 100-300mm3 in low BMI group')\n",
    "print(\"There are\", int(np.sum(low[['300+ tp']].iloc[:-1])[0])+int(np.sum(low[['300+ fn']].iloc[:-1])[0]),'nodules of size 300+mm3 in low BMI group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below not correct here since they also consider nodules changed above to have '0' volume.\n",
    "May also be related to 588074 being 588047!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "711da4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For a specific degree of emphysema, get all nodule sizes, a dictionary with the frequency of the number of nodules\n",
    "#that appear in the patients of that dataframe, a list with the name of the patient that each nodule corresponds to\n",
    "#(found on sizes), and the number of nodules found on that patient\n",
    "#plotting parameter should be set to False when creating plots so that we take into account nodules >300mm3\n",
    "\n",
    "def get_nodule_names_and_sizes(dataframe,plotting=False): #Input is a dataframe of a specific degree of emphysema\n",
    "    \n",
    "    #Initialize empty lists to be filled with nodule sizes, patient names, and num of nodules of each patient\n",
    "    sizes=[]\n",
    "    patient_names=[]\n",
    "    patient_nodules=[]\n",
    "    \n",
    "    all_nodules_num=[] #Keep track of the number of nodules \n",
    "    \n",
    "    for name in dataframe.columns: #There are many columns with nodules, one column for each unique nodule\n",
    "        if 'volume_all' in name: #these are columns with nodules\n",
    "            count = dataframe[name].isna().sum() #count NaN values\n",
    "            print(\"Column {} has {} non-nan values\".format(name,len(dataframe)-count))\n",
    "            \n",
    "            selected_rows = dataframe[~dataframe[name].isna()] #select rows with non nan values\n",
    "            sizes.extend(selected_rows[name].values.tolist()) #add the values of these nodules to a list\n",
    "            patient_names.extend(selected_rows['participant_id'].values.tolist()) #add patient_ids to a list\n",
    "            patient_nodules.extend(selected_rows['measured_nodules'].values.tolist()) #add number of nodules to a list\n",
    "        \n",
    "            if plotting==True:\n",
    "                #if we don't rename it then wrong indices will be added in patient_names and nodules\n",
    "                sizes_new=[val for ind,val in enumerate(sizes) if ind in np.where(np.array(sizes)<=300)[0]]\n",
    "                patient_names=[patient_names[ind] for ind,val in enumerate(sizes) if ind in np.where(np.array(sizes)<=300)[0]]\n",
    "                patient_nodules=[patient_names.count(x) for x in patient_names]\n",
    "                sizes=sizes_new\n",
    "\n",
    "    all_nodules_num.extend(patient_nodules) #Extend a list with the number of nodules counted in each participant\n",
    "            \n",
    "    num_nodules=dataframe['measured_nodules'].values #Get all the values of the number of nodules for each patient, even 0\n",
    "    \n",
    "    if plotting==False:\n",
    "        num_nodules_no_zero=num_nodules[num_nodules!=0] #Get num of nodules if nodules detected\n",
    "    else:\n",
    "        num_nodules_no_zero=all_nodules_num #Replace with number of nodules created above - only keep those <300mm3\n",
    "\n",
    "    non_zero_count={} #create a dictionary with the number of nodules and how often that number is detected\n",
    "    for elem in num_nodules_no_zero:\n",
    "        if elem in non_zero_count:\n",
    "            non_zero_count[elem]=non_zero_count[elem]+1\n",
    "        else:\n",
    "            non_zero_count[elem]=1\n",
    "        \n",
    "    if plotting==True:\n",
    "        for key,val in non_zero_count.items(): #Added to make sure that the right number was counted\n",
    "            non_zero_count[key]=np.ceil(val/key)    \n",
    "    \n",
    "        \n",
    "    non_zero_count[0.0]=num_nodules.tolist().count(0) #add at the end the case when no nodules were detected\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"Dictionary with number of measured nodules and how many times that number occured\")\n",
    "    \n",
    "    non_zero_count_sorted = dict(sorted(non_zero_count.items())) #sorted dictionary\n",
    "    print(non_zero_count_sorted)\n",
    "    print('\\n')\n",
    "    \n",
    "    print(\"All participants\",patient_names)\n",
    "    \n",
    "    return sizes, non_zero_count_sorted, patient_names, patient_nodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3cb07aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_severity_names=['high_redcap','low_redcap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a02b3b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'high_redcap': 1, 'low_redcap': 0}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BMI types ordered by severity\n",
    "severity=['high_redcap','low_redcap']\n",
    "severity_nums=[1,0] #create integers from the most severe to the least one \n",
    "severity_dict={} #create a dict with severity name and corresponding integer\n",
    "for index,val in enumerate(severity):\n",
    "    severity_dict[val]=severity_nums[index]\n",
    "severity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2c724b4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Barplots for different degrees of emphysema\n",
    "fig, axs = plt.subplots(ncols=len(severity_dict),figsize=(17,4)) #set to 23,6 if trace and mild are included\n",
    "fig2, axs2 = plt.subplots(ncols=len(severity_dict),figsize=(17,4)) \n",
    "# fig3, axs3 = plt.subplots(ncols=len(severity_dict),figsize=(17,4)) \n",
    "# fig_4, axs_4 = plt.subplots(ncols=len(severity_dict),figsize=(17,4))\n",
    "\n",
    "for ind,name in enumerate(severity_dict.keys()): #loop over names of emphysema degrees\n",
    "    print(name)\n",
    "    \n",
    "    #Execute above function to get nodule names and sizes for each emphysema degree\n",
    "    exec(\"sizes_\"+new_severity_names[ind]+',nodule_num_'+new_severity_names[ind]+',patient_names_'+new_severity_names[ind]+\n",
    "         ',patient_nodules_'+new_severity_names[ind]+'=get_nodule_names_and_sizes('+name+',False)') \n",
    "       \n",
    "    #Barplot of the distribution of the num of nodules for each degree of emphysema using key-value pairs from dictionaries   \n",
    "    freq=np.asarray(list(eval('nodule_num_'+new_severity_names[ind]+'.values()')))\n",
    "    vals=np.asarray(list(eval('nodule_num_'+new_severity_names[ind]+'.keys()')))\n",
    "    fig=sns.barplot(vals,freq,ax=axs[ind])\n",
    "    fig.set(xlabel=new_severity_names[ind]+'_num_of_nodules', ylabel='num_of_individuals')\n",
    "    \n",
    "    #Histogram of sizes when outliers were removed   \n",
    "    sizes_no_outliers = [x for x in eval('sizes_'+new_severity_names[ind]) if x <=300 and x>0]  \n",
    "    s = pd.DataFrame({'val': sizes_no_outliers, 'color':['<=100$mm^3$' if x<=100 else '>100$mm^3$ & <=300$mm^3$' for x in sizes_no_outliers]})\n",
    "    fig2=sns.histplot(data=s,ax=axs2[ind],x='val',hue='color',hue_order=[\"<=100$mm^3$\", \">100$mm^3$ & <=300$mm^3$\"])\n",
    "#     for ax in axs2: #delete legend\n",
    "#         ax.legend([],[], frameon=False)\n",
    "    fig2.set(xlabel=new_severity_names[ind]+'_sizes ($mm^3$)', ylabel='num_of_nodules')\n",
    "    \n",
    "#     #The above was originally as:\n",
    "#     sizes_no_outliers = [x for x in eval('sizes_'+new_severity_names[ind]) if x <=300]  \n",
    "#     fig2=sns.histplot(sizes_no_outliers,ax=axs2[ind])\n",
    "#     fig2.set(xlabel=new_severity_names[ind]+'_sizes ($mm^3$)', ylabel='num_of_nodules')\n",
    "    \n",
    "#     #Histogram of all sizes, even outliers\n",
    "#     vals=np.asarray(eval('sizes_'+new_severity_names[ind]))\n",
    "#     fig3=sns.histplot(vals,ax=axs3[ind])\n",
    "#     fig3.set(xlabel=new_severity_names[ind]+'_sizes_all ($mm^3$)', ylabel='num_of_nodules')\n",
    "    \n",
    "    #A Distplot or distribution plot, depicts the variation in the data distribution. \n",
    "    #Seaborn Distplot represents the overall distribution of continuous data variables.\n",
    "    #Same graph as above with a distribution line fit\n",
    "#     vals=np.asarray(eval('sizes_'+new_severity_names[ind]))\n",
    "#     fig_4=sns.distplot(vals,ax=axs_4[ind])\n",
    "#     fig_4.set(xlabel=new_severity_names[ind]+'_sizes_all ($mm^3$3)', ylabel='num_of_nodules')\n",
    "\n",
    "\n",
    "#Save each of the above graphs seperately\n",
    "# bar_num_nods = fig.get_figure()\n",
    "# bar_num_nods.savefig(\"distribution_num_nods.png\")\n",
    "# hist_no_outliers = fig2.get_figure()\n",
    "# hist_no_outliers.savefig(\"size_distribution.png\")\n",
    "# hist_all = fig3.get_figure()\n",
    "# hist_all.savefig(\"size_distribution_all.png\")\n",
    "# dist_plot_all = fig_4.get_figure()\n",
    "# dist_plot_all.savefig(\"size_distribution_all_line_fit.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c4f26b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 351\n",
      "Length of df is 352\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(BMI_data['Severity'])):\n",
    "    try:\n",
    "        if BMI_data['Severity'].iloc[i]<BMI_data['Severity'].iloc[i+1]:\n",
    "            print(i)\n",
    "    except:\n",
    "        print('Error in {}'.format(i))\n",
    "        pass\n",
    "print('Length of df is {}'.format(len(BMI_data['Severity'])))    \n",
    "#Above means that we have patients in order, if error in len of dataframe (-1) only"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d01c97fe",
   "metadata": {},
   "source": [
    "### Create size groups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "231d013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each degree of emphysema print nodule distribution, total number of nodules and confirm that we have the expected num\n",
    "for degree in new_severity_names:\n",
    "    print(degree)\n",
    "    print(\"Number of nodules and number of patients with that many nodules:{}\".format(eval('nodule_num_'+degree)))\n",
    "    \n",
    "    #For each number of nodules (0-10) multiply it by th number of occurences and add them all to get total num of nodules\n",
    "    total=np.sum([a*b for a,b in zip(list(eval('nodule_num_'+degree).keys()),list(eval('nodule_num_'+degree).values()))])\n",
    "    \n",
    "    print(\"Total num of nodules for {} is {}\".format(degree,total))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9e08af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Low BMI group median size:\",np.median(severe_emph_sizes),\"IQR:\",st.iqr(severe_emph_sizes))\n",
    "# print(\"High BMI group median size:\",np.median(new_vols_small),\"IQR:\",st.iqr(new_vols_small))\n",
    "\n",
    "# res = st.median_test(severe_emph_sizes, new_vols_small)\n",
    "# print(\"P value is {}\".format(res[1])) #this is pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "845c5c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get group sizes - '_100' corresponds to nodules between 30-100mm3 since nodules <30mm3 were not annotated by radiologists\n",
    "for degree in new_severity_names:\n",
    "    \n",
    "    print(degree)\n",
    "       \n",
    "    #Create groups based on size of nodules for each degree - eg. sizes_low, sizes_low_30, etc.\n",
    "    #Each group is: '_30'=nodules smaller than 30mm3, '_30_100' from 30-100mm3 etc.\n",
    "    exec(\"sizes_\"+degree+\"=np.asarray(eval('sizes_'+degree)).astype(int)\")\n",
    "    exec(\"sizes_\"+degree+\"_100\"+\"=sizes_\"+degree+\"[sizes_\"+degree+\"<=100]\")\n",
    "    exec(\"sizes_\"+degree+\"_100_300\"+\"=sizes_\"+degree+\"[(sizes_\"+degree+\">100) & (sizes_\"+degree+\"<=300)]\")\n",
    "    exec(\"sizes_\"+degree+\"_300\"+\"=sizes_\"+degree+\"[(sizes_\"+degree+\">300)]\")\n",
    "    \n",
    "    #Create list with indices of the patients of the above groups\n",
    "    exec(\"patient_ind_\"+degree+\"=eval('patient_names_'+degree)\") #Needed for patient_names below\n",
    "    exec(\"patient_ind_\"+degree+\"_100=[index for index,value in enumerate(\"+'sizes_'+degree+\") if value <=100]\")\n",
    "    exec(\"patient_ind_\"+degree+\"_100_300=[index for index,value in enumerate(\"+'sizes_'+degree+\") if value>100 and value<=300]\")\n",
    "    exec(\"patient_ind_\"+degree+\"_300=[index for index,value in enumerate(\"+'sizes_'+degree+\") if value>300]\")\n",
    "    \n",
    "    #Get the actual names of the above patients for each size group\n",
    "    exec(\"patient_\"+degree+\"_names=patient_names_\"+degree)\n",
    "    exec(\"patient_\"+degree+\"_100_names=[patient_ind_\"+degree+\"[i] for i in patient_ind_\"+degree+\"_100]\")\n",
    "    exec(\"patient_\"+degree+\"_100_300_names=[patient_ind_\"+degree+\"[i] for i in patient_ind_\"+degree+\"_100_300]\")\n",
    "    exec(\"patient_\"+degree+\"_300_names=[patient_ind_\"+degree+\"[i] for i in patient_ind_\"+degree+\"_300]\")\n",
    "\n",
    "    #Number of nodules contained in the particular patient\n",
    "    exec(\"patient_nodules_\"+degree+\"_100=[patient_nodules_\"+degree+\"[i] for i in patient_ind_\"+degree+\"_100]\")\n",
    "    exec(\"patient_nodules_\"+degree+\"_100_300=[patient_nodules_\"+degree+\"[i] for i in patient_ind_\"+degree+\"_100_300]\")\n",
    "    exec(\"patient_nodules_\"+degree+\"_300=[patient_nodules_\"+degree+\"[i] for i in patient_ind_\"+degree+\"_300]\")    \n",
    "    \n",
    "    #Confirm that the above worked\n",
    "    assert len(eval(\"sizes_\"+degree))==len(eval(\"sizes_\"+degree+\"_100\"))+len(eval(\"sizes_\"+degree+\"_100_300\"))+len(eval(\"sizes_\"+degree+\"_300\"))\n",
    "    assert len(eval(\"patient_nodules_\"+degree))==len(eval(\"patient_nodules_\"+degree+\"_100\"))+len(eval(\"patient_nodules_\"+degree+\"_100_300\"))+len(eval(\"patient_nodules_\"+degree+\"_300\"))\n",
    "    \n",
    "    #Print unique patients contained in each degree of BMI - Only those with nodules in the lists below\n",
    "    #The rest have no nodules in GT annotations\n",
    "    print(len(np.unique(eval(\"patient_\"+degree+\"_names\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total num of nodules in high:\",len(sizes_high_redcap))\n",
    "print(\"Total num of nodules of 30-100mm3 in high:\",len(sizes_high_redcap_100))\n",
    "print(\"Total num of nodules of 100-300mm3 in high:\",len(sizes_high_redcap_100_300))\n",
    "print(\"Total num of nodules of 300+mm3 in high:\",len(sizes_high_redcap_300))\n",
    "\n",
    "print(\"Total num of nodules in low:\",len(sizes_low_redcap))\n",
    "print(\"Total num of nodules of 30-100mm3 in low:\",len(sizes_low_redcap_100))\n",
    "print(\"Total num of nodules of 100-300mm3 in low:\",len(sizes_low_redcap_100_300))\n",
    "print(\"Total num of nodules of 300+mm3 in low:\",len(sizes_low_redcap_300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d68f8cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high_redcap\n",
      "low_redcap\n"
     ]
    }
   ],
   "source": [
    "#Get lists of all participants within each low and high subgroups - even those without nodules\n",
    "for ind,name in enumerate(new_severity_names):\n",
    "    print(name)\n",
    "    exec('all_'+name+'=[]')\n",
    "    exec(\"all_\"+name+\".append(list(BMI_data[BMI_data['BMI']==list(severity_dict.keys())[\"+str(ind)+\"]]['participant_id'].values))\")\n",
    "    exec('all_'+name+'=all_'+name+'[0]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b7af9ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(all_high_redcap) #Check cases with high BMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f4391510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(all_low_redcap) #Check cases with low BMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "db0c5282",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_300_low=[pat for pat in patient_low_redcap_300_names if pat not in patient_low_redcap_100_300_names\n",
    "              and pat not in patient_low_redcap_100_names] #These are cases with only nodules >300mm3\n",
    "print(\"Total num of participants with low is {}\".format(len(all_low_redcap)))\n",
    "print(\"Total num of participants with low and with nodules is {}\".format(len(list(np.unique(eval(\"patient_low_redcap_names\"))))))\n",
    "print(\"Total num of participants with low and without nodules is {}\".format(len(list(set(all_low_redcap)-set(list(np.unique(eval(\"patient_low_redcap_names\"))))))))\n",
    "print(\"The participants without nodules with low are the following: {}\".format(list(set(all_low_redcap)-set(list(np.unique(eval(\"patient_low_redcap_names\")))))))\n",
    "print('\\n')\n",
    "\n",
    "only_300_high=[pat for pat in patient_high_redcap_300_names if pat not in patient_high_redcap_100_300_names\n",
    "              and pat not in patient_high_redcap_100_names] #These are cases with only nodules >300mm3\n",
    "print(\"Total num of participants with high is {}\".format(len(all_high_redcap)))\n",
    "print(\"Total num of participants with high and with nodules is {}\".format(len(list(np.unique(eval(\"patient_high_redcap_names\"))))))\n",
    "print(\"Total num of participants with high and without nodules is {}\".format(len(list(set(all_high_redcap)-set(list(np.unique(eval(\"patient_high_redcap_names\"))))))))\n",
    "print(\"The participants without nodules with high are the following: {}\".format(list(set(all_high_redcap)-set(list(np.unique(eval(\"patient_high_redcap_names\")))))))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ab2d094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Volumes of nodules of non-emphysema list - Similar as above without participant_id to be used below\n",
    "only_vols=BMI_data[BMI_data['participant_id'].isin(np.unique(all_pats))].iloc[:,[-10,-9,-8,-7,-6,-5,-4,-3,-2,-1]]\n",
    "\n",
    "#Get number of nodules of participants in the non-emphysema list\n",
    "new_vols=[]\n",
    "for col in only_vols.columns:\n",
    "    new_vols.append(only_vols[col]) \n",
    "\n",
    "new_vols=[i for x in new_vols for i in x]\n",
    "new_vols=[x for x in new_vols if np.isnan(x)==False]\n",
    "\n",
    "print(\"Total num of nodules in high+low BMI list is {}\".format(len(new_vols)))\n",
    "# np.sum(only_vols.count()) #another way to get their number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0ba7daa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vols_100=[x for x in new_vols if x<=100] #If we want only nodules <=100mm3\n",
    "new_vols_100_300=[x for x in new_vols if x<=300 and x>100] #If we want only nodules 100<x<=300mm3\n",
    "new_vols_300=[x for x in new_vols if x>300] #If we want only nodules >300mm3\n",
    "print(\"Total num of nodules in high+low BMI list of size 30-100mm3 is {}\".format(len(new_vols_100)))\n",
    "print(\"Total num of nodules in high+low BMI list of size 100-300mm3 is {}\".format(len(new_vols_100_300)))\n",
    "print(\"Total num of nodules in high+low BMI list of size >300mm3 is {}\".format(len(new_vols_300)))\n",
    "#We expect to have nodules >300mm3 here, even though not explicitly sampled, since we sample on a scan basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b9faa1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to execute the script is 5.993586540222168 sec\n"
     ]
    }
   ],
   "source": [
    "end=time.time()\n",
    "print(\"Time to execute the script is {} sec\".format(end-start)) #~15secs without"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
